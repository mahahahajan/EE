Spherical Structured Feature Maps for Kernel Approximation

Yueming Lyu 1

Abstract

We propose Spherical Structured Feature (SSF)
maps to approximate shift and rotation invari-
ant kernels as well as bth-order arc-cosine ker-
nels (Cho & Saul, 2009). We construct SSF
maps based on the point set on d − 1 dimen-
sional sphere Sd−1. We prove that the inner
product of SSF maps are unbiased estimates for
above kernels if asymptotically uniformly dis-
tributed point set on Sd−1 is given. According
to (Brauchart & Grabner, 2015), optimizing the
discrete Riesz s-energy can generate asymptot-
ically uniformly distributed point set on Sd−1.
Thus, we propose an efﬁcient coordinate decent
method to ﬁnd a local optimum of the discrete
Riesz s-energy for SSF maps construction. The-
oretically, SSF maps construction achieves linear
space complexity and loglinear time complexity.
Empirically, SSF maps achieve superior perfor-
mance compared with other methods.

1. Introduction
Kernel methods such as Gaussian processes (GPs) (Ras-
mussen, 2006; Srinivas et al., 2009; Snoek et al., 2012)
and support vector machines (SVMs) (Chang & Lin, 2011;
Fan et al., 2008) have been successfully used in many sta-
tistical modeling and machine learning tasks. Despite of
strong expressive power, kernel methods usually cannot
scale up to the large scale datasets with L samples due to
the need of manipulating L×L Gram matrix. Recently, ran-
dom feature maps (Rahimi et al., 2007; Rahimi & Recht,
2009; Sutherland & Schneider, 2015) have demonstrated
their effectiveness on kernel approximation to scale up ker-
nel methods. Roughly speaking, a shift invariant kernel
K(x, z) = K(x − z) : Rd → C can be approximated
by K(x, z) ≈ Ψ(x)TΨ(z), where Ψ is the explicit mapped
√
N, where f (·)
feature constructed as Ψ(x) = f (WT x)/
1Department of Computer Science, City University of Hong
Kong, Tat Chee Avenue, Hong Kong . Correspondence to: Yuem-
ing Lyu <LV Yueming@outlook.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

denotes the nonlinear function, W ∈ Rd×N is constructed
by N i.i.d samples drawn from a distribution deﬁned by
K. Therefore, the training and inference of kernel methods
can be greatly accelerated by working directly on the pri-
mal space of Ψ(·). For example, Gaussian Processes (GPs)
have O(L3) computation and O(L2) storage complexity.
By using feature maps, it reduces to O(N 2L + N 3) com-
putation and O(N L + N 2) storage complexity. All these
elegant properties make random feature maps promising
for large scale kernel methods. Thus, many kernel meth-
ods (Le & Wilson, 2015; Cutajar et al., 2016; Oliva et al.,
2016) have been proposed to deal with large scale statistical
learning by directly working on feature maps.
Generally, two aspects of random feature maps are mostly
concerned by literature for scaling up kernel methods. One
is the approximation accuracy of feature maps while the
other is the computational cost of feature maps construc-
tion. To achieve better approximation accuracy, (Yang,
2014; Avron et al., 2016) employ QMC (Dick et al., 2013)
sampling instead of standard Monte Carlo sampling to con-
struct feature maps. By mapping QMC points on [0, 1]d
through the inverse cumulative distribution function, they
construct more effective feature maps. To reduce time
complexity, (Le et al., 2013) propose Fastfood to construct
feature maps. Beneﬁting from the special structured ma-
trix multiplication, it reduces time complexity of feature
maps construction from O(N d) to O(N log d). However,
it achieves computational efﬁciency at the expense of in-
creasing the variance of approximation. Recently, (Feng
et al., 2015) employ the property of circulant matrix to ac-
celerate feature maps construction of Gaussian kernel with-
out increasing the variance. (Choromanski & Sindhwani,
2016) generalize the Fastfood and circulant feature maps
to P model and particularly discuss the structured matrix
with low-displacement rank. Despite of the success of P
model, it still cannot achieve better approximation accuracy
compared with feature maps obtained with fully Gaussian
matrix.
To achieve better approximation accuracy and loglinear
time complexity, we propose Spherical Structured Feature
(SSF) maps to approximate shift and rotation invariant ker-
nels as well as bth-order arc-cosine kernels (Cho & Saul,
2009). Speciﬁcally, We construct SSF maps based on the
point set on d − 1 dimensional sphere Sd−1, where the

Spherical Structured Feature Maps for Kernel Approximation

points are columns of a particular structured matrix pro-
duced by a discrete Fourier matrix. The points on Sd−1
for SSF maps construction can be generated by optimiz-
ing the discrete Riesz s-energy. According to (Brauchart
et al., 2014), optimizing the discrete Riesz s-energy (for s
in some ranges) can generate QMC designs on Sd−1, which
usually can achieve smaller approximation error compared
with fully random methods. Moreover, Because of spe-
cial structure of the point set, SSF maps construction can
achieve loglinear time complexity via Fast Fourier Trans-
form (FFT).
Our contributions are summarized as follows:

• We propose Spherical Structured Feature (SSF) maps
to approximate shift and rotation invariant kernels as
well as bth-order arc-cosine kernels (Cho & Saul,
2009). We prove that the inner product of SSF maps
are unbiased estimates for above kernels if asymptoti-
cally uniformly distributed point set on d − 1 dimen-
sional sphere Sd−1 is given.

• We propose an efﬁcient coordinate decent method
to ﬁnd a local optimum of the discrete Riesz s-
energy (Brauchart & Grabner, 2015),
thereby ap-
proximately generating asymptotically uniformly dis-
tributed points on Sd−1.

• We can construct SSF maps with linear space com-
plexity and loglinear time complexity. Empirically,
SSF maps achieve superior performance compared
with other methods.

2. Background and Preliminaries
We provide a brief review of random feature maps and the
discrete Riesz s-energy in this section as preliminaries.

2.1. Random Feature Maps

Random feature maps can be viewed as equal weight ap-
proximation of multidimensional integrals. One earlier
work (Rahimi et al., 2007) approximates the shift invariant
kernels based on the Bochner’s Theorem.
Theorem 2.1 Bochner’s Theorem ((Rudin, 2011)) : A con-
tinuous shift invariant scaled kernel function K(x, z) =
K(x − z) : Rd → C is positive deﬁnite if and only if
it is the Fourier Transform of a unique ﬁnite probability
measure p on Rd.

Rd e−i(x−z)Twp(w)dw

(1)

K(x, z) =(cid:82)

For a real valued kernel K(x, z), p(w) = p(−w) ≥ 0 can
ensure the imaginary parts of the integral vanish. Accord-
ing to the Bochner’s theorem, there is a one-to-one corre-

spondence between the kernel functions K(x, z) and prob-
ability densities p(w) deﬁned on Rd.
Shift and rotation invariant kernels are shift invariant
kernels with the rotation invariant property, i.e. K(x, z) =
K(Rx, Rz), given any rotation R ∈ SO(d), where SO(d)
denotes rotation groups. The Gaussian kernel K(x, z) =
e−(cid:107)x−z(cid:107)2
2/2σ2 is a member of this family. From Bochner’s
theorem,
the corresponding probability density is also
Gaussian. For a general Gaussian RBF kernel K(x, z) =
e−(x−z)T Σ(x−z)/2, it can be transformed into rotation in-
variant form by using y = Σ1/2x in the original domain.
bth-order arc-cosine kernels are rotation invariant ker-
nels. As discussed in (Cho & Saul, 2009), bth-order arc-
cosine kernels have the following form:
2 (cid:107)z(cid:107)b

Kb(x, z) = 1

2 Jb(θ)

(2)

where θ = cos−1(cid:16) xT z

(cid:17)
π (cid:107)x(cid:107)b

(cid:107)x(cid:107)2(cid:107)z(cid:107)2

bth-order arc-cosine kernels have trivial dependence on the
norm of x and z. The dependence on the angle is deﬁned
by function Jb(θ). bth-order arc-cosine kernels are rota-
tion invariant kernels but not shift invariant kernels in gen-
eral. For example, the zero-order (3) and ﬁrst-order (4)
arc-cosine kernel are not shift invariant kernels.

K0(x, z) = 1 − θ

π

K1(x, z) = 1

π (cid:107)x(cid:107)2 (cid:107)z(cid:107)2 (sin θ + (π − θ) cos θ)

(3)

(4)

The bth-order arc-cosine kernel Kb(x, z) can be reformu-
lated via the integral representation:

Kb(x, z) = 2(cid:82)

b

b
(wT z)

Rd s(wT x)s(wT z)(wT x)

p(w)dw
(5)
where s(·) is a step function (i.e. s(x) = 1 if x > 0 and 0
otherwise) and the density p is standard Gaussian.
Feature maps: Both Monte Carlo and Quasi-Monte Carlo
approximation (Dick et al., 2013) are equal weight approx-
imation to integrals. Based on equal weight approximation,
the feature maps can be constructed as:

f(cid:0)wT
i x(cid:1) f(cid:0)wT

i x(cid:1) = Ψ(x)T Ψ(z)

K(x, z) ≈ 1

N

N(cid:80)

i=1

(6)
where wi, i ∈ 1, ..., N are samples constructed by Monte
Carlo or Quasi-Monte Carlo methods. f (·) is a nonlinear
function depending on the kernel. Ψ(·) is the explicit ﬁnite
dimensional feature map. For Gaussian kernel with band-
width σ, the associated nonlinear function is a complex ex-
ponential function f (x) = eix/σ. For a zero-order arc-
cosine kernel in (3) and ﬁrst-order arc-cosine kernel in (4),
the associated nonlinear functions are step function f (x) =
s(x) and ReLU activation function f (x) = max(0, x) re-
spectively.

Spherical Structured Feature Maps for Kernel Approximation

2.2. Discrete Riesz s-energy

The discrete Riesz s-energy is related to the equal weight
numerical integration and uniformly distributed point set.
Equal weight numerical integration over a d-dimensional
sphere Sd := {x ∈ Rd+1 | (cid:107)x(cid:107)2 = 1} uses equal weight
summation of ﬁnite point evaluations of the integrands to
approximate the integrals:

Sd f (v)dσ(v) ≈ 1

N

f (vi)

(7)

(cid:82)

N(cid:80)

i=1

where σ denotes the normalized surface area measure on
Sd.
According to (Brauchart & Grabner, 2015), the point set
V = [v1, ..., vN ] ∈ Sd×N is asymptotically uniformly dis-
tributed if equation (8) holds true.

f (vi) =(cid:82)

N(cid:80)

i=1

lim
N→∞

1
N

Sd f (v)dσ(v)

(8)

The discrete Riesz s-energy(G¨otz, 2003; Brauchart &
Grabner, 2015) is deﬁned as equation (9):



Es (V) :=

N(cid:80)
N(cid:80)

i=1

N(cid:80)
N(cid:80)

i=1

j=1,j(cid:54)=i

j=1,j(cid:54)=i

1

(cid:107)vi−vj(cid:107)s

2

, s (cid:54)= 0

(9)

log

1

(cid:107)vi−vj(cid:107)2

, s = 0

Theorem 2.2 ((Brauchart & Grabner, 2015)): For s > −2,
the optimum N-point conﬁguration of the Riesz s-energy
on Sd is asymptotically uniformly distributed w.r.t the nor-
malized surface area measure σ on Sd.
According to (Brauchart et al., 2014; Brauchart & Grabner,
2015), the discrete Riesz s-energy can serve as a criterion
to construct the point set V = [v1, ..., vN ] ∈ Sd×N for
QMC designs. Particularly, (Brauchart et al., 2014) have
proved that maximizing the discrete Riesz s-energy with
s ∈ (−2, 0) can generate QMC designs for functions in
Sobolev space. They also prove that QMC designs have
higher convergence rate of worst-case error than fully ran-
domly chosen points for functions in Sobolev space.

3. Spherical Structured Feature Maps
In this section, we propose SSF maps to approximate shift
and rotation invariant kernels as well as bth-order arc-
cosine kernels by employing their rotation invariant prop-
erty.

3.1. Feature Maps for Shift and Rotation Invariant

Kernels

Shift and rotation invariant kernels are highly symmet-
ric and structured because they satisfy both shift invari-
ant property and rotation invariant property. Rotation in-
variant property means that K(x, z) = K(Rx, Rz), given
any rotation R ∈ SO(d), where SO(d) denotes rotation
groups. To beneﬁt from rotation invariant property, it is
reasonable to construct the feature maps by using spherical
equal weight approximation in equation (7) and (8).
The feature maps for real valued shift and rotation invariant
kernels K(x, z) can be constructed as equation (10):

Ψ (x) = 1√

..., cos(cid:0)Φ−(tM )xT vN

[cos(cid:0)Φ−(t1)xT v1

(cid:1) , sin(cid:0)Φ−(t1)xT v1
(cid:1)]T

(cid:1) , sin(cid:0)Φ−(tM )xT vN

(cid:1) ,

(10)
M +1, V = [v1, ..., vN ] ∈ Sd−1×N denotes the
where tj = j
point set asymptotically uniformly distributed on Sd−1and
Φ−(x) denotes the inverse cumulative distribution function
w.r.t the nonnegative radial scale.
Theorem 3.1: Ψ(x)T Ψ (z) is an unbiased estimate of a
real valued shift and rotation invariant kennel K(x, z).
Proof: From Bochner’s Theorem, a shift invariant kernel
K(x, z) can be written as equation (1). Let r = (cid:107)w(cid:107)2and
p(r) be the density function of r. Because of the rotation
invariant property of K(x, z), we achieve equation (11).

NM

K(x, z) =(cid:82)
=(cid:82)

R+

[0,1]

(cid:82)
(cid:82)

Sd−1 e−ir(x−z)T vp(r)drdσ(v)
Sd−1 e−i Φ−(t)(x−z)T vdσ(v)dt

(11)

where R+ denotes the nonnegative real values.
For real valued kernel K(x, z), the imaginary parts of the
integral vanish. We can achieve equation (12).
Φ−(t)(x − z)T v

K(x, z) =(cid:82)

dσ(v)dt

(cid:16)

(cid:17)

(cid:82)

Sd−1 cos

(12)
According to the property of asymptotically uniformly
distributed point set V in equation (8) and the one-
dimensional QMC rule, we obtain equation (13).

[0,1]

lim

M,N→∞ Ψ(x)T Ψ (z) =

(cid:1)

i=1

(cid:1) cos(cid:0)Φ−(tj)zT vi
(cid:1))

M(cid:80)
N(cid:80)
+ sin(cid:0)Φ−(tj)xT vi
(cid:16)
M(cid:80)
(cid:16)

(cos(cid:0)Φ−(tj)xT vi
(cid:1) sin(cid:0)Φ−(tj)zT vi
(cid:17)
N(cid:80)
(cid:17)

Φ−(tj)(x − z)T vi

cos

j=1

j=1

i=1

Φ−(t)(x − z)T v

dσ(v)dt

lim

M,N→∞

1

M N

=(cid:82)

(cid:82)

= lim

M,N→∞

1

M N

Sd−1 cos

[0,1]

= K(x, z)

(13)
(cid:3)

Spherical Structured Feature Maps for Kernel Approximation

Proposition 3.1: Let U = [V,−V], using point set U to
approximate a real valued shift and rotation invariant kernel
K(x, z) by using equation (10) is equal to using point set
V to approximate K(x, z):

Ψ(x; U)T Ψ (z; U) = Ψ(x; V)T Ψ (z; V)

(14)

Proof: Note that cosine function is an even function. Thus,
we obtain equation (15).
Φ−(tj)(x − z)T vi

(cid:16)−Φ−(tj)(x − z)T vi

= cos

(cid:16)

(cid:17)

(cid:17)

cos

Thus, we achieve equation (16).

Ψ(x; U)T Ψ (z; U)

Φ−(tj)(x − z)T vi

(cid:17)
(cid:16)
(cid:16)−Φ−(tj)(x − z)T vi
(cid:17)
(cid:16)
(cid:17)

Φ−(tj)(x − z)T vi

cos

cos

2 cos

N(cid:80)
N(cid:80)
N(cid:80)

M(cid:80)
M(cid:80)
M(cid:80)

i=1

j=1

i=1

j=1

i=1

j=1

= 1

2NM

+ 1

2NM

= 1

2NM

= Ψ(x; V)T Ψ (z; V)

(15)

(16)

(cid:3)

Proposition 3.1 shows that for a shift and rotation invariant
kernel, computing N points can achieve the same approxi-
mation effect compared with using 2N points.

3.2. Feature Maps for bth-order Arc-cosine Kernels

In this subsection, we discuss the feature maps for bth-
order arc-cosine kernels. We discuss them separately be-
cause they are rotation invariant kernels but not shift invari-
ant kernels in general. Moreover, they are closely related to
deep neural networks (Cho & Saul, 2009), which demon-
strate super performance in many areas.
Lemma 3.1: The bth-order arc-cosine kernels can be cal-
culated as equation (17).

Sd−1 χ(cid:0)vT x(cid:1) χ(cid:0)vT z(cid:1)
(cid:82)
where χ(x) = max(0, sign(x)|x|b), Cb =(cid:82)

+χ(−vT x)χ(−vT z)dσ(v)

r2bp(r)dr.
Cb is a constant that is independent of x and z. p(r) is
the density function of the chi-distribution with d degrees
freedom. For example, the constants associated with the
zero, ﬁrst and second-order arc-cosine kernels are C0 = 1,
C1 = d and C2 = d(d + 2) respectively.
Proof: From equation (5), we can achieve equation (18).

Kb(x, z) = Cb

(17)

R+

Kb(x, z) = 2(cid:82)
= 2(cid:82)

Rd χ(cid:0)wT x(cid:1) χ(cid:0)wT z(cid:1) p(w)dw

Rd s(wT x)s(wT z)(wT x)

b

(wT z)

b

p(w)dw

(18)

Let r = (cid:107)w(cid:107)2. Since p is standard Gaussian, by taking
rotation invariant property, we obtain equation (19).

Rd χ(cid:0)wTx(cid:1) χ(cid:0)wTz(cid:1) p(w)dw
Kb(x, z) = 2(cid:82)
(cid:82)
= 2(cid:82)
(cid:82)
= 2(cid:82)
r2bp(r)dr(cid:82)
= 2(cid:82)
Sd−1 χ(cid:0)vT x(cid:1) χ(cid:0)vT z(cid:1) dσ(v)
(cid:82)

χ(cid:0)rbvT x(cid:1) χ(cid:0)rbvT z(cid:1) p(r)dσ(v)dr
r2bχ(cid:0)vT x(cid:1) χ(cid:0)vT z(cid:1) p(r)dσ(v)dr
Sd−1 χ(cid:0)vT x(cid:1) χ(cid:0)vT z(cid:1) dσ(v)

Sd−1
Sd−1
R+
= 2Cb

R+
R+

(19)
Since Kb(x, z) is rotation invariant, we have Kb(x, z) =
Kb(−x,−z). Together with equation (19), we achieve
equation (20).

Sd−1 χ(cid:0)vT x(cid:1) χ(cid:0)vT z(cid:1)
(cid:82)

+χ(−vT x)χ(−vT z)dσ(v)

Kb(x, z) = Cb

(20)
(cid:3)

The feature maps for a bth-order arc-cosine kernel Kb(x, z)
can be constructed as equation (21).

(cid:113) Cb
N [χ(cid:0)vT
1 x(cid:1) , χ(cid:0)−vT
N x(cid:1) , χ(cid:0)−vT
χ(cid:0)vT

1 x(cid:1) , ....,
N x(cid:1) ]T ∈ R2N

Ψ (x) =

(21)

Theorem 3.2: Ψ(x)T Ψ (z) is an unbiased estimate of a
bth-order arc-cosine kernel Kb(x, z).
Proof: According to the Lemma 3.1 and the property of
the asymptotically uniformly distributed point set V, we
obtain equation (22).
N→∞ Ψ(x)T Ψ (z)
lim
= lim
N→∞

N(cid:80)
Sd−1 χ(cid:0)vT x(cid:1) χ(cid:0)vT z(cid:1) + χ(−vT x)χ(−vT z)dσ(v)

χ(cid:0)vT
i x(cid:1) χ(cid:0)vT

i z(cid:1) + χ(−vT

i x)χ(−vT

i z)

(cid:82)

Cb
N

i=1

= Cb
= Kb(x, z)

(22)
(cid:3)

From equation (17) and (22), we observe that the approx-
imation is actually operated on the (d − 1)-dimensional
domain instead of d-dimensional domain (Cho & Saul,
2009). Generally, the approximation error of Quasi Monte
Carlo methods with N points depends on the dimension of
integration. A lower dimension leads to smaller approx-
imation error, thus the feature maps in equation (21) can
achieve lower approximation error.
The feature maps in equation (21) are closely related to
the bidirectional activation neural network. Speciﬁcally,
the feature maps for the ﬁrst-order arc-cosine kernel are
related to the bidirectional ReLU activation function (An
et al., 2015) which has the distance preservation property
compared with ReLU.
From equation (14) and (21), we know that the feature
maps actually rely on the point set U = [V,−V]. The
design of the point set U will be discussed in section 4.

Spherical Structured Feature Maps for Kernel Approximation

4. Design of Matrix U
We have discussed the construction of SSF maps in last
section. However, one unsolved problem is how to obtain
the matrix U = [V,−V]. We employ the discrete Riesz
s-energy as the objective function to obtain matrix U be-
cause it can generate asymptotically uniformly distributed
points on Sd−1 (Brauchart & Grabner, 2015). Moreover,
to achieve computation and storage efﬁciency for feature
maps construction , we add a structured constraint to the
matrix U. In this section, we show the structure of matrix
U ﬁrst and then the optimization of discrete Riesz s-energy.
It is worth noting that matrix U can be used not only for
kernel approximation, but also for approximation of gen-
eral integrals over hypersphere. Moreover, by using FFT,
matrix U can accelerate the integral approximation which
involves projection operations. In addition, it only needs to
store the indexes with linear storage cost (i.e. O(d)) instead
of to explicitly store the matrix with cost O(N d).

4.1. Structure of Matrix U
Since U can be constructed by V, i.e. U = [V,−V], we
only need to deﬁne structured matrix V. To achieve log-
linear time complexity of SSF maps construction, we con-
struct V by extracting rows from a discrete Fourier matrix.
The complexity analysis of SSF maps construction based
on matrix V is given in section 5.
Mathematically, the construction of matrix V is shown as
follows. Without loss of generality, we assume that d =
2m, N = 2n, m < n. Let F ∈ Cn×n be a n × n discrete
√−1. Let Λ = [k1, k2, ..., km] ⊂ {1, ..., n − 1}
is the (k, j)thentry of F ,
Fourier matrix. Fk,j = e
where i =
be a subset of indexes.
The structured matrix V can be deﬁned as equation (23).

2πikj

n

(cid:20) ReFΛ −ImFΛ

ImFΛ ReFΛ

(cid:21)

where FΛ in equation (24) is the matrix constructed by m
rows of F .

∈ Rd×N

(23)

 ∈ Cm×n

(24)

V = 1√
m

 e

2πik1 1

n

...

2πikm1

n

e

FΛ=

···
...
···

e

2πik1 n

n

...

2πikmn

n

e

With the V given in equation (23), it is easy to verify that
(cid:107)vi(cid:107)2 = 1 for i ∈ {1, ..., n}. Thus, each column of matrix
V is a point on Sd−1.

4.2. Minimize the Discrete Riesz s-energy

With structured matrix V deﬁned in equation (23), our goal
is to select a subset of indexes Λ that optimizes the discrete

2N(cid:80)

2N(cid:80)

i=1

j=1,j(cid:54)=i

Riesz s-energy. Speciﬁcally, we will discuss how to mini-
mize the Riesz 0-energy in equation (25). The other Riesz
s-energy can be optimized in a similar way.

E(U) =

log

1

(cid:107)ui−uj(cid:107)

(25)

where U = [V,−V] = [u1, ..., u2N ].
In the following, we will discuss how to minimize equa-
tion (25) by using a coordinate decent method.
Theorem 4.1: Let U = [V,−V] with V deﬁned in (23),
the discrete Riesz 0-energy of U can be calculated as equa-
tion (26).

(cid:18)

n−1(cid:80)
n−1(cid:80)

p=1

log

p=1

log

(cid:18)

−2n

1 − (Re 1

m

2(cid:19)

m(cid:80)

2πiks p

n

e

)

2(cid:19)

s=1

2πiksp

n

e

)

m

m(cid:80)

s=1

(26)

E(U) = C − 2n

1 − (Im 1

where C is a constant independent of the choice of Λ.
Proof: Since U = [V,−V] ∈ S(d−1)×2N, we obtain
equation (27).

log (cid:107)ui − uj(cid:107)

2N(cid:80)

j=1,j(cid:54)=i

log (cid:107)2vi(cid:107)

i=1

E(U) = − 2N(cid:80)
N(cid:80)
N(cid:80)
N(cid:80)
N(cid:80)
N(cid:80)

= −2
−2

= C − 2

= C − 2

j=1,j(cid:54)=i

i=1

i=1

i=1

N(cid:80)
N(cid:80)

(log (cid:107)vi − vj(cid:107) + log (cid:107)vi + vj(cid:107))
log ((cid:107)vi − vj(cid:107)(cid:107)vi + vj(cid:107))

(cid:16)(cid:112)2 − 2vT

i vj

(cid:112)2 + 2vT

i vj

(cid:17)

j=1,j(cid:54)=i

i=1

j=1,j(cid:54)=i

log

(27)
Recall that N = 2n. By separating the summation term
into two parts (each part has n× n term), we achieve equa-
tion (28).

E(U) = C − 2

log

2

1 − (vT

i vj)2

(cid:18)

(cid:113)

2n(cid:80)
2n(cid:80)
(cid:18)

i=1

log

2

2n(cid:80)
(cid:18)
(cid:113)

log

j=1,j(cid:54)=i

(cid:113)

2
1 − (vT

1 − (vT

i vj)2

(cid:19)

i vj)2

(cid:19)

n(cid:80)
n(cid:80)

= C − 4
−4

n(cid:80)

i=1

j=1,j(cid:54)=i

i=1

j=n+1

(cid:19)

(28)
Let V·,1:n = [v1, ..., vn] and V·,n+1:2n = [vn+1, ..., v2n]
be the matrix consisting of the ﬁrst n and last n columns of
V respectively. We can obtain equation (29).

VT·,1:nV·,n+1:2n = 1

T(−ImFΛ)
m (ImFΛ)T ReFΛ

m ReFΛ
+ 1

(29)

Spherical Structured Feature Maps for Kernel Approximation

Note that all diagonal elements of VT·,1:nV·,n+1:2n are
zero. By further separating the ﬁrst summation term of
equation (28) into two parts, we obtain equation (30).

√

log(cid:0)2
(cid:113)

i=1

j=n+i

n(cid:80)
(cid:18)
2n(cid:80)
(cid:18)

2n(cid:80)
(cid:18)
(cid:113)
(cid:113)

E(U) = C − 4

2n(cid:80)

−4
−4

i=1

j=n+1,j(cid:54)=n+i

log

2
1 − (vT

log

2

i=1

j=1,j(cid:54)=i

= C − 4
−4

i=1

j=n+1,j(cid:54)=n+i

i=1

j=1,j(cid:54)=i

log

2

n(cid:80)
n(cid:80)
n(cid:80)

n(cid:80)
n(cid:80)
n(cid:80)

(cid:18)

i vj)2

(cid:113)

log
2
1 − (vT

i vj)2

(cid:19)

1 − 0(cid:1)
(cid:19)
(cid:19)

1 − (vT

i vj)2

(cid:19)

1 − (vT

i vj)2

(30)

(cid:19)2

(cid:19)2

(cid:18)

(cid:18)

To be concise, let Z = [z1, , ..., zn] = 1√
m FΛ.
For 1 ≤ j ≤ n, j (cid:54)= i, we achieve equation (31).

(vT

i vj)2 = (Rez∗

i zj)2 =

1
m Re

e2πiksp/n

(31)
For n + 1 ≤ j ≤ 2n, , j (cid:54)= n + i, we attain equation (32).

(vT

i vj)2 = (Imz∗

i zj−n)2 =

1
m Im

e2πiksp/n

(32)
(mod n), where

In equation (31) and (32), p ≡ i − j
mod denotes the modulus operation on integers.
Note that z∗
j
equation (33).

i zj has at most n − 1 distinct values when i (cid:54)=
(mod n) . Together with equation (30), we achieve
(cid:19)

(cid:113)

log

2

1 − (vT

i vj)2

2n(cid:80)

m(cid:80)

s=1

m(cid:80)

s=1

(cid:18)
(cid:19)

j=n+1,j(cid:54)=n+i
1 − (vT

2

(cid:18)

i vj)2

(cid:113)

log
2
1 − (Rez∗

(cid:113)
(cid:113)
(cid:115)

j=n+1,j(cid:54)=n+i

i=1

j=1,j(cid:54)=i

log

2

i=1

n(cid:80)
(cid:18)
2n(cid:80)
(cid:18)
(cid:32)
(cid:115)
(cid:18)

E(U) = C − 4
−4

i=1

j=1,j(cid:54)=i

log

n(cid:80)
n(cid:80)

= C − 4
−4

p=1

n−1(cid:80)
n−1(cid:80)

p=1

−4n

−2n

i=1

n(cid:80)
n(cid:80)
n(cid:80)
n−1(cid:80)
(cid:32)
n−1(cid:80)
(cid:18)

p=1

p=1

log

log

2

1 − (Re 1

m

= C − 2n

log

1 − (Im 1

m

m(cid:80)

s=1

1 − (Re 1

m

e2πiksp/n)

1 − (Imz∗

i zj−n)2

(cid:19)

2(cid:33)

i zj)2

m(cid:80)

s=1

2(cid:33)
2(cid:19)

m

m(cid:80)
m(cid:80)

s=1

s=1

e2πiksp/n)

e2πiksp/n)

2(cid:19)

= C − 4n

log

2

1 − (Im 1

e2πiksp/n)

Algorithm 1

Initialization: random sample Λ = [k1, k2, ..., km] from

{1, 2, ...n − 1} without replacement. Set(cid:101)h = 1T FΛ

repeat

Set g = [e2πikq/n, e2πikq2/n..., e2πikq(n−1)/n]

q = arg max
kq∈{1,...,n−1}
q /n, e2πik∗

J(kq) in (35)
q 2/n..., e2πik∗

q (n−1)/n]

Set J = J(Λ)
for q = 1 to m do

Set h =(cid:101)h − g
Set(cid:101)h = h + g

Find k∗
Update g = [e2πik∗

q by k∗

end for

until J does not change

From Theorem 4.1, we know that minimizing E(U) is
equivalent to maximizing J(Λ) which is deﬁned in equa-
tion (34).

(cid:18)

n−1(cid:80)
(cid:18)

p=1

J(Λ) =

log

n−1(cid:80)

p=1

1 − (Im 1

m

m(cid:80)

s=1

+

log

1 − (Re 1

m

e2πiksp/n)

2(cid:19)

m(cid:80)

s=1

e2πiksp/n)

2(cid:19)

(34)

By keeping all the indexes in Λ = [k1, k2, ..., km] ﬁxed
except the qth element, we can obtain equation (35).

(cid:16)

n−1(cid:80)
(cid:16)
1 − (Re(cid:0)hp + e2πikqp/n(cid:1) /m)

2(cid:17)
1 − (Im(cid:0)hp + e2πikqp/n(cid:1) /m)
2(cid:17)

log

p=1

J(kq) =

n−1(cid:80)

+

log

p=1

where kq ∈ {1, 2, ...n − 1}, hp =

m(cid:80)

s=1,s(cid:54)=q

(35)

e2πiksp/n.

(cid:19)

With equation (35), we can maximize J(Λ) by maximiz-
ing J(kq) with other indexes ﬁxed each time. Let h =
[h1, ..., hn−1] , g = [e2πikq/n, e2πikq2/n..., e2πikq(n−1)/n].
1 = [1, ..., 1]T ∈ Rm is the vector of all ones. A coordinate
ascent method to maximize J(Λ) is given in Algorithm 1.
Obviously, it is a discrete optimization problem. Algo-
rithm 1 can ﬁnd a local optimum. The time complexity of
the Algorithm 1 is O(T mn2), where T denotes the number
of outer iteration. Empirically, the outer iteration T is less
than ten.

5. Fast Feature Maps Construction
In this section, we will discuss how to construct SSF maps
in loglinear time complexity and linear space complexity
by using the structure property of V.
Theorem 5.1 Assume that d = 2m, N = 2n, m < n. Let

(33)
(cid:3)

Spherical Structured Feature Maps for Kernel Approximation

culant (Choromanski & Sindhwani, 2016) matrices, QMC
with Halton set and QMC with Sobol set (Avron et al.,
2016). For Halton set and Sobol set, the implementation
in MATLAB are employed in the experiments. The scram-
bling and shifting techniques are used for Haltonset and
Sobolset. In all the experiments, we ﬁx M = 1 (the num-
ber of one-dimensional QMC points) for SSF maps.

Figure 1. Convergence of the Logarithmic Energy

(cid:21)

(37)

(cid:3)

(cid:20) x1

(cid:21)

x2

∈ R2m and z = x1 + ix2 ∈ Cm. Given
x =
Λ = [k1, k2, ..., km] ⊂ {1, ..., n − 1} , let y ∈ Cn with
yΛ = z. Other elements outside the index set Λ are equal
to zero. Given V deﬁned in equation (23), equation (36)
holds.

VT x = 1√

m [Re(F ∗y), Im(F ∗y)]T

(36)
Proof: Let Ω ∈ Rn×n be a diagonal matrix with all diago-
nal elements inside the index set Λ equal to one , the others
equal to zero.

(cid:21)T(cid:20) x1

(cid:21)

T )x1 + (ImFΛ
T )x1 + (ReFΛ

x2
T )x2
T )x2

VT x = 1√
m

(cid:20) ReFΛ −ImFΛ
(cid:20) (ReFΛ
(cid:21)
(cid:20) Re(F ∗
(cid:20) Re(F ∗Ωy)
(cid:20) Re(F ∗y)
(cid:21)

ImFΛ ReFΛ
(−ImFΛ
Λz)
Im(F ∗
Λz)
Im(F ∗Ωy)
Im(F ∗y)

(cid:21)

= 1√
m

= 1√
m
= 1√
m
= 1√
m

Thus, the projection operation VT x (previously mentioned
in equation (10) and (21)) can be calculated by Fast Fourier
Transform algorithm (FFT) in O(n log n) time complexity.
Because scaling and taking nonlinear transform can be ﬁn-
ished in O(n), the total time complexity to construct SSF
maps is O(n log n).
All steps to construct SSF maps are summarized as follows:

a diagonal matrix where diagonal elements are uniformly
sampled from {−1, +1}.

(a) Compute(cid:101)x by(cid:101)x = Dx, where D ∈ {−1, +1}d×d is
(b) Construct y such that yΛ =(cid:101)x1 + i(cid:101)x2, other elements
(c) Compute VT(cid:101)x by equation (36) via FFT.

outside the index set Λ are equal to zero.

(d) Construct feature maps Ψ (x) via equation (10) or (21).
For each (m, n) pair , the index set Λ only need to be com-
puted once.
It takes O(m) space to store Λ. For shift
and rotation invariant kernels, it takes O(M ) space to store
Φ−(tj), j ∈ 1, ..., M and takes O(d) (d = 2m) space to
store Λ and D. For bth-order arc-cosine kernels, it only
needs to store one parameter Cb and takes O(d) space to
store Λ and D. By setting M ≤ d, the total space com-
plexity to store the projection matrix is O(d).

6. Empirical Studies
We compare SSF maps with feature maps obtained by fully
Gaussian (Cho & Saul, 2009; Rahimi et al., 2007), the Cir-

Figure 2. Speedup of the Feature Maps Construction

6.1. Convergence and Speedup
First, the convergence of the logarithmic energy ( −J(Λ)
in equation (34)) with (m, n) = (160, 1600) is shown in
Figure 1. From Figure 1, we ﬁnd that it takes less than
ten iterations (i.e. T < 10) for Algorithm 1 to ﬁnd a local
optimum.
Second, the speedup results of all methods are shown in

Spherical Structured Feature Maps for Kernel Approximation

(a) (cid:107)(cid:102)K−K(cid:107)F

(cid:107)K(cid:107)F

for Gaussian Kernel

(b) (cid:107)(cid:102)K−K(cid:107)F

(cid:107)K(cid:107)F

for Zero-order Arc Kernel

(c) (cid:107)(cid:102)K−K(cid:107)F

(cid:107)K(cid:107)F

for First-order Arc Kernel

(d) (cid:107)(cid:102)K−K(cid:107)∞

(cid:107)K(cid:107)∞ for Gaussian Kernel

(e) (cid:107)(cid:102)K−K(cid:107)∞

(cid:107)K(cid:107)∞ for Zero-order Arc Kernel

(f) (cid:107)(cid:102)K−K(cid:107)∞

(cid:107)K(cid:107)∞ for First-order Arc Kernel

Figure 3. Relative Mean and Max Reconstruction Error for Gaussian, Zero-order and First-order Arc-cosine Kernel on MNIST

Figure 2. We set N = 2d for all the methods. The speedup
of fully Gaussian projection is the baseline. We can observe
that the speedup of QMC with Halton set is constant as the
dimension d increases and is slower than the baseline. The
speedup of both SSF maps and the Circulant increase fast
as dimension increases, which is consistent with theoretical
analysis. The speedup of Sobol set is not shown because
the inbuilt Sobolset routine of MATLAB does not support
dimension larger than 1,111.

6.2. Approximation Accuracy

We evaluate reconstruction error of Gaussian kernel, zero-
order arc-cosine kernel and ﬁrst-order arc-cosine kernel on
CIFAR10 (Krizhevsky & Hinton, 2009), MNIST (LeCun
& Cortes, 2010), usps and dna dataset. MNIST is a hand-
written digit image dataset, which contains 70,000 samples
with 784-dimensional features(pixel). For CIFAR10 with
60,000 samples, the 320-dimensional gist feature (Gong
et al., 2013) are employed in the experiments. Both the
relative Frobenius error (i.e.
) and the relative

(cid:107)(cid:101)K−K(cid:107)F

(cid:107)K(cid:107)F

(cid:107)(cid:101)K−K(cid:107)∞

(cid:107)K(cid:107)∞

element-wise maximum error (i.e.

ated, where K and (cid:101)K denote the exact and approximated
(cid:113)(cid:80)
Gram matrices respectively. The Frobenius norm and
the elementwise maximum norm are deﬁned as (cid:107)X(cid:107)F =

j |Xij|2 and (cid:107)X(cid:107)∞ = max

|Xij| respectively.

) are evalu-

(cid:80)

i

i,j

The reconstruction error in the experiments is the mean
value over 10 independent runs. The dimensions of the fea-
ture maps are set to {2×d, 3×d, 4×d, 5×d}, where d is the
dimension of the data. For MNIST and CIFAR10 dataset,
each run randomly select 2,000 samples to construct the
Gram matrix. The mean value of the reconstruction errors
with different norms on MNIST are shown in Figure 3. Re-
sults on the other datasets are similar to that of Figure 3.
One can refer to the supplementary material for results on
other datasets.
Figure 3 shows that the feature maps obtained with fully
Gaussian matrix, the Circulant matrix, QMC with Halton
set and QMC with Sobol set have similar reconstruction
error. SSF maps have the smallest approximation error
among ﬁve methods. Especially for the ﬁrst-order arc-
cosine kernel, it achieves nearly one-ﬁfth relative mean er-
ror and one-seventh relative max error of other methods.
Moreover, even if M = 1, SSF maps can achieve about
one-third relative mean error and half of the relative max
error of other methods for Gaussian Kernel approximation.

7. Conclusion
We propose Spherical Structured Feature (SSF) maps to ap-
proximate shift and rotation invariant kernels as well as bth-
order arc-cosine kernels. SSF maps can achieve computa-
tion and storage efﬁciency as well as better approximation
accuracy.

Spherical Structured Feature Maps for Kernel Approximation

Acknowledgements
We thank the anonymous reviewers for their valuable com-
ments and suggestions.

References
An, Senjian, Boussaid, Farid, and Bennamoun, Mo-
hammed. How can deep rectiﬁer networks achieve linear
separability and preserve distances? In ICML, pp. 514–
523, 2015.

Avron, Haim, Sindhwani, Vikas, Yang, Jiyan, and Ma-
honey, Michael W. Quasi-monte carlo feature maps for
shift-invariant kernels. Journal of Machine Learning Re-
search, 17(120):1–38, 2016.

Brauchart, J, Saff, E, Sloan, I, and Womersley, R. Qmc
designs: optimal order quasi monte carlo integration
schemes on the sphere. Mathematics of computation, 83
(290):2821–2851, 2014.

Brauchart, Johann S and Grabner, Peter J. Distributing
many points on spheres: minimal energy and designs.
Journal of Complexity, 31(3):293–326, 2015.

Chang, Chih-Chung and Lin, Chih-Jen. Libsvm: a library
for support vector machines. ACM Transactions on In-
telligent Systems and Technology (TIST), 2(3):27, 2011.

Cho, Youngmin and Saul, Lawrence K. Kernel methods
In Advances in neural information

for deep learning.
processing systems, pp. 342–350, 2009.

Choromanski, Krzysztof and Sindhwani, Vikas. Recycling
randomness with structure for sublinear time kernel ex-
pansions. 2016.

Cutajar, Kurt, Bonilla, Edwin V, Michiardi, Pietro, and Fil-
ippone, Maurizio. Practical learning of deep gaussian
processes via random fourier features. arXiv preprint
arXiv:1610.04386, 2016.

Dick, Josef, Kuo, Frances Y, and Sloan, Ian H. High-
the quasi-monte carlo way.

dimensional integration:
Acta Numerica, 22:133–288, 2013.

Fan, Rong-En, Chang, Kai-Wei, Hsieh, Cho-Jui, Wang,
Xiang-Rui, and Lin, Chih-Jen. Liblinear: A library for
large linear classiﬁcation. Journal of machine learning
research, 9(Aug):1871–1874, 2008.

Feng, Chang, Hu, Qinghua, and Liao, Shizhong. Random
feature mapping with signed circulant matrix projection.
In IJCAI, pp. 3490–3496, 2015.

Gong, Yunchao, Lazebnik, Svetlana, Gordo, Albert, and
Perronnin, Florent. Iterative quantization: A procrustean

approach to learning binary codes for large-scale image
IEEE Transactions on Pattern Analysis and
retrieval.
Machine Intelligence, 35(12):2916–2929, 2013.

G¨otz, Mario. On the riesz energy of measures. Journal of

Approximation Theory, 122(1):62–78, 2003.

Krizhevsky, Alex and Hinton, Geoffrey. Learning multiple

layers of features from tiny images. 2009.

Le, Quoc, Sarl´os, Tam´as, and Smola, Alex. Fastfood-
approximating kernel expansions in loglinear time.
In
Proceedings of the international conference on machine
learning, 2013.

Le, Zichao Yang Alexander J Smola and Wilson, Song An-
drew Gordon. A la cartelearning fast kernels. 38, 2015.

LeCun, Yann and Cortes, Corinna. MNIST handwritten
digit database. 2010. URL http://yann.lecun.
com/exdb/mnist/.

Oliva, Junier B, Dubey, Avinava, Poczos, Barnabas,
Schneider, Jeff, and Xing, Eric P. Bayesian nonpara-
metric kernel-learning. In Proceedings of the 19th Inter-
national Conference on Artiﬁcial Intelligence and Statis-
tics, pp. 1078–1086, 2016.

Rahimi, Ali and Recht, Benjamin. Weighted sums of ran-
dom kitchen sinks: Replacing minimization with ran-
domization in learning. In Advances in neural informa-
tion processing systems, pp. 1313–1320, 2009.

Rahimi, Ali, Recht, Benjamin, et al. Random features for
large-scale kernel machines. In NIPS, volume 3, pp. 5,
2007.

Rasmussen, Carl Edward. Gaussian processes for machine

learning. 2006.

Rudin, Walter. Fourier analysis on groups. John Wiley &

Sons, 2011.

Snoek, Jasper, Larochelle, Hugo, and Adams, Ryan P.
Practical bayesian optimization of machine learning al-
gorithms. In Advances in neural information processing
systems, pp. 2951–2959, 2012.

Srinivas, Niranjan, Krause, Andreas, Kakade, Sham M, and
Seeger, Matthias. Gaussian process optimization in the
bandit setting: No regret and experimental design. arXiv
preprint arXiv:0912.3995, 2009.

Sutherland, Dougal J and Schneider, Jeff.

On the
arXiv preprint

error of
random fourier
arXiv:1506.02785, 2015.

features.

Yang, J., Sindhwani. V. Avron H. Mahoney M. Quasi-
monte carlo feature maps for shift-invariant kernels.
ICML, 32, 2014.

