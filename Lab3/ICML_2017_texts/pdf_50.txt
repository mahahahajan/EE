Efﬁcient Online Bandit Multiclass Learning with ˜O(√T ) Regret

Alina Beygelzimer 1 Francesco Orabona 2 Chicheng Zhang 3

Abstract

We present an efﬁcient second-order algorithm
√T )1 regret for the bandit online mul-
with ˜O( 1
η
ticlass problem. The regret bound holds simulta-
neously with respect to a family of loss functions
parameterized by η, for a range of η restricted
by the norm of the competitor. The family of
loss functions ranges from hinge loss (η = 0)
to squared hinge loss (η = 1). This provides a
solution to the open problem of (Abernethy, J.
and Rakhlin, A. An efﬁcient bandit algorithm
for √T -regret in online multiclass prediction?
In COLT, 2009). We test our algorithm experi-
mentally, showing that it also performs favorably
against earlier algorithms.

1. Introduction
In the online multiclass classiﬁcation problem, the learner
must repeatedly classify examples into one of k classes. At
each step t, the learner observes an example xt ∈ Rd and
In the full-information case,
predicts its label ˜yt ∈ [k].
the learner observes the true label yt ∈ [k] and incurs loss
1[˜yt �= yt]. In the bandit version of this problem, ﬁrst con-
sidered in (Kakade et al., 2008), the learner only observes
its incurred loss 1[˜yt �= yt], i.e., whether or not its pre-
diction was correct. Bandit multiclass learning is a special
case of the general contextual bandit learning (Langford &
Zhang, 2008) where exactly one of the losses is 0 and all
other losses are 1 in every round.
The goal of the learner is to minimize its regret with re-
spect to the best predictor in some reference class of pre-
dictors, that is the difference between the total number of
mistakes the learner makes and the total number of mis-
1Yahoo Research, New York, NY 2Stony Brook University,
Stony Brook, NY 3University of California, San Diego, La Jolla,
CA. Correspondence to: Alina Beygelzimer <beygel@yahoo-
inc.com>, Francesco Orabona <francesco@orabona.com>,
Chicheng Zhang <chichengzhang@ucsd.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

1 ˜O(·) hides logarithmic factors.

takes of the best predictor in the class. Kakade et al. (2008)
proposed a bandit modiﬁcation of the Multiclass Percep-
tron algorithm (Duda & Hart, 1973), called the Banditron,
that uses a reference class of linear predictors. Note that
even in the full-information setting, it is difﬁcult to provide
a true regret bound. Instead, performance bounds are typ-
ically expressed in terms of the total multiclass hinge loss
of the best linear predictor, a tight upper bound on 0-1 loss.
The Banditron, while computationally efﬁcient, achieves
only O(T 2/3) expected regret with respect to this loss,
where T is the number of rounds. This is suboptimal as the
Exp4 algorithm of Auer et al. (2003) can achieve ˜O(√T )
regret for the 0-1 loss, albeit very inefﬁciently. Abernethy
& Rakhlin (2009) posed an open problem: Is there an ef-
ﬁcient bandit multiclass learning algorithm that achieves
expected regret of ˜O(√T ) with respect to any reasonable
loss function?
The ﬁrst attempt to solve this open problem was by Cram-
mer & Gentile (2013). Using a stochastic assumption about
the mechanism generating the labels, they were able to
show a ˜O(√T ) regret, with a second-order algorithm.
Later, Hazan & Kale (2011), following a suggestion by
Abernethy & Rakhlin (2009), proposed the use of the log-
loss coupled with a softmax prediction. The softmax de-
pends on a parameter that controls the smoothing factor.
The value of this parameter determines the exp-concavity
of the loss, allowing Hazan & Kale (2011) to prove
worst-case regret bounds that range between O(log T ) and
3 ), again with a second-order algorithm. However, the
O(T
choice of the smoothing factor in the loss becomes critical
in obtaining strong bounds.
The original Banditron algorithm has been also extended
in many ways. Wang et al. (2010) have proposed a variant
based on the exponentiated gradient algorithm (Kivinen &
Warmuth, 1997). Valizadegan et al. (2011) proposed dif-
ferent strategies to adapt the exploration rate to the data in
the Banditron algorithm. However, these algorithms suffer
from the same theoretical shortcomings as the Banditron.
There has been signiﬁcant recent focus on developing ef-
ﬁcient algorithms for the general contextual bandit prob-
lem (Dud´ık et al., 2011; Agarwal et al., 2014; Rakhlin &
Sridharan, 2016; Syrgkanis et al., 2016a;b). While solving

2

Efﬁcient Online Bandit Multiclass Learning

a more general problem that does not make assumptions
on the structure of the reward vector or the policy class,
these results assume that contexts or context/reward pairs
are generated i.i.d., or the contexts to arrive are known be-
forehand, which we do not assume here.
In this paper, we follow a different route. Instead of design-
ing an ad-hoc loss function that allows us to prove strong
guarantees, we propose an algorithm that simultaneously
satisﬁes a regret bound with respect to all the loss functions
in a family of functions that are tight upper bounds to the 0-
1 loss. The algorithm, named Second Order Banditron Al-
gorithm (SOBA), is efﬁcient and based on the second-order
Perceptron algorithm (Cesa-Bianchi et al., 2005). The re-
gret bound is of the order of ˜O(√T ), providing a solution
to the open problem of Abernethy & Rakhlin (2009).

2. Deﬁnitions and Settings
We ﬁrst introduce our notation. Denote the rows of a matrix
V ∈ Rk×d by v1, v2, . . . , vk. The vectorization of V is
deﬁned as vec(V ) = [v1, v2, . . . , vk]T , which is a vector
in Rkd. We deﬁne the reverse operation of reshaping a kd×
1 vector into a k× d matrix by mat(V ), using a row-major
order. To simplify notation, we will use V and vec(V )
interchangeably throughout the paper. For matrices A and
B, denote by A⊗B their Kronecker product. For matrices
X and Y of the same dimension, denote by �X, Y � =
�i,j Xi,jYi,j their inner product. We use � · � to denote
the �2 norm of a vector, and � · �F to denote the Frobenius
norm of a matrix. For a positive deﬁnite matrix A, we use
�x�A =��x , Ax� to denote the Mahalanobis norm of x
with respect to A. We use 1k to denote the vector in Rk
whose entries are all 1s.
We use Et−1[·] to denote the conditional expectation given
the observations up to time t − 1 and xt, yt, that is, x1, y1,
˜y1, . . . , xt−1, yt−1, ˜yt−1, xt, yt.
Let [k] denote {1, . . . , k}, the set of possible labels. In our
setting, learning proceeds in rounds:
For t = 1, 2, . . . , T :

learner, and commits to a hidden label yt ∈ [k].
Δk−1 is a probability distribution over [k].

1. The adversary presents an example xt ∈ Rd to the
2. The learner predicts a label ˜yt ∼ pt, where pt ∈
3. The learner receives the bandit feedback 1[˜yt �= yt].
The goal of the learner is to minimize the total number of
mistakes, MT =�T
We will use linear predictors speciﬁed by a matrix
W ∈ Rk×d. The prediction is given by W (x) =
arg maxi∈[k](W x)i, where (W x)i is the ith element of

1[˜yt �= yt].

t=1

the vector W x, corresponding to class i.
A useful notion to measure the performance of a competitor
U ∈ Rk×d is the multiclass hinge loss

�(U , (x, y)) := max
i�=y

[1 − (U x)y + (U x)i]+,

(1)

where [·]+ = max(·, 0).
3. A History of Loss Functions
As outlined in the introduction, a critical choice in obtain-
ing strong theoretical guarantees is the choice of the loss
function. In this section we introduce and motivate a fam-
ily of multiclass loss functions.
In the full information setting, strong binary and multiclass
mistake bounds are obtained through the use of the Percep-
tron algorithm (Rosenblatt, 1958). A common misunder-
standing of the Perceptron algorithm is that it corresponds
to a gradient descent procedure with respect to the (binary
or multiclass) hinge loss. However, it is well known that
the Perceptron simultaneously satisﬁes mistake bounds that
depend on the cumulative hinge loss and also on the cumu-
lative squared hinge loss, see for example Mohri & Ros-
tamizadeh (2013). Note also that the squared hinge loss is
not dominated by the hinge loss, so, depending on the data,
one loss can be better than the other.
We show that the Perceptron algorithm satisﬁes an even
stronger mistake bound with respect to the cumulative loss
of any power of the multiclass hinge loss between 1 and 2.
Theorem 1. On any sequence (x1, y1), . . . , (xT , yT ) with
�xt� ≤ X for all t ∈ [T ], and any linear predictor U ∈
Rk×d, the total number of mistakes MT of the multiclass
Perceptron satisﬁes, for any q ∈ [1, 2],

MT ≤ M

1− 1
T L

q

1
q

MH,q(U ) + �U�F X√2�MT ,

it simultaneously satisﬁes the following:

t=1 �(W , (xt, yt))q. In particular,

F + X�U�F√2�LMH,1(U )
F +X�U�F 2√2�LMH,2(U ) .

where LMH,q(U ) = �T
MT ≤ LMH,1(U ) + 2X 2�U�2
MT ≤ LMH,2(U )+2X 2�U�2
For the proof, see Appendix B.
A similar observation was done by Orabona et al. (2012)
who proved a logarithmic mistake bound with respect to
all loss functions in a similar family of functions smoothly
interpolating between the hinge loss to the squared hinge
loss.
In particular, Orabona et al. (2012) introduced the
following family of binary loss functions

�η(x) :=�1 − 2

0,

2−η x + η

2−η x2, x ≤ 1
x > 1 .

(2)

Efﬁcient Online Bandit Multiclass Learning

Algorithm 1 Second Order Banditron Algorithm (SOBA)
Input: Regularization parameter a > 0, exploration pa-

rameter γ ∈ [0, 1].

1k

Receive instance xt ∈ Rd
ˆyt = arg maxi∈[k](Wtxt)i
Deﬁne pt = (1 − γ)eˆyt + γ
Randomly sample ˜yt according to pt
Receive bandit feedback 1[˜yt �= yt]
Initialize update indicator nt = 0
if ˜yt = yt then

1: Initialization: W1 = 0, A0 = aI, θ0 = 0
2: for t = 1, 2, . . . , T do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:

¯yt = arg maxi∈[k]\{yt}(Wtxt)i
gt = 1
(e¯yt − eyt ) ⊗ xt
pt,yt
zt = √pt,yt gt
mt = �Wt , zt�2+2�Wt , gt�

k

Figure 1. Plot of the loss functions in �η for different values of η.

where 0 ≤ η ≤ 1. Note that η = 0 recovers the bi-
nary hinge loss, and η = 1 recovers the squared hinge
loss. Meanwhile, for any 0 ≤ η ≤ 1, �η(x) ≤
max{�0(x), �1(x)}, and �η is an upper bound on 0-1 loss:
1[x < 0] ≤ �η(x). See Figure 1 for a plot of the different
functions in the family.
Here, we deﬁne a multiclass version of the loss in (2) as

�η(U , (x, y)) := �η�(U x)y − max

i�=y

(U x)i� .

(3)

Hence, �0(U , (x, y)) = �(U , (x, y)) is the classical mul-
ticlass hinge loss and �1(U , (x, y)) = �2(U , (x, y)) is the
squared multiclass hinge loss.
√T ) regret bound that holds si-
Our algorithm has a ˜O( 1
η
multaneously for all loss functions in this family, with η in
a range that ensure that (U x)i − (U x)j ≤ 2−η
η , i, j ∈ [k].
We also show that there exists a setting of the parame-
ters of the algorithm that gives a mistake upper bound of
˜O((L∗T )1/3 +√T ), where L∗ is the cumulative hinge loss
of the competitor, which is never worse that the best bounds
in Kakade et al. (2008).

4. Second Order Banditron Algorithm
This section introduces our algorithm for bandit multiclass
online learning, called Second Order Banditron Algorithm
(SOBA), described in Algorithm 1.
SOBA makes a prediction using the γ-greedy strategy: At
each iteration t, with probability 1 − γ, it predicts ˆyt =
arg maxi∈[k](Wtxt)i; with the remaining probability γ, it
selects a random action in [k]. As discussed in Kakade
et al. (2008), randomization is essential for designing ban-
dit multiclass learning algorithms. If we deterministically
output a label and make a mistake, then it is hard to make an
update since we do not know the identity of yt. However,

1+zT

t A−1

t−1zt

end if

if mt +�t−1

s=1 nsms ≥ 0 then

Turn on update indicator nt = 1

end if
Update At = At−1 + ntztzT
Update θt = θt−1 − ntgt
Set Wt+1 = mat(A−1
t θt)

14:
15:
16:
17:
18:
19:
20:
21: end for
Remark: matrix At is of dimension kd × kd, and vector
θt is of dimension kd; in line 20, the matrix multiplication
results in a kd dimensional vector, which is reshaped to
matrix Wt+1 of dimension k × d.

t

s=1 nszszT

Perceptron-style updates θt = −�t
corrected covariance matrix At = aI +�t

if randomization is used, we can estimate yt and perform
online stochastic mirror descent type updates (Bubeck &
Cesa-Bianchi, 2012).
SOBA keeps track of two model parameters: cumulative
s=1 nsgs ∈ Rkd and
s ∈
Rkd×kd. The classiﬁer Wt is computed by matricizing over
the matrix-vector product A−1
t−1θt−1 ∈ Rkd. The weight
vector θt is standard in designing online mirror descent
type algorithms (Shalev-Shwartz, 2011; Bubeck & Cesa-
Bianchi, 2012). The matrix At is standard in designing on-
line learning algorithms with adaptive regularization (Cesa-
Bianchi et al., 2005; Crammer et al., 2009; McMahan &
Streeter, 2010; Duchi et al., 2011; Orabona et al., 2015).
The algorithm updates its model (nt = 1) only when the
following conditions hold simultaneously: (1) the predicted
label is correct (˜yt = yt), and (2) the “cumulative regular-
s=1 nsms +mt) is positive if this
update were performed. Note that when the predicted label
is correct we know the identity of the true label.
As we shall see, the set of iterations where nt = 1 in-
cludes all iterations where ˜yt = yt �= ˆyt. This fact is cru-

ized negative margin” (�t−1

Efﬁcient Online Bandit Multiclass Learning

cial to the mistake bound analysis. Furthermore, there are
some iterations where ˜yt = yt = ˆyt but we still make an
update. This idea is related to “online passive-aggressive
algorithms” (Crammer et al., 2006; 2009) in the full infor-
mation setting, where the algorithm makes an update even
when it predicts correctly but the margin is too small.
Let’s now describe our algorithm more in details. Through-
out, suppose all the examples are �2-bounded: �xt�2 ≤ X.
As outlined above, we associate a time-varying regularizer
s is
Rt(W ) = 1
2�W�2
a kd × kd matrix and

At, where At = aI +�t

s=1 nszszT

zt = √pt,yt gt =

1

√pt,yt

(e¯yt − eyt ) ⊗ xt .

Note that this time-varying regularizer is constructed by
scaled versions of the updates gt. This is critical, because
in expectation this becomes the correct regularizer. Indeed,
it is easy to verify that, for any U ∈ Rk×d,
Et−1[1[yt = ˜yt] gt] = (e¯yt − eyt ) ⊗ xt,
Et−1[1[yt = ˜yt] �U , zt�2] = �U , (e¯yt − eyt ) ⊗ xt�2 .
In words, this means that in expectation the regularizer con-
tains the outer products of the updates, that in turn promote
the correct class and demotes the wrong one. We stress
that it is impossible to get the same result with the estima-
tor proposed in Kakade et al. (2008). Also, the analysis
is substantially different from the Online Newton Step ap-
proach (Hazan et al., 2007) used in Hazan & Kale (2011).
In reality, we do not make an update in all iterations in
which ˜yt = yt, since the algorithm need to maintain the
s=1 msns ≥ 0, which is crucial to the
proof of Lemma 2. Instead, we prove a technical lemma
that gives an explicit form on the expected update ntgt and
expected regularization ntztzT

invariant that �t

t . Deﬁne

qt := 1�t−1�s=1

nsms + mt ≥ 0� ,

ht := 1[ˆyt �= yt] + qt1[ˆyt = yt] .

Lemma 1. For any U ∈ Rkd,
Et−1 [nt �U , gt�] = ht �U , (eyt − e¯yt ) ⊗ xt� ,
Et−1�nt �U , zt�2� = ht �U , (eyt − e¯yt ) ⊗ xt�2 .

The proof of Lemma 1 is deferred to the end of Subsec-
tion 4.1.
Our last contribution is to show how our second order al-
gorithm satisﬁes a mistake bound for an entire family of

loss functions. Finally, we relate the performance of the
algorithm that predicts ˆyt to the γ-greedy algorithm.
Putting all together, we have our expected mistake bound
for SOBA. 2
Theorem 2. SOBA has the following expected upper bound
on the number of mistakes, MT , for any U ∈ Rk×d and any
0 < η ≤ min(1,

2 maxi �ui�X+1 ),

2

E [MT ] ≤ Lη(U ) +
k

+

γη(2 − η)

aη

F

2 − η�U�2
T�t=1
E�zT

t A−1

t zt� + γT,

t=1 �η(U , (xt, yt)) is the cumulative
i=1 are rows of

where Lη(U ) := �T
η-loss of the linear predictor U, and {ui}k
U.
In particular, setting γ = O(� k2 d ln T
E [MT ] ≤ Lη(U ) + O�X 2�U�2

have

F +

k
η

T

) and a = X 2, we

√dT ln T� .

Note that, differently from previous analyses (Kakade
et al., 2008; Crammer & Gentile, 2013; Hazan & Kale,
2011), we do not need to assume a bound on the norm of
the competitor, as in the full information Perceptron and
Second Order Perceptron algorithms. In Appendix A, we
also present an adaptive variant of SOBA that sets explo-
ration rate γt dynamically, which achieves a regret bound
within a constant factor of that using optimal tuning of γ.
We prove Theorem 2 in the next Subsection, while in Sub-
section 4.2 we prove a mistake bound with respect to the
hinge loss, that is not fully covered by Theorem 2.

4.1. Proof of Theorem 2
Throughout the proofs, U, Wt, gt, and zt’s should be
thought of as kd × 1 vectors. We ﬁrst show the follow-
ing lemma. Note that this is a statement over any sequence
and no expectation is taken.
Lemma 2. For any U ∈ Rkd, with the notation of Algo-
rithm 1, we have:

T�t=1

nt�2�U , −gt� − �U , zt�2�
≤ a�U�2

t A−1

ntgT

F +

t gt .

T�t=1

2Throughout the paper, expectations are taken with respect to

the randomization of the algorithm.

Efﬁcient Online Bandit Multiclass Learning

Proof. First, from line 14 of Algorithm 1, it can be seen
(by induction) that SOBA maintains the invariant that

Proof. Using Lemma 2 with ηU, we get that

t�s=1

nsms ≥ 0.

(4)

T�t=1

nt�2η �U , −gt� − η2 �U , zt�2�
≤ aη2�U�2

t A−1

t gt .

ntgT

F +

T�t=1

s=1 nszszT

We next reduce the proof to the regret analysis of online
least squares problem. For iterations where nt = 1, de-
so that gt = αtzt. From the algorithm,
ﬁne αt = 1√pt,yt
At = aI +�t
s , and Wt is the ridge regres-
sion solution based on data collected in time 1 to t − 1, i.e.
Wt = A−1
s=1 nsαszs).
By per-step analysis in online least squares, (see, e.g.,
Orabona et al., 2012)(See Lemma 6 for a proof), we have
that if an update is made at iteration t, i.e. nt = 1, then

t−1(−�t−1

t−1(−�t−1

s=1 nsgs) = A−1

1
2

t A−1
(�Wt , zt� + αt)2(1 − zT
1
2�U − Wt+1�2
≤
At−1 −

1
2�U − Wt�2

t zt) −

(�U , zt� + αt)2
At .

1
2

Otherwise nt = 0, in which case we have Wt+1 = Wt
and At+1 = At.
Denoting by kt = 1 − zT
formula, kt =
1+zT
t−1zt
[T ] such that nt = 1,

t A−1
t zt, by Sherman-Morrison
. Summing over all rounds t ∈

t A−1

1

1
2

T�t=1

≤

nt�(�Wt , zt� + αt)2kt − (�U , zt� + αt)2�
a
1
2�U�2
2�U�2
F .

1
2�U − WT +1�2

AT ≤

A0 −

We also have by deﬁnition of mt,

t zt .

we have the stated bound.

t A−1
t zT
t=1 ntmt ≥ 0,

(�Wt , zt� + αt)2kt − (�U , zt� + αt)2
= mt − 2�U , gt� − �U , zt�2 − α2
Putting all together and using the fact that�T
diction ˆyt, deﬁned as ˆMT :=�T
1[ˆyt �= yt].
Theorem 3. For any U ∈ Rk×d, and any 0 < η ≤
2 maxi �ui�X+1 ), the expected number of mistakes
min(1,
committed by ˆyt can be bounded as

We can now prove the following mistake bound for the pre-

t=1

2

E� ˆMT� ≤ Lη(U ) +

≤ Lη(U ) +

where Lη(U ) :=�T

linear predictor U.

F

aη�U�2
2 − η
aη�U�2
2 − η

F

+

+

t zt]

t A−1

t=1 E[ntzT
γη(2 − η)

k�T
a d k �
dk2 ln�1 + 2T X 2

γη(2 − η)

,

t=1 �η(U , (xt, yt)) is the η-loss of the

Taking expectations, using Lemma 1 and the fact that
pt,yt ≤ k

γ and that At is positive deﬁnite, we have

1

0 ≤ −E� T�t=1
+ E� T�t=1

ht · 2η �U , (eyt − e¯yt ) ⊗ xt��
ht · η2(�U , (eyt − e¯yt ) ⊗ xt�)2�

(5)

k
γ

F +

t zt] .

t A−1

E[ntzT

+ aη2�U�2

vide both sides by η(2 − η), to have

T�t=1
t=1 ht� to both sides and di-
Add the terms η(2 − η)E��T
htf (�U , (eyt − e¯yt ) ⊗ xt�)�
E� T�t=1

ht� ≤ E� T�t=1
2 − η�U�2
2−η z + η
2−η z2. Taking a close look
where f (z) := 1 − 2
at the function f, we observe that the two roots of the
quadratic function are 1 and 2−η
η , respectively. Setting
η ≤ 1, the function is negative in (1, 2−η
η ] and positive in
2 maxi �ui�2X+1, then
(−∞, 1]. Additionally, if 0 < η ≤
for all i, j ∈ [k], �U , (ei − ej) ⊗ xt� ≤ 2−η
η . Therefore,
we have that

γη(2 − η)

T�t=1

E[ntzT

t A−1

t zt],

F +

aη

+

k

2

f (�U , (eyt − e¯yt ) ⊗ xt�)

= f ((U xt)yt − (U xt)¯yt )
≤ �η ((U xt)yt − (U xt)¯yt )

r�=yt

(U xt)r� = �η(U , (xt, yt)) .

≤ �η�(U xt)yt − max
where the ﬁrst equality is from algebra, the ﬁrst inequal-
ity is from that f (·) ≤ �η(·) in (−∞, 2−η
η ], the second
inequality is from that �η(·) is monotonically decreasing.
Putting together the two constraints on η, and noting that
ˆMT ≤�T

The second statement follows from Lemma 3 below.

t=1 ht, we have the ﬁrst bound.

Efﬁcient Online Bandit Multiclass Learning

t A−1

E[ntzT

T�t=1

Lemma 3. If d ≥ 1, k ≥ 2, T ≥ 2, then
t zt] ≤ dk ln�1 +

a d k � .
Speciﬁcally, if a = X 2, the right hand side is ≤ dk ln T .
Proof. Observe that

2X 2T

ntzT

t A−1

T�t=1
≤ d k ln1 +

t zt ≤ ln |AT|
|A0|
2X 2�T

t=1
a d k

1[˜yt=yt]

pt,yt

 ,

�= ˆyt.

• yt = ˜yt

In this case, ¯yt = ˆyt, therefore
�Wt , gt� ≥ 0, making mt ≥ 0. This implies that
�t−1
s=1 nsms + mt ≥ 0, guaranteeing nt = 1.
• yt = ˜yt = ˆyt. In this case, nt is set to 1 if and only if
qt = 1, i.e.�t−1

s=1 nsms + mt ≥ 0.

This gives that nt = Gt + Ht.
Second, we have the following two equalities:

Et−1 [Ht �U , gt�]
= Et−1� 1[˜yt = yt]

pt,yt

1[ˆyt �= yt]�U , (eyt − e¯yt ) ⊗ xt��

= 1[ˆyt �= yt]�U , (eyt − e¯yt ) ⊗ xt� ,

Et−1 [Gt �U , gt�]
= Et−1� 1[˜yt = yt]

pt,yt

1[ˆyt = yt]qt �U , (eyt − e¯yt ) ⊗ xt��

is

the

statement

second
identi-
�U , (eyt − e¯yt ) ⊗ xt� with

= 1[ˆyt �= yt]qt �U , (eyt − e¯yt ) ⊗ xt� .
The ﬁrst statement follows from adding up the two equali-
ties above.
The
proof
for
cal,
except
replacing
�U , (eyt − e¯yt ) ⊗ xt�2.
4.2. Fall-Back Analysis
The loss function �η is an interpolation between the hinge
and the squared hinge losses. Yet, the bound becomes vac-
uous for η = 0. Hence, in this section we show that SOBA
also guarantees a ˜O((L0(U )T )1/3 + √T ) mistake bound
w.r.t. L0(U ), the multiclass hinge loss of the competitor,
assuming L0(U ) is known. Thus the algorithm achieves
a mistake guarantee no worse than the sharpest bound im-
plicit in Kakade et al. (2008).
Theorem 4. Set a = X 2 and denote by MT the number
of mistakes done by SOBA. Then SOBA has the following
guarantees:3

where the ﬁrst inequality is a well-known fact from linear
algebra (e.g. Hazan et al., 2007, Lemma 11). Given that the
AT is kd × kd, the second inequality comes from the fact
that |AT| is maximized when all its eigenvalues are equal
to tr(AT )
.
Finally, using Jensen’s inequality, we have that,

t=1 nt�zt�2

≤ a +

t=1
d k

1[ ˜yt=yt ]

pt,yt

d k

d k = a + �T
T�t=1

E[ntzT

t A−1

t zt] ≤ d k ln�1 +

2X 2�T
a d k � .

2X 2T

If a = X 2, then the right hand side is d k ln(1+ 2T
is at most dk ln T under the conditions on d, k, T .

d k ), which

Proof of Theorem 2. Observe that by triangle inequality,
1[˜yt �= yt] ≤ 1[˜yt �= ˆyt] + 1[yt �= ˆyt]. Summing over
t, taking expectation on both sides, we conclude that

E[MT ] ≤ E[ ˆMT ] + γT .

(6)

The ﬁrst statement follows from combining the above in-
equality with Theorem 3.
For the second statement, ﬁrst note that from Theorem 3,
and Equation (6), we have

E[MT ] ≤ Lη(U ) +

F

aη�U�2
2 − η
≤ Lη(U ) + X 2�U�2

+

F +

a d k �
dk2 ln�1 + 2T X 2

γη(2 − η)

2d k2 ln T

+ γT,

γη

+ γT

T

).

where the second inequality is from that η ≤ 1, and
Lemma 3 with a = X 2. The statement is concluded by
the setting of γ = O(� k2d ln T
Proof of Lemma 1. We show the lemma in two steps. Let
Gt := qt · 1[yt = ˜yt = ˆyt], and Ht := 1[yt = ˜yt �= ˆyt].
First, we show that nt = Gt + Ht. Recall that SOBA
maintains the invariant (4), hence�t−1
s=1 nsms ≥ 0. From
line 14 of SOBA, we see that nt = 1 only if ˜yt = yt. Now
consider two cases:

1. If L0(U ) ≥ (�U�2

F + 1)√dk2X 2T ln T , then with
)1/3),

parameter setting γ = min(1, ( dk2X 2L0(U ) ln T
one has the following expected mistake bound:

T 2

E[MT ] ≤ L0(U )
+ O��U�F (d k2X 2L0(U )T ln T )1/3� .

3Assuming the knowledge of �U�F it would be possible to
reduce the dependency on �U�F in both bounds. However such
assumption is extremely unrealistic and we prefer not to pursue it.

Efﬁcient Online Bandit Multiclass Learning

2. If L0(U ) < (�U�2

F + 1)√dk2X 2T ln T , then with
)1/2), one

parameter setting γ = min(1, ( d k2X 2 ln T
has the following expected mistake bound:

T

E[MT ] ≤ L0(U ) + O�k(�U�2

F + 1)X√dT ln T� .

t=1 �0(U , (xt, yt)) is the hinge loss of

Proof. Recall that ˆMT the mistakes made by ˆyt, that is
1[ˆyt �= yt]. Adding to both sides of (5) the term
t=1 ht] and dividing both sides by η, and plugging

t=1

a = X 2, we get that for all η > 0,

ht · (1 − �U , (eyt − e¯yt ) ⊗ xt�)

2 �U , (e¯yt − eyt ) ⊗ xt�2�

d k2
2 γ η

ln T

the linear classiﬁer U.

where L0(U ) :=�T
�T
ηE[�T
ht� ≤ E� T�t=1
E� T�t=1
T�t=1
ηX 2
2 �U�2
≤ E� T�t=1

ht ·

F +

+

+

η

+

d k2
2 γ η

ln T .

�0(U , (xt, yt)) +� T�t=1

1

2� · η�U�2

F X 2�

ht +

inequality

uses Lemma

the ﬁrst
inequality

3,
from Cauchy-Schwarz

the
where
that
second
�U , (eyt − e¯yt ) ⊗ xt� ≤ �U�F · �(eyt − e¯yt ) ⊗ xt� ≤
�U�F√2X and that (1 − �U , (eyt − e¯yt ) ⊗ xt�) ≤
�(U , (xt, yt)).

is

Taking η =

�U�2

dk2
2γ ln T

F (E[�t ht]+ 1

2 )X 2 , we have

1

2γ

ln T

2� d k2 X 2
ht� +

ht� +
F�E� T�t=1

E� T�t=1
ht� ≤ L0(U )
+�����U�2
F�E� T�t=1
≤ L0(U ) +�����U�2
inequality √c + d ≤ √c + √d, and the setting of
a = X 2. Solving the inequality and using the fact that
E[MT ] ≤ E[ ˆMT ] + γT ≤ E[�T
t=1 ht] + γT , we have
+ O d k2 �U�2
+�L0(U )

inequality is due to the elementary

2� d k2 X 2

d k2 �U�2
F X 2 ln T
γ

E[MT ] ≤ L0(U ) + γT

F X 2 ln T
γ

where the last

 .

ln T ,

γ

1

F , H = d k2 X 2 ln T , L = L0(U ).

The theorem follows from Lemma 5 in Appendix B, taking
U = �U�2
5. Empirical Results
We tested SOBA to empirically validate the theoretical
ﬁndings. We used three different datasets from Kakade
et al. (2008): SynSep, SynNonSep, Reuters4.
The ﬁrst two are synthetic, with 106 samples in R400 and
9 classes. SynSep is constructed to be linearly separa-
ble, while SynNonSep is the same dataset with 5% ran-
dom label noise. Reuters4 is generated from the RCV1
dataset (Lewis et al., 2004), extracting the 665,265 ex-
amples that have exactly one label from the set {CCAT,
ECAT, GCAT, MCAT}. It contains 47,236 features. We
also report the performance on Covtype from LibSVM
repository.4 We report averages over 10 different runs.
SOBA, as the Newtron algorithm, has a quadratic complex-
ity in the dimension of the data, while the Banditron and
the Perceptron algorithm are linear. Following the long tra-
dition of similar algorithms (Crammer et al., 2009; Duchi
et al., 2011; Hazan & Kale, 2011; Crammer & Gentile,
2013), to be able to run the algorithm on large datasets,
we have implemented an approximated diagonal version of
SOBA, named SOBAdiag. It keeps in memory just the di-
agonal of the matrix At. Following Hazan & Kale (2011),
we have tested only algorithms designed to work in the
fully adversarial setting. Hence, we tested the Banditron
and the PNewtron, the diagonal version of the Newtron al-
gorithm in Hazan & Kale (2011). The multiclass Percep-
tron algorithm was used as a full-information baseline.
In the experiments, we only changed the exploration rate γ,
leaving ﬁxed all the other parameters the algorithms might
have.
In particular, for the PNewtron we set α = 10,
β = 0.01, and D = 1, as in Hazan & Kale (2011). In
SOBA, a is ﬁxed to 1 in all the experiments. We explore
the effect of the exploration rate γ in the ﬁrst row of Figure
5. We see that the PNewtron algorithm,5 thanks to the ex-
ploration based on the softmax prediction, can achieve very
good performance for a wide range of γ.
It is important to note that SOBAdiag has good perfor-
mance on all four datasets for a value of γ close to 1%. For
bigger values, the performance degrades because the best
possible error rate is lower bounded by k−1
k γ due to explo-

4https://www.csie.ntu.edu.tw/˜cjlin/

libsvmtools/datasets/

5We were unable to make the PNewtron work on Reuters4.
For any setting of γ the error rate is never better than 57%. The
reason might be that the dataset RCV1 has 47,236 features, while
the one reported in Kakade et al. (2008); Hazan & Kale (2011)
has 346,810, hence the optimal setting of the 3 other parameters
of PNewtron might be different. For this reason we prefer not to
report the performance of PNewtron on Reuters4.

SynSep

Banditron
PNewtron
SOBAdiag
Perceptron

0.5

0.4

0.3

0.2

0.1

t

e
a
r
 
r
o
r
r
e

0

10 -4

10 -3

10 -2

10 -1

SynSep

Banditron, 

=2 -13
=2 -13
PNewtron, 
SOBAdiag,  =2 -7
Perceptron

10 0

10 -1

10 -2

10 -3

e

t

a
r
 
r
o
r
r
e

10 -4

10 -5

10 2

Efﬁcient Online Bandit Multiclass Learning

SynNonSep

Banditron
PNewtron
SOBAdiag
Perceptron

Covtype

Banditron
PNewtron
SOBAdiag
Perceptron

0.7

0.65

0.6

0.55

0.5

t

e
a
r
 
r
o
r
r
e

0.45

0.4

10 -4

10 -3

10 -2

10 -1

0.35

10 -4

10 -3

10 -2

10 -1

Covtype

Banditron, 

=2 -5
=2 -9
PNewtron, 
SOBAdiag,  =2 -6
Perceptron

SynNonSep

Banditron, 

=2 -6
=2 -13
PNewtron, 
SOBAdiag,  =2 -7
Perceptron

0.8

0.7

0.6

0.5

0.4

t

e
a
r
 
r
o
r
r
e

t

e
a
r
 
r
o
r
r
e

0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

10 0

e

t

a
r
 
r
o
r
r
e

10 -1

Reuters4

Banditron
SOBAdiag
Perceptron

10 -3

10 -2

10 -1

Reuters4

=2 -5
Banditron, 
SOBAdiag,  =2 -7
Perceptron

0.5

0.4

0.3

0.2

0.1

e

t

a
r
 
r
o
r
r
e

0
10 -4

10 0

e

t

a
r
 
r
o
r
r
e

10 -1

10 3

10 4

10 5

10 6

10 2

10 3

10 4

10 5

10 6

10 2

10 3

10 4

10 5

10 2

10 3

10 4

10 5

number of examples

number of examples

number of examples

number of examples

Figure 2. Error rates vs.
logarithmic in all the plots, while the y-axis is logarithmic in the plots in the second row. Figure best viewed in colors.

the value of the exploration rate γ (top row) and vs.

the number examples (bottom row). The x-axis is

ration. For smaller values of exploration, the performance
degrades because the algorithm does not update enough. In
fact, SOBA updates only when ˜yt = yt, so when γ is too
small the algorithms does not explore enough and remains
stuck around the initial solution. Also, SOBA requires an
initial number of updates to accumulate enough negative

terms in the�t ntmt in order to start updating also when

ˆyt is correct but the margin is too small.
The optimal setting of γ for each algorithm was then used
to generate the plots in the second row of Figure 5, where
we report the error rate over time. With the respective opti-
mal setting of γ, we note that the performance of PNewtron
does not seem better than the one of the Multiclass Percep-
tron algorithm, and on par or worse to the Banditron’s one.
On the other hand, SOBAdiag has the best performance
among the bandits algorithms on 3 datasets out of 4.
The ﬁrst dataset, SynSep, is separable and with their opti-
mal setting of γ, all the algorithms converge with a rate of
T ), as can be seen from the log-log plot, but the
roughly O( 1
bandit algorithms will not converge to zero error rate, but
to k−1
k γ. However, SOBA has an initial phase in which the
error rate is high, due to the effect mentioned above.
On the second dataset, SynNonSep, SOBAdiag out-
performs all
the other algorithms (including the full-
information Perceptron), achieving an error rate close to
the noise level of 5%. This is due to SOBA being a second-
order algorithm, while the Perceptron is a ﬁrst-order algo-
rithm. A similar situation is observed on Covtype. On the
last dataset, Reuters4, SOBAdiag achieves performance
better than the Banditron.

6. Discussion and Future Work
In this paper, we study the problem of online multiclass
learning with bandit feedback. We propose SOBA, an algo-
√T ) with respect to η-
rithm that achieves a regret of ˜O( 1
η
loss of the competitor. This answers a COLT open problem
posed by (Abernethy & Rakhlin, 2009). Its key ideas are to
apply a novel adaptive regularizer in a second order online
learning algorithm, coupled with updates only when the
predictions are correct. SOBA is shown to have compet-
itive performance compared to its precedents in synthetic
and real datasets, in some cases even better than the full-
information Perceptron algorithm. There are several open
questions we wish to explore:
Is it possible to design efﬁcient algorithms with mis-
1.
take bounds that depend on the loss of the competitor, i.e.

E[MT ] ≤ Lη(U ) + ˜O(�kdLη(U ) + kd)? This type of

bound occurs naturally in the full information multiclass
online learning setting, (see e.g. Theorem 1), or in multi-
armed bandit setting, e.g. (Neu, 2015).
2. Are there efﬁcient algorithms that have a ﬁnite mistake
bound in the separable case?
(Kakade et al., 2008) pro-
vides an algorithm that performs enumeration and plurality
vote to achieve a ﬁnite mistake bound in the ﬁnite dimen-
sional setting, but unfortunately the algorithm is impracti-
cal. Notice that it is easy to show that in SOBA ˆyt makes a
logarithmic number of mistakes in the separable case, with
a constant rate of exploration, yet it is not clear how to de-
crease the exploration over time in order to get a logarith-
mic number of mistakes for ˜yt.

Acknowledgments. We thank Claudio Gentile for sug-
gesting the original plan of attack for this problem, and
thank the anonymous reviewers for thoughtful comments.

Efﬁcient Online Bandit Multiclass Learning

References
Abernethy, J. and Rakhlin, A. An efﬁcient bandit algorithm
for √T -regret in online multiclass prediction? In COLT,
2009.

Agarwal, A., Hsu, D., Kale, S., Langford, J., Li, L., and
Schapire, R. E. Taming the monster: a fast and simple
algorithm for contextual bandits. ICML, 2014.

Auer, P., Cesa-Bianchi, N., and Gentile, C. Adaptive and
self-conﬁdent on-line learning algorithms. J. Comput.
Syst. Sci., 64(1):48–75, 2002.

Auer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R. E.
The nonstochastic multiarmed bandit problem. SIAM J.
Comput., 32(1):48–77, January 2003.

Bubeck, S. and Cesa-Bianchi, N. Regret analysis of
stochastic and nonstochastic multi-armed bandit prob-
lems. FnTML, 5(1):1–122, 2012.

Cesa-Bianchi, N. and Lugosi, G. Prediction, learning, and

games. Cambridge University Press, 2006.

Cesa-Bianchi, N., Conconi, A., and Gentile, C. A second-
order Perceptron algorithm. SIAM Journal on Comput-
ing, 34(3):640–668, 2005.

Crammer, K. and Gentile, C. Multiclass classiﬁcation with
bandit feedback using adaptive regularization. Machine
learning, 90(3):347–383, 2013.

Crammer, K., Dekel, O., Keshet, J., Shalev-Shwartz, S.,
and Singer, Y. Online passive-aggressive algorithms.
JMLR, 7(Mar):551–585, 2006.

Crammer, K., Kulesza, A., and Dredze, M. Adaptive regu-
larization of weight vectors. In NIPS, pp. 414–422, 2009.

Kakade, S. M., Shalev-Shwartz, S., and Tewari, A. Efﬁ-
cient bandit algorithms for online multiclass prediction.
In ICML, pp. 440–447. ACM, 2008.

Kivinen, J. and Warmuth, M. Exponentiated gradient ver-
Information

sus gradient descent for linear predictors.
and Computation, 132(1):1–63, January 1997.

Langford, J. and Zhang, T. The epoch-greedy algorithm for
multi-armed bandits with side information. In NIPS 20,
pp. 817–824. 2008.

Lewis, D. D., Yang, Y., Rose, T. G., and Li, F. RCV1:
A new benchmark collection for text categorization re-
search. JMLR, 5(Apr):361–397, 2004.

McMahan, H Brendan and Streeter, Matthew. Adap-
tive bound optimization for online convex optimization.
COLT, 2010.

Mohri, M. and Rostamizadeh, A.

Perceptron mistake

bounds. arXiv preprint arXiv:1305.0208, 2013.

Neu, G. First-order regret bounds for combinatorial semi-

bandits. In COLT, pp. 1360–1375, 2015.

Orabona, F., Cesa-Bianchi, N., and Gentile, C. Beyond
logarithmic bounds in online learning. In AISTATS, pp.
823–831, 2012.

Orabona, F., Crammer, K., and Cesa-Bianchi, N. A gener-
alized online mirror descent with applications to classi-
ﬁcation and regression. Machine Learning, 99(3):411–
435, 2015.

Rakhlin, A. and Sridharan, K. BISTRO: An efﬁcient
In

relaxation-based method for contextual bandits.
ICML, 2016.

Duchi, J., Hazan, E., and Singer, Y. Adaptive subgradient
methods for online learning and stochastic optimization.
JMLR, 12(Jul):2121–2159, 2011.

Rosenblatt, F. The Perceptron: A probabilistic model for
information storage and organization in the brain. Psy-
chological review, 65(6):386–407, 1958.

Duda, R. O. and Hart, P. E. Pattern classiﬁcation and scene

analysis. John Wiley, 1973.

Shalev-Shwartz, S. Online learning and online convex op-

timization. FnTML, 4(2):107–194, 2011.

Dud´ık, M., Hsu, D. J., Kale, S., Karampatziakis, N., Lang-
ford, J., Reyzin, L., and Zhang, T. Efﬁcient optimal
learning for contextual bandits. In UAI 2011, pp. 169–
178, 2011.

Hazan, E. and Kale, S. Newtron: an efﬁcient bandit al-
In NIPS, pp.

gorithm for online multiclass prediction.
891–899, 2011.

Syrgkanis, Vasilis, Krishnamurthy, Akshay, and Schapire,
Robert. Efﬁcient algorithms for adversarial contextual
learning. In ICML, pp. 2159–2168, 2016a.

Syrgkanis, Vasilis, Luo, Haipeng, Krishnamurthy, Akshay,
and Schapire, Robert E.
Improved regret bounds for
oracle-based adversarial contextual bandits. In NIPS, pp.
3135–3143, 2016b.

Hazan, E., Agarwal, A., and Kale, S. Logarithmic re-
gret algorithms for online convex optimization. Machine
Learning, 69(2-3):169–192, 2007.

Valizadegan, H., Jin, R., and Wang, S. Learning to trade off
between exploration and exploitation in multiclass ban-
dit prediction. In KDD, pp. 204–212. ACM, 2011.

Efﬁcient Online Bandit Multiclass Learning

Wang, S., Jin, R., and Valizadegan, H. A potential-based
framework for online multi-class learning with partial
feedback. In AISTATS, pp. 900–907, 2010.

