An Analytical Formula of Population Gradient for two-layered ReLU network

and its Applications in Convergence and Critical Point Analysis

Yuandong Tian 1

Abstract

j=1 σ(w⊺
j

work g(x; w) = PK

In this paper, we explore theoretical prop-
erties of
training a two-layered ReLU net-
x) with cen-
tered d-dimensional spherical Gaussian input x
(σ=ReLU). We train our network with gradient
descent on w to mimic the output of a teacher
network with the same architecture and ﬁxed pa-
rameters w∗. We show that its population gra-
dient has an analytical formula, leading to inter-
esting theoretical analysis of critical points and
convergence behaviors. First, we prove that criti-
cal points outside the hyperplane spanned by the
teacher parameters (“out-of-plane“) are not iso-
lated and form manifolds, and characterize in-
plane critical-point-free regions for two ReLU
case. On the other hand, convergence to w∗ for
one ReLU node is guaranteed with at least (1 −
ǫ)/2 probability, if weights are initialized ran-
domly with standard deviation upper-bounded by
O(ǫ/√d), consistent with empirical practice. For

network with many ReLU nodes, we prove that
an inﬁnitesimal perturbation of weight initializa-
tion results in convergence towards w∗ (or its
permutation), a phenomenon known as sponta-
neous symmetric-breaking (SSB) in physics. We
assume no independence of ReLU activations.
Simulation veriﬁes our ﬁndings.

1. Introduction

Despite empirical success of deep learning (e.g., Computer
Vision (He et al., 2016; Simonyan & Zisserman, 2015;
Szegedy et al., 2015; Krizhevsky et al., 2012), Natural
Language Processing (Sutskever et al., 2014) and Speech
Recognition (Hinton et al., 2012)), it remains elusive how

1Facebook AI Research. Correspondence to: Yuandong Tian

<yuandong@fb.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

and why simple methods like gradient descent can solve
the complicated non-convex optimization during training.
In this paper, we focus on a two-layered ReLU network:

g(x; w) =

K

Xj=1

σ(w⊺
j

x),

(1)

Here σ(x) = max(x, 0) is the ReLU nonlinearity. We con-
sider the setting that a student network is optimized to min-
imize the l2 distance between its prediction and the super-
vision provided by a teacher network of the same archi-
tecture with ﬁxed parameters w∗. Note that although the
network prediction (Eqn. 1) is convex, when coupled with
loss (e.g., l2 loss Eqn. 2), the optimization becomes highly
non-convex and has exponential number of critical points.

To analyze it, we introduce a simple analytic formula for
population gradient in the case of l2 loss, when inputs x are
sampled from zero-mean spherical Gaussian. Using this
formula, critical point and convergence analysis follow.

For critical points, we show that critical points outside the
principal hyperplane (the subspace spanned by w∗) form
manifolds. We also characterize the region in the principal
hyperplane that has no critical points, in two ReLU case.

We also analyze the convergence behavior under the pop-
ulation gradient. Using Lyapunov method (LaSalle & Lef-
schetz, 1961), for single ReLU case we prove that gra-

dient descent converges to w∗ with at least (1 − ǫ)/2
upper-bounded by O(ǫ/√d), verifying common initializa-

probability, if initialized randomly with standard deviation

tion techniques (Bottou, 1988; Glorot & Bengio, 2010; He
et al., 2015; LeCun et al., 2012). For multiple ReLU case,
when the teacher parameters {wj}K
j=1 form an orthonor-
mal basis, we prove that (1) a symmetric weight initial-
ization gets stuck at a saddle point and (2) a particular
inﬁnitesimal perturbation of (1) leads to convergence to-
wards w∗ or its permutation. The behavior that the popula-
tion gradient ﬁeld is invariant under certain symmetry but
the solution breaks it, is known as spontaneous symmetry
breaking in physics. Although such behaviors are known
practically, to our knowledge, we ﬁrst formally character-
ize them in 2-layered ReLU network. Codes are available 1.

1github.com/yuandong-tian/ICML17_ReLU

An Analytic Formula of Population Gradient for 2-layered ReLU and its Applications

(a)

Student	
Network

w

Teacher
Network

∗

w

g

(b)

1

1

1

j
wj

X

Figure 1. (a) We consider the student and teacher network as non-
linear neural networks with ReLU nonlinearity. The student net-
work updates its weight w from the output of the teacher with
(b) The 2-layered ReLU network structure
ﬁxed weights w
(Eqn. 1) discussed in this paper. The ﬁrst layer contains ﬁxed
weights of value 1, while the second layers has K ReLU nodes.
Each node j has a d-dimensional weight wj to be optimized.
Teacher network has the same architecture as the student.

∗.

2. Related Works

linear network, many works

For multilayer
ana-
lyze its critical points and convergence behaviors.
(Saxe et al., 2013) analyzes its dynamics of gradient de-
scent and (Kawaguchi, 2016) shows every local minimum
is global. On the other hand, very few theoretical works
have been done for nonlinear networks. (Mei et al., 2016)
shows the global convergence for a single nonlinear node
whose derivatives of activation σ′, σ′′, σ′′′ are bounded and
σ′ > 0. Similar to our approach, (Saad & Solla, 1996) also
uses the student-teacher setting and analyzes the student
dynamics when the teacher’s parameters w∗ are orthonor-
mal. However, their activation is Gaussian error function
erf(x), and only the local behaviors of the two critical
points (the initial saddle point near the origin and w∗) are
analyzed. Recent paper (Zhang et al., 2017) analyzes a
similar teacher-student setting on 2-layered network when
the involved function is harmonic, but it is unclear how the
conclusion is generalized to ReLU case. To our knowl-
edge, our close-form formula for 2-layered ReLU network
is novel, as well as the critical point and convergence
analysis. Concurrent work (Brutzkus & Globerson, 2017)
proposes the same formula with a different approach,
and provides similar convergence analysis for one node.
For multiple nodes, they assume non-overlapping shared
weights, a special case of our assumption (Sec. 6.2) that
weights are cyclically symmetric and orthonormal.

Many previous works analyze nonlinear network based
on the assumption of independent activations: the activa-
tions of ReLU (or other nonlinear) nodes are independent
of the input and/or mutually independent. For example,
(Choromanska et al., 2015a;b) relates the nonlinear ReLU
network with spin-glass models when several assumptions
hold, including the assumption of independent activations
(A1p and A5u). (Kawaguchi, 2016) proves that every local
minimum in nonlinear network is global based on similar
assumptions. (Soudry & Carmon, 2016) shows the global

optimality of the local minimum in a two-layered ReLU
network, when independent multiplicative Bernoulli noise
is applied to the activations.
In practice, activations that
share the input are highly dependent. Ignoring such depen-
dency misses important behaviors, and may lead to mis-
leading conclusions. In this paper, no assumption of inde-
pendent activations is made. Instead, we assume input to
follow spherical Gaussian distribution, which gives more
realistic and interdependent activations during training.

For sigmoid activation, (Fukumizu & Amari, 2000) gives
complicated conditions for a local minimum to be
global when adding a new node to a 2-layered network.
(Janzamin et al., 2015) gives guarantees for parameter re-
covery of a 2-layered network learnt with tensor decompo-
sition. In comparison, we analyze ReLU networks trained
with gradient descent, which is more popular in practice.

3. Problem Deﬁnition

Denote N as the number of samples and d as the input di-
mension. The N -by-d matrix X is the input data and w∗
is the ﬁxed parameter of the teacher network. Given the
current estimation w, we have the following l2 loss:

J(w) =

1
2kg(X; w∗) − g(X; w)k2,

(2)

Here we focus on population loss EX [J], where the in-
put X is assumed to follow spherical Gaussian distri-
bution N (0, I).
Its gradient is the population gradient
EX [∇Jw(w)] (abbrev. E [∇J]). In this paper, we study
critical points E [∇J] = 0 and vanilla gradient dynamics
wt+1 = wt − ηE [∇J(wt)], where η is the learning rate.

4. The Analytical Formula

Properties of ReLU. ReLU nonlinearity has useful prop-
erties. We deﬁne the gating function D(w) ≡ diag(Xw >
0) as an N -by-N binary diagonal matrix. Its l-th diagonal
element is a binary variable showing whether the neuron
is activated for sample l. Using this notation, σ(Xw) =
D(w)Xw which means D(w) selects the output of a lin-
ear neuron, based on their activations. Note that D(w) only
depends on the direction of w but not its magnitude.

D(w) is also “transparent” with respect to derivatives. For
example, at differentiable regions, Jacobianw[σ(Xw)] =
σ′(Xw)X = D(w)X. This gives a very concise rule for
gradient descent update in ReLU networks.

One ReLU node. Given the properties of ReLU, the pop-
ulation gradient E [∇J] can be written as:
E [∇J] = EX [X ⊺D(w) (D(w)Xw − D(w∗)Xw∗)]
Intuitively, this term vanishes when w → w∗, and should

(3)

An Analytic Formula of Population Gradient for 2-layered ReLU and its Applications

2 (w − w∗) if the data are evenly distributed,

be around N
since roughly half of the samples are blocked. However,
such an estimation fails to capture the nonlinear behavior.
If we deﬁne Population Gating (PG) function F (e, w) ≡
X ⊺D(e)D(w)Xw, then E [∇J] can be written as:
E [∇J] = E [F (w/kwk, w)] − E [F (w/kwk, w∗)] . (4)

Interestingly, F (e, w) has an analytic formula if the data
X follow spherical Gaussian distribution:

Theorem 1 Denote F (e, w) = X ⊺D(e)D(w)Xw where
e is a unit vector, X = [x1, x2,··· , xN ]⊺ is the N -by-d
data matrix and D(w) = diag(Xw > 0) is a binary diag-
onal matrix. If xi ∼ N (0, I) (and thus bias-free), then:

E [F (e, w)] =

N
2π

[(π − θ)w + kwk sin θe]

(5)

where θ = ∠(e, w) ∈ [0, π] is the angle between e and w.
See the link2 for the proof of all theorems. Note that we
do not require X to be independent between samples. Intu-
itively, the ﬁrst mass term N
2π (π− θ)w aligns with w and is
proportional to the amount of activated data whose ReLU
are on. When θ = 0, the gating function is fully on and half
of the data contribute to the term; when θ = π, the gating
function is completely switched off. The gate is controlled
by the angle between w and the control signal e. The sec-
ond asymmetric term is aligned with e, and is proportional
to the asymmetry of the activated data samples (Fig. 2).

N

N
2

Note that the expectation analysis smooths out ReLU and
leaves only one singularity at the origin, where E [∇J] is
not continuous. That is, if approaching from different di-
rections towards w = 0, E [∇J] is different.
With the close form of F , E [∇J] also has a close form:
sin θw(cid:19) (6)
E [∇J] =
where θ = ∠(w, w∗) ∈ [0, π]. The ﬁrst term is from linear

2π (cid:18)θw∗ − kw∗k
kwk

(w−w∗)+

approximation, while the second term shows the nonlinear
behavior.
For linear case, D ≡ I (no gating) and thus ∇J ∝
X ⊺X(w − w∗).
For spherical Gaussian input X,
EX [X ⊺X] = I and E [∇J] ∝ w − w∗. Therefore, the dy-

namics has only one critical point and global convergence
follows, which is consistent with its convex nature.

Extension to other distributions. From its deﬁnition,
E [F (e, w)] = E [X ⊺D(e)D(w)Xw] is linear to kwk,
regardless of the distribution of X. On the other hand,
isotropy in spherical Gaussian distribution leads to the fact

2http://yuandong-tian.com/ssb-supp.pdf

mass term

asymmetric term

e

θ

O

w

w

θ

O

π −

e

θ

O

w

Data	with
ReLU gating	on

(π − θ)w

sin θkwke

Figure 2. Decomposition of Population Gating (PG) function
F (e, w) (Eqn. 5) into mass term and asymmetric term. F (e, w)
is computed from the portion of data with ReLU gate on. The
mass term is proportional to the amount of data, while the asym-
metric term is related to the data asymmetry with respect to e.

that E [F (e, w)] only depends on angles between vectors.
For other isotropic distributions, we could similarly derive:

E [F (e, w)] = A(θ)w + kwkB(θ)e

(7)

where A(0) = N/2 (gating fully on), A(π) = 0 (gating
fully off), and B(0) = B(π) = 0 (no asymmetry when w
and e are aligned). Although we focus on spherical Gaus-
sian case, many following analysis, in particular critical
point analysis, can also be applied to Eqn. 7.

Multiple ReLU node. For Eqn. 1 that contains K ReLU
node, we could similarly write down the population gradi-
ent with respect to wj (note that ej = wj/kwjk):
E(cid:2)∇wj J(cid:3) =

E(cid:2)F (ej, w∗
j ′ )(cid:3)

E [F (ej, wj ′ )] −

Xj ′=1

Xj ′=1

K

K

(8)

5. Critical Point Analysis

By solving Eqn. 8 (the normal equation, E(cid:2)∇wj J(cid:3) = 0),

we could identify all critical points of g(x). However, it is
highly nonlinear and cannot be solved easily. In this paper,
we provide conditions for critical points using the structure
of Eqn. 8. The case study for K = 2 gives examples for
saddle points and regions without critical points.

For convenience, we deﬁne Π∗ as the Principal Hyperplane
spanned by K ground truth weight vectors. Note that Π∗ is
at most K dimensional. {wj}K
j=1 is said to be in-plane, if
all wj ∈ Π∗. Otherwise it is out-of-plane.

5.1. Normal Equation

The normal equation {E(cid:2)∇wj J(cid:3) = 0}K

scalar equations and can be written as the following:

j=1 contain Kd

Y E ⊺ = B∗W ∗ ⊺

(9)

where Y = diag(sin Θ⊺ ¯w − sin Θ∗ ⊺ ¯w∗) + (π11⊺ −
Θ⊺)diag ¯w and B∗ = π11⊺ − (Θ∗)⊺. Here θ∗j ′
j ≡
j ′ ), θj ′
∠(wj, w∗
j] (i-th row, j-th
column of Θ is θi

j ≡ ∠(wj, wj ′ ), Θ = [θi

j ) and Θ∗ = [θ∗i
j ].

An Analytic Formula of Population Gradient for 2-layered ReLU and its Applications

Note that Y and B∗ are both K-by-K matrices that only
depend on angles and magnitudes, and hence rotational in-
variant. This leads to the following theorem characterizing
the structure of out-of-plane critical points:

Theorem 2 If d ≥ K + 2, then out-of-plane critical points
(solutions of Eqn. 9) are non-isolated and lie in a manifold.

The intuition is to construct a rotational matrix that is not
identity matrix but keeps Π∗ invariant. Such matrices form
a Lie group L that transforms critical points to critical
points. Then for any out-of-plane critical point, there is
one matrix in L that changes at least one of its weights,
yielding a non-isolated different critical point.

This is due to the symmetry of

Note that Thm. 2 also works for any general isotropic
in which E [F (e, w)] has the form of
distribution,
Eqn. 7.
the in-
put X, which in turn affects the geometry of critical
points. The theorem also explains why we have ﬂat
minima (Hochreiter et al., 1995; Dauphin et al., 2014) of-
ten occuring in practice.

5.2. In-Plane Normal Equation

To analyze in-plane critical points, it sufﬁces to study gra-
dient projections on Π∗. When {wj} is full-rank, the pro-
jections could be achieved by right-multiplying both sides
by {ej ′}, which gives K 2 equations:

M (Θ) ¯w = M ∗(Θ, Θ∗) ¯w∗

(10)

This again shows decomposition of angles and magnitudes,
and linearity with respect to the norms of weight vectors.
Here ¯w = [kw1k,kw2k, . . . ,kwKk]⊺ and similarly for
¯w∗. M and M ∗ are K 2-by-K matrices that only depend
on angles. Entries of M and M ∗ are:

mjj ′,k = (π − θk
jj ′,k = (π − θ∗k
m∗

j ) cos θk
j ) cos θ∗k

j ′ + sin θk
j ′ + sin θ∗k

j cos θj
j cos θj

j ′

(11)

j ′ (12)

Here index j is the j-th column of Eqn. 9, j′ is from pro-
jection vector ej ′ and k is the k-th weight magnitude.

Diagnoal constraints. For “diagonal” constraints (j, j) of
Eqn. 10, we have cos θj
jj,k =
h(θ∗k
j ), where h(θ) = (π − θ) cos θ + sin θ. Therefore, we
arrive at the following subset of the constraints:

j = 1 and mjj,k = h(θk

j ), m∗

w

∗
1

w1

Critical(        )

∗
2

w

w2

w

∗
1

w1

12(        )

L

w2

w1

12(        )

∗
2

w

L

w2

Figure 3. Separable property of critical points using Ljj ′ function
(Eqn. 14). Checking the criticability of {w1, w2, w
2} can be
decomposed into two subproblems, one related to {w1, w2, w
1}
and the other is related to {w1, w2, w

1, w

2}.

∗

∗

∗

∗

Separable Property. Interestingly, the plugging back op-
eration leads to conditions that are separable with respect
to ground truth weight (Fig. 3). To see this, we ﬁrst deﬁne
the following quantity Ljj ′ which is a function between a
single (rather than K) ground truth unit weight vector e∗
and all current unit weights {el}K

l=1:

r

mjj ′

l }, Θ) = m∗

jj ′ − v⊺M −1

Ljj ′ ({θ∗
l = ∠(e∗, el) is the angle between e∗ and el,
jj ′ =
j ′ (like Eqn. 12). Note that
r . Fig. 3 illustrates the

where θ∗
v = v({θ∗
j ) cos θ∗
(π − θ∗
v({θ∗j
l }) is the j-th column of M ∗
case when K = 2. Ljj ′ has the following properties:

l }) = [h(θ∗
j ′ + sin θ∗

K)]⊺, and m∗

1), . . . , h(θ∗

j cos θj

(14)

l }, Θ) = 0 when there exists l so

Proposition 1 Ljj ′ ({θ∗
that e∗ = el. In addition, Ljj({θ∗
Intuitively, Ljj ′ characterizes the relative geometric rela-
tionship among e∗ and {el}. It is like determinant of a ma-
trix whose columns are {el} and e∗. With Ljj ′ , we have

l }, Θ) = 0 always.

the following necessary conditions for critical points:

Theorem 3 If ¯w∗ 6= 0, and for a given parameter w,
Ljj ′ ({θ∗k
l }, Θ) > 0 (or < 0) for all 1 ≤ k ≤ K, then

w cannot be a critical point.

5.3. Case study: K = 2 network

In this case, Mr and M ∗
discuss the case that both w1 and w2 are in Π∗.

r are 2-by-2 matrices. Here we

Saddle points. When θ1
2 = 0 (w1 and w2 are collinear),
Mr = π11⊺ is singular since e1 and e2 are identical.
From Eqn. 9, if θ∗1
1 , i.e., they are both aligned
with the bisector angle of w∗
2, and π ¯w⊺1 =

1 = θ∗2

1 and w∗

h(cid:0)θ∗1

∗2/2(cid:1) ( ¯w∗)⊺1, then the current solution is a saddle

point. Note that this gives one constraint for two weight
magnitudes, and thus there exist inﬁnite solutions.

Mr ¯w = M ∗

r ¯w∗

(13)

where Mr = h(Θ⊺) and M ∗
r = h(Θ∗ ⊺) are both K-by-
K matrices. Note that if Mr is full-rank, then we could
solve ¯w from Eqn. 13 and plug it back in Eqn. 10 to check
whether it is indeed a critical point. This gives necessary
conditions for critical points that only depend on angles.

Region without critical points. We rely on the follow-
ing conjecture that is veriﬁed empirically in an exhaustive
manner (Sec. 7.2). It characterizes zero-crossings of a 2D
function on a closed region [0, 2π] × [0, π]. In comparison,
in-plane 2 ReLU network has 6 parameters and is more dif-
ﬁcult to handle: 8 for w1, w2, w∗
2, minus the rota-
tional and scaling symmetries.

1 and w∗

An Analytic Formula of Population Gradient for 2-layered ReLU and its Applications

(a)

w1

∗

w

w1

(b)

w

∗
1

w1

w1

w

∗
1

∗

w

w2

L12 > 0

L12 < 0

w2

w2

w

∗
2

(a)

w

∗
2

w2

Figure 4. Critical point analysis for K = 2. (a) L12 changes sign
∗ is in/out of the cone spanned by weights w1 and w2.
when w
(b) Two cases that (w1, w2) cannot be critical points.

Conjecture 1 If e∗ is in the interior of Cone(e1, e2), then
L12(θ∗
2) > 0. If e∗ is in the exterior, then L12 < 0.

2, θ1

1, θ∗

This is also empirically true for L21. Combined with
Thm. 3, we know that (Fig. 4):

Theorem 4 If Conjecture 1 is correct, then for 2 ReLU net-
work, (w1, w2) (w1 6= w2) is not a critical point, if they
both are in Cone(w∗

2), or both out of it.

1, w∗

When exact one w∗ is inside Cone(w1, w2), whether
(w1, w2) is a critical point remains open.

6. Convergence Analysis

Application of Eqn. 5 also yields interesting convergence
analysis. We focus on inﬁnitesimal analysis, i.e., when
learning rate η → 0 and the gradient update becomes a
ﬁrst-order differential equation:

dw/dt = −EX [∇wJ(w)]

(15)

Then the populated objective EX [J] does not increase:
dE [J] /dt = −E [∇J]⊺ dw/dt = −E [∇J]⊺ E [∇J] ≤ 0
(16)
The goal of convergence analysis is to determine speciﬁc
weight initializations w0 that leads to convergence to w∗
following the gradient descent dynamics (Eqn. 15).

6.1. Single ReLU case

Using Lyapunov method (LaSalle & Lefschetz, 1961), we
show that the gradient dynamics (Eqn. 15) converges to w∗
when w0 ∈ Ω = {w : kw − w∗k < kw∗k}:
Theorem 5 When w0 ∈ Ω = {w : kw − w∗k < kw∗k},
following the dynamics of Eqn. 15, the Lyapunov function
V (w) = 1
2kw − w∗k2 has dV /dt < 0 and the system is
asymptotically stable and thus wt → w∗ when t → +∞.
The intuition is to represent dV /dt as a 2-by-2 bilinear
form of vector [kwk,kw∗k], and the bilinear coefﬁcient
matrix, as a function of angles, is negative deﬁnite (except
for w = w∗). Note that similar approaches do not apply to
regions including the origin because at the origin, the pop-
ulation gradient is discontinuous. Ω does not include the

kw−w

∗k < kw
Convergent region

∗k

(b)

O

∗

w

Sample
region

Successful samples

δ

θ

r

O

∗

w

≥ Vd(1 − ✏)/2

Figure 5. (a) Sampling strategy to maximize the probability of
convergence. (b) Relationship between sampling range r and de-
sired probability of success (1 − ǫ)/2.

origin and for any initialization w0 ∈ Ω, we could always
ﬁnd a slightly smaller subset Ω′
δ = {w : kw − w∗k ≤
kw∗k− δ} with δ > 0 that covers w0, and apply Lyapunov
method within. Note that the global convergence claim
in (Mei et al., 2016) for l2 loss does not apply to ReLU,
since it requires σ′(x) > 0.
Random Initialization. How to sample w0 ∈ Ω without
knowing w∗? Uniform sampling around origin with ra-
dius r ≥ γkw∗k for any γ > 1 results in exponentially
small success rate (r/kw∗k)d ≤ γ−d in high-dimensional
space. A better idea is to sample around the origin with
very small radius (but not at w = 0), so that Ω looks like
a hyperplane near the origin, and thus almost half samples
are useful (Fig. 5(a)), as shown in the following theorem:

Theorem 6 The dynamics in Eqn. 6 converges to w∗ with
probability at least (1 − ǫ)/2, if the initial value w0 is
sampled uniformly from Br = {w : kwk ≤ r} with
r ≤ ǫq 2π

d+1kw∗k.

The idea is to lower-bound the probability of the shaded
area (Fig. 5(b)). Thm. 6 gives an explanation for common
initialization techniques (Glorot & Bengio, 2010; He et al.,
2015; LeCun et al., 2012; Bottou, 1988) that uses random

variables with O(1/√d) standard deviation.

6.2. Multiple ReLU case

For multiple ReLUs, Lyapunov method on Eqn. 8 yields
no decisive conclusion. Here we focus on the symmet-
ric property of Eqn. 8 and discuss a special case, that
the teacher parameters {w∗
j=1 and the initial weights
{w0
j}K
j=1 respect the following symmetry: wj = Pj w and
j = Pj w∗, where Pj is an orthogonal matrix whose col-
lection P ≡ {Pj}K
j=1 forms a group. Without loss of gen-
erality, we set P1 as the identity. Then from Eqn. 8 the
population gradient becomes:

j}K

w∗

E(cid:2)∇wj J(cid:3) = Pj E [∇w1 J]

(17)

This means that if all wj and w∗
j are symmetric under
group actions, so does their population gradients. There-

An Analytic Formula of Population Gradient for 2-layered ReLU and its Applications

(a)

(b)

1

2

Figure 6. Spontaneous Symmetric-Breaking (SSB): Objective /
gradient ﬁeld is symmetric but the solution is not. (a) Reﬂection
keeps the gradient ﬁeld invariant, but transforms 1 to 2 and vice
versa. (b) The Mexican hat example. Rotation keeps the objective
invariant, but transforms any local minimum to a different one.

fore, the trajectory {wt} also respects the symmetry (i.e.,
Pj wt
j ) and we only need to solve one equation for
E [∇wJ] instead of K (here e = w/kwk):

1 = wt

E [∇wJ] =

K

Xj ′=1

E [F (e, Pj ′ w)] − E [F (e, Pj ′ w∗)] (18)

Eqn. 18 has interesting properties, known as Spontaneous
Symmetric-Breaking (SSB) in physics (Brading & Castel-
lani, 2003), in which the equations of motion respect a cer-
tain symmetry but its solution breaks it (Fig. 6). In our lan-
guage, despite that the population gradient ﬁeld E [∇wJ]
and the objective E [J] are invariant to the group transfor-
mation P, i.e., for w∗ → Pj w∗, E [J] and E [∇wJ] remain
the same, its solution is not (Pj w 6= w). Furthermore,
since P is ﬁnite, as we will see, the ﬁnal solution converges
to different permutations of w∗ due to inﬁnitesimal pertur-
bations of initialization.

j}K

To illustrate such behaviors, consider the following exam-
ple in which {w∗
j=1 forms an orthonormal basis and un-
der this basis, P is a cyclic group in which Pj circularly
shifts dimension by j − 1 (e.g., P2[1, 2, 3]⊺ = [3, 1, 2]⊺).
In this case, if we start with w0 = x0w∗ +Pj6=1 Pj w∗
j =
[x0, y0, . . . , y0] under the basis of w∗, then Eqn. 18 is fur-
ther reduced to a convergent 2D nonlinear dynamics and
Thm. 7 holds (Please check Supplementary Materials for
the associated close-form of the 2D dynamics):

j

Theorem 7 For a bias-free two-layered ReLU network
g(x; w) = Pj σ(w⊺
x) that takes spherical Gaussian in-
puts, if the teacher’s parameters {w∗
j} form orthnomal
bases, then (1) when the student parameters is initialized to
be [x0, y0, . . . , y0] under the basis of w∗, where (x0, y0) ∈
Ω = {x ∈ (0, 1], y ∈ [0, 1], x > y}, then Eqn. 8 con-
verges to teacher’s parameters {w∗
j} (or (x, y) = (1, 0));
(2) when x0 = y0 ∈ (0, 1], then it converges to a saddle
point x = y = 1

πK (√K − 1 − arccos(1/√K) + π).

Thm. 7 suggests that when w0 = [y0, x0, . . . , y0], the sys-
tem converges to P2w∗, etc. Since |x0 − y0| can be arbi-
trarily small, a slightest perturbation around x0 = y0 leads

to a different ﬁxed point Pj w∗ for some j. Unlike single
ReLU case, the initialization in Thm. 7 is w∗-dependent,
and serves as an example for the branching behavior.

Thm. 7 also suggests that for convergence, x0 and y0 can
be arbitrarily small, regardless of the magnitude of w∗,
showing a global convergence behavior.
In comparison,
(Saad & Solla, 1996) uses Gaussian error function (σ =
erf) as the activation, and only analyzes local behaviors
near the two ﬁxed points (origin and w∗).

In practice, even with noisy initialization, Eqn. 18 and the
original dynamics (Eqn. 8) still converge to w∗ (and its
transformations). We leave it as a conjecture, whose proof
may lead to an initialization technique for 2-layered ReLU
that is w∗-independent.

Conjecture 2 If

the initialization w0 = x0w∗ +

y0Pj6=1 Pj w∗ + ǫ, where ǫ is noise and (x0, y0) ∈ Ω,

then Eqn. 8 also converges to w∗ with high probability.

7. Simulations

7.1. The analytical solution to F (e, w)

We verify E [F (e, w)] = E [X ⊺D(e)D(w)Xw] (Eqn. 5)
with simulation. We randomly pick e and w so that their
angle ∠(e, w) is uniformly distributed in [0, π]. The an-
alytical formula E [F (e, w)] is compared with F (e, w),
which is computed via sampling on the input X that
follows spherical Gaussian distribution. We use relative
RMS error: err = kE [F (e, w)] − F (e, w)k/kF (e, w)k.
Fig. 7(a) shows the error distribution with respect to angles.
For small θ, the gating function D(w) and D(e) mostly
overlap and give a reliable estimation. When θ → π, D(w)
and D(e)overlap less and the variance grows. Note that our
convergence analysis operate on θ ∈ [0, π/2] and is not af-
fected. In the following, we sample angles from [0, π/2].

Fig. 7(a) shows that the formula is more accurate with more
samples. We also examine other zero-mean distributions of
X, e.g., U [−1/2, 1/2]. As shown in Fig. 7(d), the formula
still works for large d. Note that the error is computed up to
a global scale, due to different normalization constants in
probability distributions. Whether Eqn. 5 applies for more
general distributions remains open.

7.2. Empirical Results in critical point analysis K = 2

Conjecture 1 can be reduced to enumerate a complicated
but 2D function via exhaustive sampling. In comparison, a
full optimization of 2-ReLU network constrained on prin-
cipal hyperplane Π∗ involves 6 parameters (8 parameters
minus 2 degrees of symmetry) and is more difﬁcult to han-
dle. Fig. 10 shows that empirically L12 has no extra zero-
crossing other than e∗ = e1 or e2. As shown in Fig. 10(c),
we have densely enumerated θ1
2 ∈ [0, π] and e∗ on a

An Analytic Formula of Population Gradient for 2-layered ReLU and its Applications

(a)

Distribution of relative RMS error on angle

(b)

Relative RMS error w.r.t #sample (Gaussian distribution)

(c)

Relative RMS error w.r.t #sample (Uniform distri.)

 

r
o
r
r
e
S
M
R
e
v
i
t
a
e
R

l

 

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.0

π/2

π

0.0        0.5       1.0        1.5        2.0        2.5       3.0

Angle (in radius)

0.40

0.35

0.30

0.25

0.20

0.15

0.10

0.05

0.00

0.40

0.35

0.30

0.25

0.20

0.15

0.10

0.05

0.00

#Samples

#Samples

#Samples

Figure 7. (a) Distribution of relative RMS error with respect to θ = ∠(w, e). (b) Relative RMS error decreases with sample size,
showing the asympototic behavior of the analytical formula (Eqn. 5). Note that the y-axis of the right plot is in log scale. (c) Eqn. 5 also
works well when the input data X are generated by other zero-mean distribution X, e.g., uniform distribution in [−1/2, 1/2].

(a)

Vector (cid:31)eld in (x, y) plane (K = 2)

(b)

Vector (cid:31)eld in (x, y) plane (K = 5)

1.0

0.8

0.6

y

0.4

0.2

1.0

0.8

0.6

y

0.4

0.2

0.0

0.0

0.0                     0.2                    0.4                    0.6                    0.8                    1.0

0.0                     0.2                    0.4                    0.6                    0.8                    1.0

x

x

(c)

Trajectory in (x, y) plane.

Saddle points

y = x

iter200

iter100

iter100

iter200

iter100

x

(d)

Convergence

#Iterations

y

 

e
c
n
a
t
s
i
d
2
L
 
t
h
g
e
W
d
e
r
a
u
q
S

i

 

Figure 8. (a)-(b) Vector ﬁeld in (x, y) plane following 2D dynamics (Thm. 7, See Supplementary Materials for the close-form formula)
∗ = (1, 0). (c) Trajectory in (x, y) plane
for K = 2 and K = 5. Saddle points are visible. The parameters of teacher’s network are at w
for K = 2, K = 5, and K = 10. All trajectories start from w
∗, gradient descent takes
detours. (d) Training curve. Interestingly, when K is larger the convergence is faster.

0 = (10−3, 0). Even w

0 is aligned with w

104 × 104 grid without ﬁnding any counterexamples.

7.3. Convergence analysis for multiple ReLU nodes

Fig. 8(a) and (b) shows the 2D vector ﬁeld in Thm 7.
Fig. 8(c) shows the 2D trajectory towards convergence to
the teacher’s parameters w∗.
Interestingly, even when
we initialize the weights as [10−3, 0]⊺, whose direction is
aligned with w∗ at [1, 0]⊺, the gradient descent still takes
detours to reach the destination. This is because at the be-
ginning of optimization, all ReLU nodes explain the train-
ing error in the same way (both x and y increases); when
the “obvious” component is explained, the error pushes
some nodes to explain other components. Hence, special-
ization follows (x increases but y decreases).
Fig. 9 shows empirical convergence for K ≥ 2, when
the initialization deviates from initialization [x, y, . . . , y]
in Thm. 7. Unless the deviation is large, w converges to
w∗. For more general network g2(x) = PK
x),
when aj > 0 convergence follows. When some aj is neg-
ative, the network fails to converge to w∗, even when the
j}K
student is initialized with the true values {a∗

j=1 ajσ(w⊺
j

j=1.

8. Extension to multilayer ReLU network

A natural question is whether the proposed method can be
extended to multilayer ReLU network. In this case, there is
similar subtraction structure for gradient as Eqn. 3:

Proposition 2 Denote [c] as all nodes in layer c. Denote
u∗
j and uj as the output of node j at layer c of the teacher
and student network, then the gradient of the parameters
wj immediate under node j ∈ [c] is:

∇wj J = X ⊺

c DjQj Xj ′∈[c]

(Qj ′ uj ′ − Q∗

j ′ u∗

j ′ )

(19)

k.

where Xc is the data fed into node j, Qj and Q∗
j are N -
by-N diagonal matrices. For any node k ∈ [c + 1], Qk =
Pj∈[c] wjkDjQj and similarly for Q∗

The 2-layered network in this paper is a special case with
Qj = Q∗
j = I. Despite the difﬁculty that Qj is now de-
pends on the weights of upper layers, and the input Xc is
not necessarily Gaussian distributed, Proposition 2 gives a
mathematical framework to explore the structure of gradi-
ent. For example, a similar deﬁnition of Population Gradi-

An Analytic Formula of Population Gradient for 2-layered ReLU and its Applications

1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0

noise = 0.5, top-w = 1

0              20             40             60             80           100

#Iteration

noise = 0.5, top-w ∈ [1, 2]

0              20             40             60             80           100

1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0

noise = 1.0, top-w = 1

0              20             40             60             80           100

#Iteration

noise = 0.5, top-w ∈ [0.1, 1.1]

0              20             40             60             80           100

1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0

noise = 1.5, top-w = 1

0              20             40             60             80           100

#Iteration

noise = 0.5, top-w ∈ [0.01, 0.11]

0              20             40             60             80           100

1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0

#Iteration

#Iteration

#Iteration

noise = 2.0, top-w = 1

0              20             40             60             80           100

#Iteration

noise = 0.5, top-w ∼ Ν(0, 1)

0              20             40             60             80           100

#Iteration

 

r
o
r
r
e
S
M
R
e
v
i
t
a
e
R

l

 

 

r
o
r
r
e
S
M
R
e
v
i
t
a
e
R

l

 

Figure 9. Top row: Convergence when weights are initialized with noise: w
2-layered network converges to w
has 8 runs. Bottom row: Convergence for g2(x) = PK
Large positive aj corresponds to fast convergence. When {aj} contains mixture signs, convergence to w

∗ + ǫ, where ǫ ∼ N (0, 10−3 ∗ noise). The
x). Each experiment
x). Here we ﬁx top weights aj at different numbers (rather than 1).

∗ until huge noise. Both teacher and student networks use g(x) = PK

∗ is not achieved.

j=1 ajσ(w

0 = 10−3

j=1 σ(w

⊺
j

⊺
j

w

(a)

(b)

(c)

2 , θ1

2 , θ1

1 , θ∗

1 , θ∗

2) and L21(θ∗

2 = ∠(e1, e2) and vary e

2) in 2
Figure 10. Quantity L12(θ∗
ReLU network. We ﬁx θ1
∗ =
[cos φ, sin φ]⊺. In this case, θ∗
2 are both dependent vari-
∗ ∈ Cone(e1, e2), L12 and
ables with respect to φ. When e
L21 > 0, otherwise negative. There are no extra zero-crossings.
(a)-(b) Examples: θ1
2 = 7π/8. (c) Empirical
2, φ) ∈ [0, π] × [0, 2π] with grid size 104 × 104.
evaluation on (θ1

2 = 3π/8 and θ1

1 and θ∗

ent function is possible.

9. Conclusion and Future Work

In this paper, we study the gradient descent dynamics of a
2-layered bias-free ReLU network. The network is trained
using gradient descent to reproduce the output of a teacher
network with ﬁxed parameters w∗ in the sense of l2 norm.
We propose a novel analytic formula for population gradi-
ent when the input follows zero-mean spherical Gaussian
distribution. This formula leads to interesting critical point
and convergence analysis. Speciﬁcally, we show that crit-
ical points out of the hyperplane spanned by w∗ are not
isolated and form manifolds. For two ReLU case, we char-
acterize regions that contain no critical points. For con-
vergence analysis, we show guaranteed convergence for a
single ReLU case with random initialization whose stan-

dard deviation is on the order of O(1/√d). For multiple

ReLU case, we show that an inﬁnitesimal change of weight
initialization leads to convergence to different optima.

Our work opens many future directions. First, Thm. 2 char-
acterizes the non-isolating nature of critical points in the
case of isotropic input distribution, which explains why of-
ten practical solutions of NN are degenerated. What if the
input distribution has different symmetries? Will such sym-
metries determine the geometry of critical points? Second,
empirically we see convergence cases that are not covered
by the theorems, suggesting the conditions imposed by the
theorems can be weaker. Finally, how to apply similar anal-
ysis to broader distributions and how to generalize the anal-
ysis to multiple layers are also open problems.

Acknowledgement We thank L´eon Bottou, Ruoyu Sun, Ja-
son Lee, Yann Dauphin and Nicolas Usunier for discus-
sions and insightful suggestions.

An Analytic Formula of Population Gradient for 2-layered ReLU and its Applications

References

Bottou, L´eon. Reconnaissance de la parole par reseaux
connexionnistes. In Proceedings of Neuro Nimes 88, pp.
197–218, Nimes, France, 1988. URL http://leon.
bottou.org/papers/bottou-88b.

Brading, Katherine and Castellani, Elena. Symmetries in
physics: philosophical reﬂections. Cambridge Univer-
sity Press, 2003.

Brutzkus, Alon and Globerson, Amir. Globally optimal
gradient descent for a convnet with gaussian inputs. In-
ternational Conference on Machine Learning (ICML),
2017.

Choromanska, Anna, Henaff, Mikael, Mathieu, Michael,
Arous, G´erard Ben, and LeCun, Yann. The loss surfaces
of multilayer networks. In AISTATS, 2015a.

Choromanska, Anna, LeCun, Yann,

and Arous,
G´erard Ben. Open problem: The landscape of the
loss surfaces of multilayer networks. In Proceedings of
The 28th Conference on Learning Theory, COLT 2015,
Paris, France, July 3, volume 6, pp. 1756–1760, 2015b.

Dauphin, Yann N, Pascanu, Razvan, Gulcehre, Caglar,
Cho, Kyunghyun, Ganguli, Surya, and Bengio, Yoshua.
Identifying and attacking the saddle point problem in
high-dimensional non-convex optimization. In Advances
in neural information processing systems, pp. 2933–
2941, 2014.

Fukumizu, Kenji and Amari, Shun-ichi. Local minima and
plateaus in hierarchical structures of multilayer percep-
trons. Neural Networks, 13(3):317–327, 2000.

Glorot, Xavier and Bengio, Yoshua. Understanding the dif-
ﬁculty of training deep feedforward neural networks. In
Aistats, volume 9, pp. 249–256, 2010.

He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun,
Jian. Delving deep into rectiﬁers: Surpassing human-
level performance on imagenet classiﬁcation.
In Pro-
ceedings of the IEEE International Conference on Com-
puter Vision, pp. 1026–1034, 2015.

He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun,
Jian. Deep residual learning for image recognition. Com-
puter Vision anad Pattern Recognition (CVPR), 2016.

Hinton, Geoffrey, Deng, Li, Yu, Dong, Dahl, George E,
Mohamed, Abdel-rahman, Jaitly, Navdeep, Senior, An-
drew, Vanhoucke, Vincent, Nguyen, Patrick, Sainath,
Tara N, et al. Deep neural networks for acoustic mod-
eling in speech recognition: The shared views of four
research groups. IEEE Signal Processing Magazine, 29
(6):82–97, 2012.

Hochreiter, Sepp, Schmidhuber, J¨urgen, et al. Simplify-
ing neural nets by discovering ﬂat minima. Advances
in Neural Information Processing Systems, pp. 529–536,
1995.

Janzamin, Majid, Sedghi, Hanie, and Anandkumar, An-
ima. Beating the perils of non-convexity: Guaranteed
training of neural networks using tensor methods. CoRR
abs/1506.08473, 2015.

Kawaguchi, Kenji. Deep learning without poor local min-
ima. Advances in Neural Information Processing Sys-
tems, 2016.

Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.
Imagenet classiﬁcation with deep convolutional neural
networks. In Advances in neural information processing
systems, pp. 1097–1105, 2012.

LaSalle, J. P. and Lefschetz, S. Stability by lyapunov’s
second method with applications. New York: Academic
Press., 1961.

LeCun, Yann A, Bottou, L´eon, Orr, Genevieve B, and
M¨uller, Klaus-Robert. Efﬁcient backprop. In Neural net-
works: Tricks of the trade, pp. 9–48. Springer, 2012.

Mei, Song, Bai, Yu, and Montanari, Andrea. The landscape
of empirical risk for non-convex losses. arXiv preprint
arXiv:1607.06534, 2016.

Saad, David and Solla, Sara A. Dynamics of on-line gradi-
ent descent learning for multilayer neural networks. Ad-
vances in Neural Information Processing Systems, pp.
302–308, 1996.

Saxe, Andrew M, McClelland, James L, and Ganguli,
Surya. Exact solutions to the nonlinear dynamics of
learning in deep linear neural networks. arXiv preprint
arXiv:1312.6120, 2013.

Simonyan, Karen and Zisserman, Andrew. Very deep con-
volutional networks for large-scale image recognition.
International Conference on Learning Representations
(ICLR), 2015.

Soudry, Daniel and Carmon, Yair. No bad local minima:
Data independent training error guarantees for multi-
layer neural networks. arXiv preprint arXiv:1605.08361,
2016.

Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc V.

Se-
quence to sequence learning with neural networks.
In
Advances in neural information processing systems, pp.
3104–3112, 2014.

Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet,
Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Du-
mitru, Vanhoucke, Vincent, and Rabinovich, Andrew.

An Analytic Formula of Population Gradient for 2-layered ReLU and its Applications

Going deeper with convolutions. In Computer Vision and
Pattern Recognition (CVPR), pp. 1–9, 2015.

Zhang, Qiuyi, Panigrahy, Rina, and Sachdeva, Sushant.
arXiv

Electron-proton dynamics in deep learning.
preprint arXiv:1702.00458, 2017.

