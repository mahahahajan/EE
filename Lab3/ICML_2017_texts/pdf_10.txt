Doubly Accelerated Methods for Faster CCA

and Generalized Eigendecomposition

Zeyuan Allen-Zhu * 1 Yuanzhi Li * 2

Abstract

We study k-GenEV, the problem of ﬁnding the
top k generalized eigenvectors, and k-CCA, the
problem of ﬁnding the top k vectors in canonical-
correlation analysis. We propose algorithms
LazyEV and LazyCCA to solve the two problems
with running times linearly dependent on the in-
put size and on k. Furthermore, our algorithms
are doubly-accelerated: our running times de-
pend only on the square root of the matrix condi-
tion number, and on the square root of the eigen-
gap. This is the ﬁrst such result for both k-
GenEV or k-CCA. We also provide the ﬁrst gap-
free results, which provide running times that de-
pend on 1/

ε rather than the eigengap.

√

1 Introduction
The Generalized Eigenvector (GenEV) problem and the
Canonical Correlation Analysis (CCA) are two fundamen-
tal problems in scientiﬁc computing, machine learning, op-
erations research, and statistics. Algorithms solving these
problems are often used to extract features to compare
large-scale datasets, as well as used for problems in regres-
sion (Kakade & Foster, 2007), clustering (Chaudhuri et al.,
2009), classiﬁcation (Karampatziakis & Mineiro, 2014),
word embeddings (Dhillon et al., 2011), and many others.
GenEV. Given two symmetric matrices A, B ∈ Rd×d
where B is positive deﬁnite. The GenEV problem is to ﬁnd
generalized eigenvectors v1, . . . , vd where each vi satisﬁes
vi ∈ arg max
v(cid:62)Bvj = 0 ∀j ∈ [i − 1]
v∈Rd
The values λi
i Avi are known as the generalized
eigenvalues, and it satisﬁes |λ1| ≥ ···|λd|. Following the
this paper
shall be found at http://arxiv.org/abs/1607.06017.
1Microsoft Research 2Princeton University. Correspondence
to: Zeyuan Allen-Zhu <zeyuan@csail.mit.edu>, Yuanzhi Li
<yuanzhil@cs.princeton.edu>.

(cid:26) v(cid:62)Bv = 1

(cid:12)(cid:12)v(cid:62)Av(cid:12)(cid:12) s.t.

*Equal contribution .

Future version of

def= v(cid:62)

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

tradition of (Wang et al., 2016; Garber & Hazan, 2015), we

assume without loss of generality that λi ∈ [−1, 1].

n X(cid:62)X, Sxy = 1

CCA. Given matrices X ∈ Rn×dx , Y ∈ Rn×dy and de-
n Y (cid:62)Y ,
noting by Sxx = 1
the CCA problem is to ﬁnd canonical-correlation vectors
{(φi, ψi)}r
(φi, ψi) ∈ arg max
φ∈Rdx ,ψ∈Rdy

n X(cid:62)Y , Syy = 1
i=1 where r = min{dx, dy} and each pair

(cid:26) φ(cid:62)Sxxφ = 1 ∧ φ(cid:62)Sxxφj = 0 ∀j ∈ [i − 1]

(cid:8)φ(cid:62)Sxyψ(cid:9)

such that

ψ(cid:62)Syyψ = 1 ∧ ψ(cid:62)Syyψj = 0 ∀j ∈ [i − 1]
i Sxyψi ≥ 0 are known as the
def= φ(cid:62)

The values σi
canonical-correlation coefﬁcients, and

1 ≥ σ1 ≥ ··· ≥ σr ≥ 0 is always satisﬁed.

It is a fact that solving CCA exactly can be reduced to solv-
ing GenEV exactly, if one deﬁnes B = diag{Sxx, Syy} ∈
xy, 0]] ∈ Rd×d for d def= dx +dy;
Rd×d and A = [[0, Sxy]; [S(cid:62)
see Lemma 2.3.
(This reduction does not always hold if
the generalized eigenvectors are computed only approxi-
mately.)
Despite the fundamental importance and the frequent ne-
cessity in applications, there are few results on obtaining
provably efﬁcient algorithms for GenEV and CCA until
very recently.
In the breakthrough result of Ma, Lu and
Foster (Ma et al., 2015), they proposed to study algorithms
to ﬁnd top k generalized eigenvectors (k-GenEV) or top k
canonical-correlation vectors (k-CCA). They designed an
alternating minimization algorithm whose running time is
only linear in the input matrix sparsity and nearly-linear in
k. Such algorithms are very appealing because in real-life
applications, it is often only relevant to obtain top correla-
tion vectors, as opposed to the less meaningful vectors in
the directions where the datasets do not correlate. Unfortu-
nately, the method of Ma, Lu and Foster has a running time
that linearly scales with κ and 1/gap, where
• κ ≥ 1 is the condition number of matrix B in GenEV,
or of matrices X(cid:62)X, Y (cid:62)Y in CCA; and
• gap ∈ [0, 1) is the eigengap λk−λk+1

in GenEV, or

λk

σk−σk+1

σk

in CCA.

These parameters are usually not constants and scale with

Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition

the problem size.

Challenge 1: Acceleration

√

For many easier scientiﬁc computing problems, we are able
to design algorithms that have accelerated dependencies on
κ and 1/gap. As two concrete examples, k-PCA can be
gap as opposed
solved with a running time linearly in 1/
to 1/gap (Golub & Van Loan, 2012); computing B−1w
κ as
for a vector w can be solved in time linearly in
opposed to κ, where κ is the condition number of matrix
B (Shewchuk, 1994; Axelsson, 1985; Nesterov, 1983).
Therefore, can we obtain doubly-accelerated methods for
k-GenEV and k-CCA, meaning that the running times lin-
gap? Before this paper,
early scale with both
for the general case k > 1, the method of Ge et al. (Ge
et al., 2016) made acceleration possible for parameter κ,
but not for parameter 1/gap (see Table 1).

κ and 1/

√

√

√

Challenge 2: Gap-Freeness

Since gap can be even zero in the extreme case, can we
design algorithms that do not scale with 1/gap? Recall
that this is possible for the easier task of k-PCA. The block
√
Krylov method (Musco & Musco, 2015) runs in time linear
gap, where ε is the approxima-
in 1/
tion ratio. There is no gap-free result previously known for
k-GenEV or k-CCA even for k = 1.

ε as opposed to 1/

√

Challenge 3: Stochasticity

√

√

λmin(B)

κ for κ being the condition number of B.

where κ(cid:48) = maxi∈[n]{(cid:107)Xi(cid:107)2}

For matrix-related problems, one can usually obtain
stochastic running times which requires some notations to
describe.
Consider a simple task of computing B−1w for some vec-
tor w, where accelerated methods solve it in time linearly
in
If B =
n X(cid:62)X is given in the form of a covariance matrix where
1
X ∈ Rn×d, then (accelerated) stochastic methods compute
√
κ,

B−1w in a time linearly in (1 +(cid:112)κ(cid:48)/n) instead of
∈ (cid:2)κ, nκ(cid:3) and Xi is the i-th
row of X. (See Lemma 2.6.) Since 1 +(cid:112)κ(cid:48)/n ≤ O(

κ),
stochastic methods are no slower than non-stochastic ones.
So, can we obtain a similar but doubly-accelerated
stochastic method for k-CCA?1 Note that, if the doubly-
accelerated requirement is dropped, this task is easier and
indeed possible, see Ge et al. (Ge et al., 2016). However,
since their stochastic method is not doubly-accelerated, in
certain parameter regimes, it runs even slower than non-
stochastic ones (even for k = 1, see Table 2).
Remark.
running time:
• Accelerated results are usually better because they are
1 Note that a similar problem can be also asked for k-GenEV
when A and B are both given in their covariance matrix forms.
We refrain from doing it in this paper for notational simplicity.

In general, if designed properly, for worst case

no slower than non-accelerated ones in the worst-case.
• Gap-free results are better because they imply gap-

dependent ones.2

• Stochastic results are usually better because they are no

slower than non-stochastic ones in the worst-case.

1.1 Our Main Results
We provide algorithms LazyEV and LazyCCA that are
doubly-accelerated, gap-free, and stochastic.3
For the general k-GenEV problem, our LazyEV can be im-
plemented to run in time4
κ

knnz(A) + k2d

√

(cid:16) knnz(B)
(cid:16) knnz(B)

√

gap
√
√

(cid:101)O
(cid:101)O

ε

+

κ

+

√

gap
√

ε

knnz(A) + k2d

(cid:17)
(cid:17)

or

√

√
ε), our algorithm LazyEV is doubly-accelerated.

in the gap-dependent and gap-free cases respectively. Since
our running time only linearly depends on
gap
(resp.
For the general k-CCA problem, our LazyCCA can be im-
plemented to run in time

κ and

√

(cid:16) knnz(X, Y ) ·(cid:0)1 +(cid:112)κ(cid:48)/n(cid:1) + k2d
(cid:16) knnz(X, Y ) ·(cid:0)1 +(cid:112)κ(cid:48)/n(cid:1) + k2d

√

(cid:17)
(cid:17)

or

(cid:101)O
(cid:101)O

gap
√

ε

in the gap-dependent and gap-free cases respectively.
Here, nnz(X, Y ) = nnz(X) + nnz(Y ) and κ(cid:48) =
2 maxi{(cid:107)Xi(cid:107)2,(cid:107)Yi(cid:107)2}
λmin(diag{Sxx,Syy}) where Xi or Yi is the i-th row vector
of X or Y . Therefore, our algorithm LazyCCA is doubly-
accelerated and stochastic.
We fully compare our results with prior work in Table 2
(for k = 1) and Table 1 (for k ≥ 1), and summarize our
main contributions:
• For k > 1, we outperform all relevant prior works (see
Table 1). Moreover, no known method was doubly-
accelerated even in the non-stochastic setting.

• For k ≥ 1, we obtain the ﬁrst gap-free running time.
• Even for k = 1, we outperform most of the state-of-

the-arts (see Table 2).

Note that for CCA with k > 1, previous result CCALin
only outputs the subspace spanned by the top k correlation
vectors but does not identify which vector gives the highest
correlation and so on. Our LazyCCA provides per-vector

2If a method depends on 1/ε then one can choose ε = gap

and this translates to a gap-dependent running time.

our k-GenEV result in non-stochastic running time.

3Recalling Footnote 1, for notational simplicity, we only state

4Throughout the paper, we use the (cid:101)O notation to hide poly-

logarithmic factors with respect to κ, 1/gap, 1/ε, d, n. We use
nnz(M ) to denote the time needed to multiply M to a vector.

Problem

Paper

k-GenEV

GenELin (Ge et al., 2016)

LazyEV Theorem 4.3
LazyEV Theorem 4.4

Problem

Paper

k-CCA

AppGrad (Ma et al., 2015)
CCALin (Ge et al., 2016)

CCALin (Ge et al., 2016)

LazyCCA (arXiv version)

LazyCCA (arXiv version)

LazyCCA (arXiv version)
LazyCCA (arXiv version)

(cid:1)
(cid:1)
(cid:1)

gap
√

+ knnz(A)+k2d
+ knnz(A)+k2d
+ knnz(A)+k2d

gap
√

ε

κB

κB

(cid:1)

√
√

gap
√

gap
√
√
ε

Running time

Running time
κB

(cid:101)O(cid:0) knnz(B)
(cid:101)O(cid:0) knnz(B)
(cid:101)O(cid:0) knnz(B)
(cid:101)O(cid:0) knnz(X,Y )·κ+k2d
(cid:101)O(cid:0) knnz(X,Y )·√
(cid:101)O(cid:0) knnz(X,Y )·(cid:0)
(cid:101)O(cid:0) knnz(X,Y )·(cid:0)
(cid:101)O(cid:0) knnz(X,Y )·(cid:0)
(cid:101)O(cid:0)knnz(X, Y ) ·(cid:0)1 +
(cid:101)O(cid:0)knnz(X, Y ) ·(cid:0)1 +

√
κ(cid:48)/n
√
κ(cid:48)/n
√
κ(cid:48)/n

1+
gap
1+
√

gap
1+
√

κ+k2d

(cid:1)

gap

gap

ε

(cid:1)
(cid:1)
(cid:1)

√

√

(× for outperformed)
×
(local conv.)
×
×

(cid:1)
(cid:1)
(cid:1)

+k2d

+k2d

+k2d

(cid:1)(cid:1)
(cid:1)(cid:1)

√

κ(cid:48)

√

gap·σk·(nnz(X,Y )/kd)1/4
ε·σk·(nnz(X,Y )/kd)1/4

κ(cid:48)

Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition

(× for outperformed)
×

gap-free?

negative EV?

no
no
yes

no
yes
yes

gap-free?

stochastic?

no
no

no

no

yes

no

yes

no
no

yes

yes

yes

doubly

doubly

Table 1: Performance comparison on k-GenEV and k-CCA.

λk

∈ [0, 1] and κB = λmax(B)
λmin(B) > 1.
∈ [0, 1], κ = λmax(diag{Sxx,Syy})

In GenEV, gap = λk−λk+1
In CCA, gap = σk−σk+1
Remark 1. Stochastic methods depend on a modiﬁed condition number κ(cid:48). The reason κ(cid:48) ∈ [κ, 2nκ] is in Fact 2.5.

Remark 2. All non-stochastic CCA methods in this table have been outperformed because 1 +(cid:112)κ(cid:48)/n ≤ O(κ).

λmin(diag{Sxx,Syy}) > 1, κ(cid:48) = 2 maxi{(cid:107)Xi(cid:107)2,(cid:107)Yi(cid:107)2}

λmin(diag{Sxx,Syy}) ∈ [κ, 2nκ], and σk ∈ [0, 1].

σk

Remark 3. Doubly-stochastic methods are not necessarily interesting. We discuss them in Section 1.2.

√
κ(cid:48)/nc√

guarantees on all the top k correlation vectors.
1.2 Our Side Results on Doubly-Stochastic Methods
Recall that when considering acceleration, there are two
parameters κ and 1/gap. One can also design stochas-
tic methods with respect to both parameters κ and 1/gap,
meaning that

gap

κ(cid:48)/n
gap

with a running time proportional to 1 +

(stochastic) or

√
√
κ√
√
instead of 1+
gap (non-stochastic).
The constant c is usually 1/2. We call such methods
doubly-stochastic.
Unfortunately, doubly-stochastic methods are usually
slower than stochastic ones. Take 1-CCA as an example.
The best stochastic running time (obtained exclusively by

us) for 1-CCA is nnz(X, Y ) · (cid:101)O(cid:0) 1+
nnz(X, Y ) · (cid:101)O(cid:0)1 +

(cid:1). In contrast,
(cid:1). Therefore, for 1-CCA,

if one uses a doubly-stochastic method —either (Wang
et al., 2016) or our LazyCCA— the running time becomes

√
κ(cid:48)/n1/4
√
gap·σ1

κ(cid:48)/n
gap

√
√

doubly-stochastic methods are faster than stochastic ones

only when

≤ o(n1/2) .

κ(cid:48)
σ1

The above condition is usually not satisﬁed. For instance,
• κ(cid:48) is usually around n for most interesting data-sets, cf.

the experiments of (Shalev-Shwartz & Zhang, 2014);
• κ(cid:48) is between n1/2 and 100n in all the CCA experi-

ments of (Wang et al., 2016); and

• by Fact 2.5 it satisﬁes κ(cid:48) ≥ d so κ(cid:48) cannot be smaller
than o(n1/2) unless d (cid:28) n1/2.5 Even worse, parameter
σ1 ∈ [0, 1] is usually much smaller than 1. Note that σ1
is scaling invariant: even if one scales X and Y up by
the same factor, σ1 remains unchanged.

Nevertheless, to compare our LazyCCA with all relevant
prior works, we obtain doubly-stochastic running times for
k-CCA as well. Our running time matches that of (Wang
et al., 2016) when k = 1, and no doubly-stochastic running
time for k > 1 was known before our work.

1.3 Other Related Works
For the easier task of PCA and SVD, the ﬁrst gap-free
result was obtained by Musco and Musco (Musco &
Musco, 2015), the ﬁrst stochastic result was obtained by
Shamir (Shamir, 2015), and the ﬁrst accelerated stochas-
tic result was obtained by Garber et al. (Garber & Hazan,
2015; Garber et al., 2016). The shift-and-invert precondi-
tioning technique of Garber et al. is also used in this paper.
For another
related problem PCR (principle compo-
5Note that item (3) κ(cid:48) ≥ d may not hold in the more general

setting of CCA, see Remark A.1.

Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition

(× for outperformed)
×

(cid:1)
(cid:1)
(cid:1)

ε

ε

gap

gap

√

κB

κ
gap

Running time

+ nnz(A)
gap
√
gap + nnz(A)√
√
gap
√
κB√
+ nnz(A)√

Running time
κB

(cid:101)O(cid:0) nnz(B)
(cid:101)O(cid:0) nnz(B)
(cid:101)O(cid:0) nnz(B)
(cid:1)
nnz(X, Y ) · (cid:101)O(cid:0) κ
(cid:1)
nnz(X, Y ) · (cid:101)O(cid:0) √
nnz(X, Y ) · (cid:101)O(cid:0) √
(cid:1)
nnz(X, Y ) · (cid:101)O(cid:0) √
nnz(X, Y ) · (cid:101)O(cid:0) 1+
nnz(X, Y ) · (cid:101)O(cid:0) 1+
nnz(X, Y ) · (cid:101)O(cid:0) 1+
nnz(X, Y ) · (cid:101)O(cid:0) 1+
(cid:16)
nnz(X, Y ) · (cid:101)O
nnz(X, Y ) · (cid:101)O(cid:0)1 +
nnz(X, Y ) · (cid:101)O(cid:0)1 +

(× for outperformed)
×
×
×
×
×
×

κ(cid:48)/n

(cid:1)

gap

κ(cid:48)/n

(cid:1)
(cid:1)
(cid:1)
(cid:1)

κ
gap2
κ√
√
gap·σ1
√
√
gap2
κ(cid:48)/n
√
√
gap
κ(cid:48)/n√
ε
√
κ(cid:48)/n1/4
√
gap·σ1

1 +

(see Remark 3)

(cid:17)
(cid:1)
(cid:1)

√
κ(cid:48)/n1/4
√
gap·σ1
√
κ(cid:48)/n1/4
√
ε·σ1

Problem

Paper

1-GenEV

GenELin (Ge et al., 2016)

LazyEV Theorem 4.3
LazyEV Theorem 4.4

Problem

Paper

1-CCA

AppGrad (Ma et al., 2015)

CCALin (Ge et al., 2016)

ALS (Wang et al., 2016)

SI (Wang et al., 2016)

CCALin (Ge et al., 2016)

ALS (Wang et al., 2016)
LazyCCA (arXiv version)
LazyCCA (arXiv version)

SI (Wang et al., 2016)

LazyCCA (arXiv version)
LazyCCA (arXiv version)

gap-free?

negative EV?

no

no

yes

no

yes

yes

gap-free?

stochastic?

no

no

no

no

no

no

no

yes

no

no

yes

no

no

no

no

yes

yes

yes

yes

doubly

doubly

doubly

Table 2: Performance comparison on 1-GenEV and 1-CCA.

In GenEV, gap = λ1−λ2
In CCA, gap = σ1−σ2

λ1

∈ [0, 1] and κB = λmax(B)
λmin(B) > 1.
∈ [0, 1], κ = λmax(diag{Sxx,Syy})

σ1

λmin(diag{Sxx,Syy}) > 1, κ(cid:48) = 2 maxi{(cid:107)Xi(cid:107)2,(cid:107)Yi(cid:107)2}

λmin(diag{Sxx,Syy}) ∈ [κ, 2nκ], and σ1 ∈ [0, 1].

Remark 1. Stochastic methods depend on modiﬁed condition number κ(cid:48); the reason κ(cid:48) ∈ [κ, 2nκ] is in Def. 2.4.

Remark 2. All non-stochastic CCA methods in this table have been outperformed because 1 +(cid:112)κ(cid:48)/n ≤ O(κ).

Remark 3. Doubly-stochastic methods are not necessarily interesting. We discuss them in Section 1.2.
Remark 4. Some CCA methods have a running time dependency on σ1 ∈ [0, 1], and this is intrinsic and cannot be removed.
In particular, if we scale the data matrix X and Y , the value σ1 stays the same.
Remark 5. The only (non-doubly-stochastic) doubly-accelerated method before our work is SI (Wang et al., 2016) (for 1-CCA

only). Our LazyEV is faster than theirs by a factor Ω((cid:112)nκ/κ(cid:48) ×(cid:112)1/σ1). Here, nκ/κ(cid:48) ≥ 1/2 and 1/σ1 ≥ 1 are two

scaling-invariant quantities usually much greater than 1.

nent regression), we recently obtained an accelerated
method (Allen-Zhu & Li, 2017) as opposed the previously
non-accelerated one (Frostig et al., 2016); however, the ac-
celeration techniques there are not relevant to this paper.
For GenEV and CCA, many scalable algorithms have been
designed recently (Ma et al., 2015; Wang & Livescu, 2015;
Michaeli et al., 2015; Witten et al., 2009; Lu & Foster,
2014). However, as summarized by the authors of CCALin,
these cited methods are more or less heuristics and do not
have provable guarantees. Furthermore, for k > 1, the
AppGrad method (Ma et al., 2015) only provides local con-
vergence guarantees and thus requires a warm-start whose

computational complexity is not discussed in their paper.
Finally, our algorithms on GenEV and CCA are based on
ﬁnding vectors one-by-one, which is advantageous in prac-
tice because one does not need k to be known and can stop
the algorithm whenever the eigenvalues (or correlation val-
ues) are too small. Known approaches for k > 1 cases
(such as GenELin, CCALin, AppGrad) ﬁnd all k vectors at
once, therefore requiring k to be known beforehand. As a
separate note, these known approaches do not need the user
to know the desired accuracy a priori but our LazyEV and
LazyCCA algorithms do.

Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition

2 Preliminaries
We denote by (cid:107)x(cid:107) or (cid:107)x(cid:107)2 the Euclidean norm of vector
x. We denote by (cid:107)A(cid:107)2, (cid:107)A(cid:107)F , and (cid:107)A(cid:107)Sq respectively the
spectral, Frobenius, and Schatten q-norm of matrix A (for
q ≥ 1). We write A (cid:23) B if A, B are symmetric and A− B
is positive semi-deﬁnite (PSD), and write A (cid:31) B if A, B
are symmetric but A − B is positive deﬁnite (PD). We de-
note by λmax(M ) and λmin(M ) the largest and smallest
eigenvalue of a symmetric matrix M, and by κM the con-
dition number λmax(M )/λmin(M ) of a PSD matrix M.
Throughout this paper, we use nnz(M ) to denote the time
to multiply matrix M to any arbitrary vector. For two ma-
trices X, Y , we denote by nnz(X, Y ) = nnz(X)+nnz(Y ),
and by Xi or Yi the i-th row vector of X or Y . We
also use poly(x1, x2, . . . , xt) to represent a quantity that
is asymptotically at most polynomial in terms of vari-
ables x1, . . . , xt. Given a column orthonormal matrix
U ∈ Rn×k, we denote by U⊥ ∈ Rn×(n−k) the column
orthonormal matrix consisting of an arbitrary basis in the
space orthogonal to the span of U’s columns.
Given a PSD matrix B and a vector v, v(cid:62)Bv is the B-semi-
norm of v. Two vectors v, w are B-orthogonal if v(cid:62)Bw =
0. We denote by B−1 the Moore-Penrose pseudoinverse
of B if B is not invertible, and by B1/2 the matrix square
root of B (satisfying B1/2 (cid:23) 0). All occurrences of B−1,
B1/2 and B−1/2 are for analysis purpose only. Our ﬁnal
algorithms only require multiplications of B to vectors.

Deﬁnition 2.1 (GenEV). Given symmetric matrices
A, B ∈ Rd×d where B is positive deﬁnite. The general-
ized eigenvectors of A with respect to B are v1, . . . , vd,
where each vi is
vi ∈ arg max
v∈Rd

(cid:40)(cid:12)(cid:12)v(cid:62)Av(cid:12)(cid:12) s.t. v(cid:62)Bv = 1

v(cid:62)Bvj = 0 ∀j ∈ [i − 1]

(cid:41)

The generalized eigenvalues λ1, . . . , λd satisfy λi =
v(cid:62)
i Avi which can be negative.

Following (Wang et al., 2016; Garber & Hazan, 2015), we
assume without loss of generality that λi ∈ [−1, 1].
Deﬁnition 2.2 (CCA). Given X ∈ Rn×dx , Y ∈ Rn×dy,
n Y (cid:62)Y ,
letting Sxx = 1
the canonical-correlation vectors are {(φi, ψi)}r
where r = min{dx, dy} and for all i ∈ [r]:

n X(cid:62)X, Sxy = 1

n X(cid:62)Y , Syy = 1

i=1

(cid:40)

(φi, ψi) ∈ arg max
φ∈Rdx ,ψ∈Rdy

(cid:110) φ(cid:62)Sxxφ = 1 ∧ φ(cid:62)Sxxφj = 0 ∀j ∈ [i − 1]

φ(cid:62)Sxyψ such that

ψ(cid:62)Syyψ = 1 ∧ ψ(cid:62)Syyψj = 0 ∀j ∈ [i − 1]

(cid:111)(cid:41)

The corresponding canonical-correlation coefﬁcients
σ1, . . . , σr satisfy σi = φ(cid:62)

i Sxyψi ∈ [0, 1].

(cid:16) Sxx 0

We emphasize that σi always lies in [0, 1] and is scaling-
invariant. When dealing with a CCA problem, we also de-
note by d = dx + dy.
Lemma 2.3 (CCA to GenEV). Given a CCA problem with
matrices X ∈ Rn×dx , Y ∈ Rn×dy, let the canonical-
correlation vectors and coefﬁcients be {(φi, ψi, σi)}r
where r = min{dx, dy}. Deﬁne A =
B =
spect to B has 2r eigenvalues {±σi}r
ing generalized eigenvectors
,
maining dx + dy − 2r eigenvalues are zeros.
Deﬁnition 2.4.
Lemma 2.3. We deﬁne condition numbers

. Then, the GenEV problem of A with re-
i=1 and correspond-
. The re-

(cid:16) 0 Sxy
(cid:17)(cid:111)n
(cid:16) −φi

In CCA, let A and B be as deﬁned in

(cid:110)(cid:16) φi

i=1
and

(cid:17)

(cid:17)

(cid:17)

0 Syy

S(cid:62)

i=1

ψi

ψi

xy

0

λmin(B)

κ def= κB = λmax(B)

λmin(B) and κ(cid:48) def= 2 maxi{(cid:107)Xi(cid:107)2,(cid:107)Yi(cid:107)2}
.
Fact 2.5. κ(cid:48) ∈ [κ, 2nκ] and κ(cid:48) ≥ d. (See full version.)
Lemma 2.6. Given matrices X ∈ Rn×dx , Y ∈ Rn×dy,
let A and B be as deﬁned in Lemma 2.3. For every w ∈
Rd, the Katyusha method (Allen-Zhu, 2017) ﬁnds a vector
w(cid:48) ∈ Rd satisfying (cid:107)w(cid:48) − B−1Aw(cid:107) ≤ ε in time
κ(cid:107)w(cid:107)2

nnz(X, Y ) ·(cid:0)1 +(cid:112)κ(cid:48)/n(cid:1) · log

(cid:16)

(cid:17)

O

.

ε

3 Leading Eigenvector via Two-Sided

Shift-and-Invert

We introduce AppxPCA±, the multiplicative approximation
algorithm for computing the two-sided leading eigenvector
of a symmetric matrix. AppxPCA± uses the shift-and-invert
framework (Garber & Hazan, 2015; Garber et al., 2016),
and shall become our building block for the LazyEV and
LazyCCA algorithms in the subsequent sections.
Our pseudo-code Algorithm 1 is a modiﬁcation of Algo-
rithm 5 in (Garber & Hazan, 2015), and reduces the eigen-
vector problem to oracle calls to an arbitrary matrix inver-
sion oracle A. The main differences between AppxPCA±
and (Garber & Hazan, 2015) are two-fold.
First, given a symmetric matrix M, AppxPCA± simultane-
ously considers an upper-bounding shift together with a
lower-bounding shift, and try to perform power methods
with respect to (λI − M )−1 and (λI + M )−1. This al-
lows us to determine approximately how close λ is to the
largest and the smallest eigenvalues of M, and decrease λ
accordingly. In the end, AppxPCA± outputs an approximate
eigenvector of M that corresponds to a negative eigenvalue
if needed. Second, we provide a multiplicative-error guar-
antee rather than additive as appeared in (Garber & Hazan,
2015). Without such guarantee, our ﬁnal running time will
depend on

gap·λmax(M ) rather than 1

gap.6

1

6This is why the SI method of (Wang et al., 2016) also uses

Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition

p2

48

p2ε

64m1

Algorithm 1 AppxPCA±(A, M, δ×, ε, p)
Input: A, an approximate matrix inversion method; M ∈ Rd×d, a symmetric matrix satisfying −I (cid:22) M (cid:22) I; δ× ∈
(cid:5) (cid:98)w0 is a random unit vector, see Def. 3.2
(0, 0.5], a multiplicative error; ε ∈ (0, 1), a numerical accuracy parameter; and p ∈ (0, 1), the conﬁdence parameter.
(cid:5) θ is the parameter of RanInit, see Def. 3.2

1: (cid:98)w0 ← RanInit(d); s ← 0; λ(0) ← 1 + δ×;
(cid:1)(cid:7), m2 ←(cid:6) log(cid:0) 36dθ
2: m1 ←(cid:6)4 log(cid:0) 288dθ
(cid:1)(cid:7);
(cid:1)m2
(cid:0) δ×
(cid:1)m1 and(cid:101)ε2 ← ε
(cid:0) δ×
3: (cid:101)ε1 ← 1
(cid:13)(cid:13) ≤(cid:101)ε1;
Apply A to ﬁnd (cid:98)wt satisfying(cid:13)(cid:13)(cid:98)wt − (λ(s−1)I − M )−1(cid:98)wt−1
wa ← (cid:98)wm1/(cid:107)(cid:98)wm1(cid:107);
(cid:5) wa is roughly (λ(s−1)I − M )−m1(cid:98)w0 then normalized
(cid:13)(cid:13) ≤(cid:101)ε1;
Apply A to ﬁnd va satisfying(cid:13)(cid:13)va − (λ(s−1)I − M )−1wa
Apply A to ﬁnd (cid:98)wt satisfying(cid:13)(cid:13)(cid:98)wt − (λ(s−1)I + M )−1(cid:98)wt−1
(cid:13)(cid:13) ≤(cid:101)ε1;
wb ← (cid:98)wm1/(cid:107)(cid:98)wm1(cid:107);
(cid:5) wb is roughly (λ(s−1)I + M )−m1(cid:98)w0 then normalized
Apply A to ﬁnd vb satisfying(cid:13)(cid:13)vb − (λ(s−1)I + M )−1wb
(cid:13)(cid:13) ≤(cid:101)ε1;

(cid:5) m1 = T PM(8, 1/32, p) and m2 = T PM(2, ε/4, p), see Lemma B.1

s ← s + 1;
for t = 1 to m1 do

for t = 1 to m1 do

8m2

48

max{w(cid:62)

a va,w(cid:62)

1

b vb}−(cid:101)ε1

and λ(s) ← λ(s−1) − ∆(s)
2 ;

2 ·
∆(s) ← 1

4: repeat
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15: until ∆(s) ≤ δ×λ(s)
16: f ← s;
17: if the last w(cid:62)
18:
19:
20:
21: else
22:
23:
24:
25: end if

12

b vb then

a va ≥ w(cid:62)
for t = 1 to m2 do

Apply A to ﬁnd (cid:98)wt satisfying(cid:13)(cid:13)(cid:98)wt − (λ(f )I − M )−1(cid:98)wt−1
return (+, w) where w def= (cid:98)wm2 /(cid:107)(cid:98)wm2(cid:107).
Apply A to ﬁnd (cid:98)wt satisfying(cid:13)(cid:13)(cid:98)wt − (λ(f )I + M )−1(cid:98)wt−1
return (−, w) where w def= (cid:98)wm2/(cid:107)(cid:98)wm2(cid:107).

for t = 1 to m2 do

(cid:13)(cid:13) ≤(cid:101)ε2;
(cid:13)(cid:13) ≤(cid:101)ε2;

(cid:17)

λ∗(cid:94) (cid:88)

We prove in full version the following theorem:
Theorem 3.1 (AppxPCA±, informal). Let M ∈ Rd×d be a
symmetric matrix with eigenvalues 1 ≥ λ1 ≥ ··· ≥ λd ≥
−1 and eigenvectors u1, . . . , ud. Let λ∗ = max{λ1,−λd}.
With probability at least 1 − p, AppxPCA± produces a pair
(sgn, w) satisfying
• if sgn = +, then w is an approx. positive eigenvector:
(w(cid:62)ui)2 ≤ ε

w(cid:62)M w ≥(cid:16)
w(cid:62)M w ≤ −(cid:16)
The number of oracle calls to A is (cid:101)O(log(1/δ×)), and each

• if sgn = −, then w is an approx. negative eigenvector:

λ∗(cid:94) (cid:88)

λi≥−(1−δ×/2)λ∗

λi≤(1−δ×/2)λ∗

1− δ×
2

1− δ×
2

(cid:17)

i∈[d]

i∈[d]

(w(cid:62)ui)2 ≤ ε

time we call A it satisﬁes
shift-and-invert but depends on

1

gap·σ1

in Table 2.

• λmax(λ(s)I−M )
•

λmin(λ(s)I−M ) , λmax(λ(s)I+M )
λmin(λ(s)I−M ) ,

λmin(λ(s)I+M ) ∈ [1, 96
λmin(λ(s)I+M ) ≤ 48
δ×λ∗ .

1

1

δ× ] and

We remark here that, unlike the original shift-and-invert
method which chooses a random (Gaussian) unit vector in
Line 1 of AppxPCA±, we have allowed this initial vector to
be generated from an arbitrary θ-conditioned random vec-
tor generator (for later use), deﬁned as follows:
Deﬁnition 3.2.
An algorithm RanInit(d) is a θ-
conditioned random vector generator if w = RanInit(d)
is a d-dimensional unit vector and, for every p ∈ (0, 1),
every unit vector u ∈ Rd, with probability at least 1 − p, it
satisﬁes (u(cid:62)w)2 ≤ p2θ
9d .
This modiﬁcation is needed in order to obtain our efﬁcient
implementations of GenEV and CCA. One can construct a
θ-conditioned random vector generator as follows:
Proposition 3.3. Given a PSD matrix B ∈ Rd×d, if we set
RanInit(d) def= B1/2v
(v(cid:62)Bv)0.5 where v is a random Gaussian
vector, then RanInit(d) is a θ-conditioned random vector

Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition

generator for θ = κB.
4 LazyEV: Generalized Eigendecomposition
In this section, we construct an algorithm LazyEV that,
given symmetric matrix M ∈ Rd×d, computes approxi-
mately the k leading eigenvectors of M that have the largest
absolute eigenvalues. Then, for the original k-GenEV
problem, we set M = B−1/2AB−1/2 and run LazyEV.
This is our plan to ﬁnd the top k leading generalized eigen-
vectors of A with respect to B.
Our algorithm LazyEV is formally stated in Algorithm 2.
The algorithm applies k times AppxPCA±, each time com-
puting an approximate leading eigenvector of M with a
multiplicative error δ×/2, and projects the matrix M into
the orthogonal space with respect to the obtained leading
eigenvector. We state our main approximation theorem be-
low.
Theorem 4.1 (informal). Let M ∈ Rd×d be a symmetric
matrix with eigenvalues λ1, . . . , λd ∈ [−1, 1] and corre-
sponding eigenvectors u1, . . . , ud, and |λ1| ≥ ··· ≥ |λd|.
If εpca is sufﬁciently small,7 LazyEV outputs a (column) or-
thonormal matrix Vk = (v1, . . . , vk) ∈ Rd×k which, with
probability at least 1 − p, satisﬁes:
k U(cid:107)2 ≤ ε where U = (uj, . . . , ud) and j is the
(a) (cid:107)V (cid:62)
1−δ×|λi|.
(b) For every i ∈ [k], (1−δ×)|λi| ≤ |v(cid:62)
Above, property (a) ensures the k columns of Vk have neg-
ligible correlation with the eigenvectors of M whose ab-
solute eigenvalues are ≤ (1 − δ×)λk; property (b) ensures
i M vi are all correct up to a 1±δ×
the Rayleigh quotients v(cid:62)
error. We in fact have shown two more useful properties in
the full version that may be of independent interest.
The next theorem states that, if M = B−1/2AB−1/2, our
LazyEV can be implemented without the necessity to com-
pute B1/2 or B−1/2.
Theorem 4.2 (running time). Let A, B ∈ Rd×d be two
symmetric matrices satisfying B (cid:31) 0 and −B (cid:22) A (cid:22) B.
Suppose M = B−1/2AB−1/2 and RanInit(d) is deﬁned
in Proposition 3.3 with respect to B. Then, the computa-
tion of V ← B−1/2LazyEV(A, M, k, δ×, εpca, p) can be
implemented to run in time

smallest index satisfying |λj| ≤ (1 − δ×)λk.

i M vi| ≤ 1

δ×

√

(cid:17)

B−1A to a vector, or

(cid:16) knnz(B)+k2d+kΥ
• (cid:101)O
(cid:17)
(cid:16) k
• (cid:101)O
7Meaning εpca ≤ O(cid:0)poly(ε, δ×,

dient to multiply B−1A to a vector.
|λ1|
|λk+1| , 1

κB nnz(B)+knnz(A)+k2d

√

√

δ×

if we use Conjugate gra-

d )(cid:1). The complete

where Υ is the time to multiply

speciﬁcations of εpca is included in the full version. Since our ﬁnal
running time only depends on log(1/εpca), we have not attempted
to improve the constants in this polynomial dependency.

Choosing parameter δ× as either gap or ε, our two main
theorems above immediately imply the following results
for the k-GenEV problem: (proved in full version)
Theorem 4.3 (gap-dependent GenEV, informal).
Let
A, B ∈ Rd×d be two symmetric matrices satisfying B (cid:31)
0 and −B (cid:22) A (cid:22) B. Suppose the generalized eigen-
value and eigenvector pairs of A with respect to B are
i=1, and it satisﬁes 1 ≥ |λ1| ≥ ··· ≥ |λd|.
{(λi, ui)}d
Then, LazyEV outputs V k ∈ Rd×k satisfying
(cid:17)

κBnnz(B) + knnz(A) + k2d

(cid:62)
k BW(cid:107)2 ≤ ε

(cid:62)
k BV k = I

and (cid:107)V

(cid:16) k

√

V

in time

(cid:101)O

√

gap

Here, W = (uk+1, . . . , ud) and gap =

|λk|−|λk+1|

|λk|

.

Theorem 4.4 (gap-free GenEV, informal).
In the same
setting as Theorem 4.3, our LazyEV outputs V k =
(v1, . . . , vk) ∈ Rd×k satisfying V

(cid:62)
k BV k = I and

(cid:12)(cid:12) ∈(cid:104)

∀s ∈ [k] :(cid:12)(cid:12)v(cid:62)
(cid:16) k
(cid:101)O

in time

s Avs
√

(cid:105)

|λs|
1 − ε

(1 − ε)|λs|,

κBnnz(B) + knnz(A) + k2d

√

ε

(cid:17)

.

Ideas Behind Theorem 4.1

Ideas Behind Theorems 4.1 and 4.2

5
In Section 5.1 we discuss how to ensure accuracy: that is,
why does LazyEV guarantee to approximately ﬁnd the top
eigenvectors of M. In the full version of this paper, we also
discuss how to implement LazyEV without compute B1/2
explicitly, thus proving Theorem 4.2.
5.1
Our approximation guarantee in Theorem 4.1 is a nat-
ural generalization of the recent work on fast iterative
methods to ﬁnd the top k eigenvectors of a PSD ma-
trix M (Allen-Zhu & Li, 2016). That method is called
LazySVD and we summarize it as follows.
At a high level, LazySVD ﬁnds the top k eigenvectors one-
by-one and approximately. Starting with M0 = M, in the
s-th iteration where s ∈ [k], LazySVD computes approxi-
mately the leading eigenvector of matrix Ms−1 and call it
vs. Then, LazySVD projects Ms ← (I − vsv(cid:62)
s )Ms−1(I −
vsv(cid:62)
While the algorithmic idea of LazySVD is simple, the anal-
ysis requires some careful linear algebraic lemmas. Most
notably, if vs is an approximate leading eigenvector of
Ms−1, then one needs to prove that the small eigenvectors
of Ms−1 somehow still “embed” into that of Ms after pro-
jection. This is achieved by a gap-free variant of the Wedin
theorem plus a few other technical lemmas, and we rec-
ommend interested readers to see the high-level overview
section of (Allen-Zhu & Li, 2016).

s ) and proceeds to the next iteration.

Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition

Algorithm 2 LazyEV(A, M, k, δ×, εpca, p)
Input: A, an approximate matrix inversion method; M ∈ Rd×d, a matrix satisfying −I (cid:22) M (cid:22) I; k ∈ [d], the desired
rank; δ× ∈ (0, 1), a multiplicative error; εpca ∈ (0, 1), a numerical accuracy; and p ∈ (0, 1), a conﬁdence parameter.
1: M0 ← M; V0 = [];
2: for s = 1 to k do
3:
4:
5:
6:
7: end for
8: return Vk.

s is an approximate two-sided leading eigenvector of Ms−1
(cid:5) project v(cid:48)
s to V ⊥
s−1
s )M (I − VsV (cid:62)
s )

s) ← AppxPCA±(A, Ms−1, δ×/2, εpca, p/k);
s−1)v(cid:48)

vs ←(cid:0)(I − Vs−1V (cid:62)

(∼, v(cid:48)
Vs ← [Vs−1, vs];
Ms ← (I − vsv(cid:62)

(cid:1)/(cid:13)(cid:13)(I − Vs−1V (cid:62)

(cid:5) we also have Ms = (I − VsV (cid:62)

s−1)v(cid:48)

s

s )Ms−1(I − vsv(cid:62)
s )

(cid:13)(cid:13);

s

(cid:5) v(cid:48)

In this paper, to relax the assumption that M is PSD, and to
ﬁnd leading eigenvectors whose absolute eigenvalues are
large, we have to make several non-trivial changes. On
the algorithm side, LazyEV uses our two-sided shift-and-
invert method in Section 3 to ﬁnd the leading eigenvector
of Ms−1 with largest absolute eigenvalue. On the analysis
side, we have to make sure all lemmas properly deal with
negative eigenvalues. For instance:
• If we perform a projection M(cid:48) ← (I − vv(cid:62))M (I −
vv(cid:62)) where v correlates by at most ε with all eigenvec-
tors of M whose absolute eigenvalues are smaller than a
threshold µ, then, after the projection, we need to prove
that these eigenvectors can be approximately “embed-
ded” into the eigenspace spanned by all eigenvectors of
M(cid:48) whose absolute eigenvalues are smaller than µ + τ.
The approximation of this embedding should depend on
ε, µ and τ.

The full proof of Theorem 4.1 is in the arXiv version. It re-
lies on a few matrix algebraic lemmas (including the afore-
mentioned “embedding lemma”).
6 Conclusion
In this paper we propose new iterative methods to solve
the generalized eigenvector and the canonical correlation
analysis problems. Our methods ﬁnd the most signiﬁcant k
eigenvectors or correlation vectors, and have running times
that linearly scales with k.
Most importantly, our methods are doubly-accelerated: the
running times have square-root dependencies both with re-
spect to the condition number of the matrix (i.e., κ) and
with respect to the eigengap (i.e., gap). They are the ﬁrst
doubly-accelerated iterative methods at least for k > 1.
They can also be made gap-free, and are the ﬁrst gap-free
iterative methods even for 1-GenEV or 1-CCA.
Although this is a theory paper, we believe that if imple-
mented carefully, our methods can outperform not only
previous iterative methods (such as GenELin, AppGrad,
CCALin), but also the commercial mathematics libraries
for sparse matrices of dimension more than 10, 000. We

leave it a future work for such careful comparisons.
References
Allen-Zhu, Zeyuan. Katyusha: The First Direct Accelera-

tion of Stochastic Gradient Methods. In STOC, 2017.

Allen-Zhu, Zeyuan and Li, Yuanzhi. LazySVD: Even
Faster SVD Decomposition Yet Without Agonizing
Pain. In NIPS, 2016.

Allen-Zhu, Zeyuan and Li, Yuanzhi. Faster Principal Com-
ponent Regression and Stable Matrix Chebyshev Ap-
In Proceedings of the 34th International
proximation.
Conference on Machine Learning, ICML ’17, 2017.

Allen-Zhu, Zeyuan and Orecchia, Lorenzo. Linear Cou-
pling: An Ultimate Uniﬁcation of Gradient and Mirror
Descent. In Proceedings of the 8th Innovations in Theo-
retical Computer Science, ITCS ’17, 2017.

Allen-Zhu, Zeyuan and Yuan, Yang.

Improved SVRG
for Non-Strongly-Convex or Sum-of-Non-Convex Ob-
jectives. In ICML, 2016.

Allen-Zhu, Zeyuan, Richt´arik, Peter, Qu, Zheng, and Yuan,
Yang. Even faster accelerated coordinate descent using
non-uniform sampling. In ICML, 2016.

Arora, Sanjeev, Rao, Satish, and Vazirani, Umesh V. Ex-
pander ﬂows, geometric embeddings and graph parti-
tioning. Journal of the ACM, 56(2), 2009.

Aujol, J-F and Dossal, Ch. Stability of over-relaxations
for the forward-backward algorithm, application to ﬁsta.
SIAM Journal on Optimization, 25(4):2408–2433, 2015.

Axelsson, Owe. A survey of preconditioned iterative meth-
ods for linear systems of algebraic equations. BIT Nu-
merical Mathematics, 25(1):165–187, 1985.

Chaudhuri, Kamalika, Kakade, Sham M, Livescu, Karen,
and Sridharan, Karthik. Multi-view clustering via canon-
ical correlation analysis. In ICML, pp. 129–136, 2009.

Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition

Shalev-Shwartz, Shai and Zhang, Tong. Accelerated Prox-
imal Stochastic Dual Coordinate Ascent for Regularized
In Proceedings of the 31st Inter-
Loss Minimization.
national Conference on Machine Learning, ICML 2014,
pp. 64–72, 2014.

Shamir, Ohad. A Stochastic PCA and SVD Algorithm with
an Exponential Convergence Rate. In ICML, pp. 144—-
153, 2015.

Shewchuk, Jonathan Richard. An introduction to the conju-
gate gradient method without the agonizing pain, 1994.

Wang, Weiran and Livescu, Karen.

Large-scale ap-
proximate kernel canonical correlation analysis. arXiv
preprint, abs/1511.04773, 2015.

Wang, Weiran, Wang, Jialei, Garber, Dan, and Srebro,
Nathan. Efﬁcient Globally Convergent Stochastic Op-
timization for Canonical Correlation Analysis. In NIPS,
2016.

Witten, Daniela M, Tibshirani, Robert, and Hastie, Trevor.
A penalized matrix decomposition, with applications to
sparse principal components and canonical correlation
analysis. Biostatistics, pp. kxp008, 2009.

Dhillon, Paramveer, Foster, Dean P, and Ungar, Lyle H.
In

Multi-view learning of word embeddings via cca.
NIPS, pp. 199–207, 2011.

Frostig, Roy, Musco, Cameron, Musco, Christopher, and
Sidford, Aaron. Principal Component Projection With-
out Principal Component Analysis. In ICML, 2016.

Garber, Dan and Hazan, Elad. Fast and simple PCA via

convex optimization. ArXiv e-prints, September 2015.

Garber, Dan, Hazan, Elad, Jin, Chi, Kakade, Sham M.,
Musco, Cameron, Netrapalli, Praneeth, and Sidford,
Aaron. Robust shift-and-invert preconditioning: Faster
and more sample efﬁcient algorithms for eigenvector
computation. In ICML, 2016.

Ge, Rong, Jin, Chi, Kakade, Sham M., Netrapalli, Pra-
neeth, and Sidford, Aaron. Efﬁcient Algorithms for
Large-scale Generalized Eigenvector Computation and
Canonical Correlation Analysis. In ICML, 2016.

Golub, Gene H. and Van Loan, Charles F. Matrix Com-
ISBN

putations. The JHU Press, 4th edition, 2012.
1421407949.

Kakade, Sham M and Foster, Dean P. Multi-view regres-
sion via canonical correlation analysis. In Learning the-
ory, pp. 82–96. Springer, 2007.

Karampatziakis, Nikos and Mineiro, Paul. Discriminative
features via generalized eigenvectors. In ICML, pp. 494–
502, 2014.

Lu, Yichao and Foster, Dean P. Large scale canonical cor-
In NIPS,

relation analysis with iterative least squares.
pp. 91–99, 2014.

Ma, Zhuang, Lu, Yichao, and Foster, Dean. Finding linear
structure in large datasets with scalable canonical corre-
lation analysis. In ICML, pp. 169–178, 2015.

Michaeli, Tomer, Wang, Weiran, and Livescu, Karen.
arXiv

Nonparametric canonical correlation analysis.
preprint, abs/1511.04839, 2015.

Musco, Cameron and Musco, Christopher. Randomized
block krylov methods for stronger and faster approxi-
mate singular value decomposition. In NIPS, pp. 1396–
1404, 2015.

Nesterov, Yurii. A method of solving a convex program-
ming problem with convergence rate O(1/k2). In Dok-
lady AN SSSR (translated as Soviet Mathematics Dok-
lady), volume 269, pp. 543–547, 1983.

Shalev-Shwartz, Shai. SDCA without Duality, Regulariza-

tion, and Individual Convexity. In ICML, 2016.

