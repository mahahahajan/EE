Semi-Supervised Classiﬁcation

Based on Classiﬁcation from Positive and Unlabeled Data

Tomoya Sakai 1 2 Marthinus Christoffel du Plessis Gang Niu 1 Masashi Sugiyama 2 1

Abstract

Most of the semi-supervised classiﬁcation meth-
ods developed so far use unlabeled data for reg-
ularization purposes under particular distribu-
tional assumptions such as the cluster assump-
tion. In contrast, recently developed methods of
classiﬁcation from positive and unlabeled data
(PU classiﬁcation) use unlabeled data for risk
evaluation, i.e., label information is directly ex-
tracted from unlabeled data.
In this paper, we
extend PU classiﬁcation to also incorporate neg-
ative data and propose a novel semi-supervised
classiﬁcation approach. We establish general-
ization error bounds for our novel methods and
show that the bounds decrease with respect to
the number of unlabeled data without the distri-
butional assumptions that are required in existing
semi-supervised classiﬁcation methods. Through
experiments, we demonstrate the usefulness of
the proposed methods.

1. Introduction

Collecting a large amount of labeled data is a critical bottle-
neck in real-world machine learning applications due to the
laborious manual annotation.
In contrast, unlabeled data
can often be collected automatically and abundantly, e.g.,
by a web crawler. This has led to the development of vari-
ous semi-supervised classiﬁcation algorithms over the past
decades.

To leverage unlabeled data in training, most of the exist-
ing semi-supervised classiﬁcation methods rely on partic-
ular assumptions on the data distribution (Chapelle et al.,
2006). For example, the manifold assumption supposes that
samples are distributed on a low-dimensional manifold in
the data space (Belkin et al., 2006). In the existing frame-
work, such a distributional assumption is encoded as a reg-

1The University of Tokyo, Japan 2RIKEN, Japan. Correspon-

dence to: Tomoya Sakai <sakai@ms.k.u-tokyo.ac.jp>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

ularizer for training a classiﬁer and biases the classiﬁer to-
ward a better one under the assumption. However, if such a
distributional assumption contradicts the data distribution,
the bias behaves adversely, and the performance of the ob-
tained classiﬁer becomes worse than the one obtained with
supervised classiﬁcation (Cozman et al., 2003; Sokolovska
et al., 2008; Li & Zhou, 2015; Krijthe & Loog, 2017).

Recently, classiﬁcation from positive and unlabeled data
(PU classiﬁcation) has been gathering growing attention
(Elkan & Noto, 2008; du Plessis et al., 2014; 2015; Jain
et al., 2016), which trains a classiﬁer only from positive and
unlabeled data without negative data. In PU classiﬁcation,
the unbiased risk estimators proposed in du Plessis et al.
(2014; 2015) utilize unlabeled data for risk evaluation, im-
plying that label information is directly extracted from un-
labeled data without restrictive distributional assumptions,
unlike existing semi-supervised classiﬁcation methods that
utilize unlabeled data for regularization. Furthermore, the-
oretical analysis (Niu et al., 2016) showed that PU classi-
ﬁcation (or its counterpart, NU classiﬁcation, classiﬁcation
from negative and unlabeled data) is likely to outperform
classiﬁcation from positive and negative data (PN classi-
ﬁcation, i.e., ordinary supervised classiﬁcation) depending
on the number of positive, negative, and unlabeled sam-
ples. It is thus naturally expected that combining PN, PU,
and NU classiﬁcation can be a promising approach to semi-
supervised classiﬁcation without restrictive distributional
assumptions.

In this paper, we propose a novel semi-supervised classiﬁ-
cation approach by considering convex combinations of the
risk functions of PN, PU, and NU classiﬁcation. Without
any distributional assumption, we theoretically show that
the conﬁdence term of the generalization error bounds de-
creases at the optimal parametric rate with respect to the
number of positive, negative, and unlabeled samples, and
the variance of the proposed risk estimator is almost always
smaller than the plain PN risk function given an inﬁnite
number of unlabeled samples. Through experiments, we
analyze the behavior of the proposed approach and demon-
strate the usefulness of the proposed semi-supervised clas-
siﬁcation methods.

Semi-Supervised Classiﬁcation Based on Classiﬁcation from Positive and Unlabeled Data

2. Background

2.3. PU Classiﬁcation

In this section, we ﬁrst introduce the notation commonly
used in this paper and review the formulations of PN, PU,
and NU classiﬁcation.

2.1. Notation

Let random variables x ∈ Rd and y ∈ {+1,−1} be
equipped with probability density p(x, y), where d is a pos-
itive integer. Let us consider a binary classiﬁcation problem
from x to y, given three sets of samples called the posi-
tive (P), negative (N), and unlabeled (U) data:

i=1

XP := {xP
XN := {xN
XU := {xU

i }nP
i }nN
i }nU

i=1

i=1

i.i.d.∼ pP(x) := p(x | y = +1),
i.i.d.∼ pN(x) := p(x | y = −1),
i.i.d.∼ p(x) := θPpP(x) + θNpN(x),

where

θP := p(y = +1),

θN := p(y = −1)

are the class-prior probabilities for the positive and negative
classes such that θP + θN = 1.
Let g : Rd → R be an arbitrary real-valued decision
function for binary classiﬁcation, and classiﬁcation is per-
formed based on its sign. Let ℓ : R → R be a loss func-
tion such that ℓ(m) generally takes a small value for large
margin m = yg(x). Let RP(g), RN(g), RU,P(g), and
RU,N(g) be the risks of classiﬁer g under loss ℓ:

RP(g) := EP[ℓ(g(x))],
RN(g) := EN[ℓ(−g(x))],
RU,P(g) := EU[ℓ(g(x))], RU,N(g) := EU[ℓ(−g(x))],
where EP, EN, and EU denote the expectations over
pP(x), pN(x), and p(x), respectively. Since we do not
have any samples from p(x, y),
the true risk R(g) =
Ep(x,y)[ℓ(yg(x))], which we want to minimize, should be
recovered without using p(x, y) as shown below.

2.2. PN Classiﬁcation

In PU classiﬁcation, we do not have labeled data for the
negative class, but we can use unlabeled data drawn from
marginal density p(x). The goal of PU classiﬁcation is
to train a classiﬁer using only positive and unlabeled data.
The basic approach to PU classiﬁcation is to discriminate P
and U data (Elkan & Noto, 2008). However, naively clas-
sifying P and U data causes a bias.

To address this problem, du Plessis et al. (2014; 2015) pro-
posed a risk equivalent to the PN risk but where pN(x) is
not included. The key idea is to utilize unlabeled data to
evaluate the risk for negative samples in the PN risk. Re-
placing the second term in Eq. (1) with1

θN EN[ℓ(−g(x))] = EU[ℓ(−g(x))] − θP EP[ℓ(−g(x))],
we obtain the risk in PU classiﬁcation (the PU risk) as

= θPRC

P(g) + RU,N(g),

RPU(g) := θP EP[eℓ(g(x))] + EU[ℓ(−g(x))]
P(g) := EP[eℓ(g(x))] andeℓ(m) = ℓ(m) − ℓ(−m)

where RC
is a composite loss function.

(2)

Non-Convex Approach:

If the loss function satisﬁes

ℓ(m) + ℓ(−m) = 1,

(3)

the composite loss function becomeseℓ(m) = 2ℓ(m) − 1.

We thus obtain the non-convex PU risk as

RN-PU(g) := 2θPRP(g) + RU,N(g) − θP.

(4)

This formulation can be seen as cost-sensitive classiﬁcation
of P and U data with weight 2θP (du Plessis et al., 2014).

The ramp loss used in the robust support vector machine
(Collobert et al., 2006),

ℓR(m) :=

1
2

max(0, min(2, 1 − m)),

(5)

In standard supervised classiﬁcation (PN classiﬁcation), we
have both positive and negative data, i.e., fully labeled data.
The goal of PN classiﬁcation is to train a classiﬁer using
labeled data.

The risk in PN classiﬁcation (the PN risk) is deﬁned as

satisﬁes the condition (3). However,
the use of the
ramp loss (and any other losses that satisfy the condi-
tion (3)) yields a non-convex optimization problem, which
may be solved locally by the concave-convex procedure
(CCCP) (Yuille & Rangarajan, 2002; Collobert et al., 2006;
du Plessis et al., 2014).

RPN(g) := θP EP[ℓ(g(x))] + θN EN[ℓ(−g(x))]

= θPRP(g) + θNRN(g),

(1)

Convex Approach:
satisﬁes

If a convex surrogate loss function

which is equal to R(g), but p(x, y) is not included. If we
use the hinge loss function ℓH(m) := max(0, 1 − m), the
PN risk coincides with the risk of the support vector ma-
chine (Vapnik, 1995).

ℓ(m) − ℓ(−m) = −m,

(6)

1The equation comes from the deﬁnition of the marginal den-

sity p(x) = θPpP(x) + θNpN(x).

Semi-Supervised Classiﬁcation Based on Classiﬁcation from Positive and Unlabeled Data

the composite loss function becomes a linear function

eℓ(m) = −m (see Table 1 in du Plessis et al., 2015). We

thus obtain the convex PU risk as

RC-PU(g) := θPRL

P(g) + RU,N(g),

where RL
P(g) := EP[−g(x)] is the risk with the linear loss
ℓLin(m) := −m. This formulation yields the convex opti-
mization problem that can be solved efﬁciently.

2.4. NU Classiﬁcation

As a mirror of PU classiﬁcation, we can consider NU clas-
siﬁcation. The risk in NU classiﬁcation (the NU risk) is
given by

= θNRC

RNU(g) := θN EN[eℓ(−g(x))] + EU[ℓ(g(x))]
N(g) := EN[eℓ(−g(x))] is the risk function with

where RC
the composite loss. Similarly to PU classiﬁcation, the non-
convex and convex NU risks are expressed as

N(g) + RU,P(g),

RN-NU(g) := 2θNRN(g) + RU,P(g) − θN,
RC-NU(g) := θNRL

N(g) + RU,P(g),

(7)

(8)

where RL

N(g) := EN[g(x)] is the risk with the linear loss.

3. Semi-Supervised Classiﬁcation Based on

PN, PU, and NU Classiﬁcation

In this section, we propose semi-supervised classiﬁcation
methods based on PN, PU, and NU classiﬁcation.

3.1. PUNU Classiﬁcation

A naive idea to build a semi-supervised classiﬁer is to com-
bine the PU and NU risks. For γ ∈ [0, 1], let us consider a
linear combination of the PU and NU risks:

Rγ
PUNU(g) := (1 − γ)RPU(g) + γRNU(g).

We refer to this combined method as PUNU classiﬁcation.

If we use a loss function satisfying the condition (3), the
non-convex PUNU risk Rγ
N-PUNU(g) can be expressed as

Rγ
N-PUNU(g) = 2(1 − γ)θPRP(g) + 2γθNRN(g)

+ EU[(1 − γ)ℓ(−g(x)) + γℓ(g(x))]
− (1 − γ)θP − γθN.

Here, R1/2
N-PUNU(g) agrees with RPN(g) due to the condi-
tion (3). Thus, when γ = 1/2, PUNU classiﬁcation is re-
duced to ordinary PN classiﬁcation.

On the other hand, γ = 1/2 is still effective when the con-
dition (6) is satisﬁed. Its risk Rγ
C-PUNU(g) can be expressed
as

Rγ
C-PUNU(g) = (1 − γ)θPRL

P(g) + γθNRL

N(g)

+ EU[(1 − γ)ℓ(g(x)) + γℓ(−g(x))].

Here, (1 − γ)ℓ(g(x)) + γℓ(−g(x)) can be regarded as a
loss function for unlabeled samples with weight γ.

When γ = 1/2, unlabeled samples incur the same loss for
the positive and negative classes. On the other hand, when
0 < γ < 1/2, a smaller loss is incurred for the negative
class than the positive class. Thus, unlabeled samples tend
to be classiﬁed into the negative class. The opposite is true
when 1/2 < γ < 1.

3.2. PNU Classiﬁcation

Another possibility of using PU and NU classiﬁcation in
semi-supervised classiﬁcation is to combine the PN and
PU/NU risks. For γ ∈ [0, 1], let us consider linear com-
binations of the PN and PU/NU risks:

Rγ
PNPU(g) := (1 − γ)RPN(g) + γRPU(g),
Rγ
PNNU(g) := (1 − γ)RPN(g) + γRNU(g).

In practice, we combine PNPU and PNNU classiﬁcation
and adaptively choose one of them with a new trade-off
parameter η ∈ [−1, 1] as

PNU(g) :=(Rη

Rη

PNPU(g)
PNNU(g)

R−η

(η ≥ 0),
(η < 0).

We refer to the combined method as PNU classiﬁcation.
Clearly, PNU classiﬁcation with η = −1, 0, +1 corre-
sponds to NU, PN, and PU classiﬁcation. As η gets
large/small, the effect of the positive/negative classes is
more emphasized.

In the theoretical analyses in Section 4, we denote the
combinations of the PN risk with the non-convex PU/NU
risks by Rγ
N-PNNU, and that with the convex
PU/NU risks by Rγ

N-PNPU and Rγ

C-PNPU and Rγ

C-PNNU.

3.3. Practical Implementation

We have so far only considered the true risks R (with
respect to the expectations over true data distributions).
When a classiﬁer is trained from samples in practice, we

placed with corresponding sample averages.

use the empirical risks bR where the expectations are re-
model given by g(x) = Pb

More speciﬁcally, in the theoretical analysis in Section 4
and experiments in Section 5, we use a linear-in-parameter
j=1 wjφj(x) = w⊤φ(x),
where ⊤ denotes the transpose, b is the number of basis

Semi-Supervised Classiﬁcation Based on Classiﬁcation from Positive and Unlabeled Data

functions, w = (w1, . . . , wb)⊤ is a parameter vector, and
φ(x) = (φ1(x), . . . , φb(x))⊤ is a basis function vector.
The parameter vector w is learned in order to minimize the
ℓ2-regularized empirical risk:

min

w bR(g) + λw⊤w,

where λ ≥ 0 is the regularization parameter.

4. Theoretical Analyses

In this section, we theoretically analyze the behavior of
the empirical versions of the proposed semi-supervised
classiﬁcation methods. We ﬁrst derive generalization er-
ror bounds and then discuss variance reduction. Finally,
we discuss whether PUNU or PNU classiﬁcation is more
promising. All proofs can be found in Appendix A.

4.1. Generalization Error Bounds

Let G be a function class of bounded hyperplanes:
G = {g(x) = hw, φ(x)i | kwk ≤ Cw,kφ(x)k ≤ Cφ},
where Cw and Cφ are certain positive constants. Since ℓ2-
regularization is always included, we can naturally assume
that the empirical risk minimizer g belongs to a certain G.
Denote by ℓ0-1(m) = (1 − sign(m))/2 the zero-one loss
and I(g) = Ep(x,y)[ℓ0-1(yg(x))] the risk of g for binary
classiﬁcation, i.e., the generalization error of g. In the fol-
lowing, we study upper bounds of I(g) holding uniformly
for all g ∈ G. We respectively focus on the (scaled) ramp
and squared losses for the non-convex and convex methods
due to limited space. Similar results can be obtained with a
little more effort if other eligible losses are used. For con-
venience, we deﬁne a function as
χ(cP, cN, cU) = cPθP/√nP + cNθN/√nN + cU/√nU.

Non-Convex Methods: A key observation is
that
ℓ0-1(m) ≤ 2ℓR(m), and consequently I(g) ≤ 2R(g). Note
that by deﬁnition we have

Rγ

N-PUNU(g) = Rγ

N-PNPU(g) = Rγ

N-PNNU(g) = R(g).

The theorem below can be proven using the Rademacher
analysis (see, for example, Mohri et al., 2012; Ledoux &
Talagrand, 1991).

Theorem 1 Let ℓR(m) be the loss for deﬁning the empir-
ical risks. For any δ > 0, the following inequalities hold
separately with probability at least 1 − δ for all g ∈ G:

N-PUNU(g) + Cw,φ,δ · χ(2 − 2γ, 2γ,|2γ − 1|),
N-PNPU(g) + Cw,φ,δ · χ(1 + γ, 1 − γ, γ),
N-PNNU(g) + Cw,φ,δ · χ(1 − γ, 1 + γ, γ),

I(g) ≤ 2bRγ
I(g) ≤ 2bRγ
I(g) ≤ 2bRγ
where Cw,φ,δ = 2CwCφ +p2 ln(3/δ).

Theorem 1 guarantees that when ℓR(m) is used, I(g) can
be bounded from above by two times the empirical risks,
N-PNNU(g), plus

i.e., 2bRγ

the corresponding conﬁdence terms of order

N-PUNU(g), 2bRγ
N-PNPU(g), and 2bRγ
Op(1/√nP + 1/√nN + 1/√nU).

Since nP, nN, and nU can increase independently, this is al-
ready the optimal convergence rate without any additional
assumption (Vapnik, 1998; Mendelson, 2008).

Convex Methods: Analogously, we have ℓ0-1(m) ≤
4ℓS(m) for the squared loss. However, it is too loose when
|m| ≫ 0. Fortunately, we do not have to use ℓS(m) if we
work on the generalization error rather than the estimation
error. To this end, we deﬁne the truncated (scaled) squared
loss ℓTS(m) as

ℓTS(m) =(ℓS(m)

ℓ0-1(m)/4

0 < m ≤ 1,
otherwise,

so that ℓ0-1(m) ≤ 4ℓTS(m) is much tighter. For ℓTS(m),
RC-PU(g) and RC-NU(g) need to be redeﬁned as follows
(see du Plessis et al., 2015):

RC-PU(g) := θPR′
RC-NU(g) := θNR′

P(g) + RU,N(g),
N(g) + RU,P(g),

where R′

P(g) and R′

N(g) are simply RP(g) and RN(g)

w.r.t. the composite losseℓTS(m) = ℓTS(m) − ℓTS(−m).
The conditioneℓTS(m) 6= −m means the loss of convexity,

but the equivalence is not lost; indeed, we still have

Rγ

C-PUNU(g) = Rγ

C-PNPU(g) = Rγ

C-PNNU(g) = R(g).

Theorem 2 Let ℓTS(m) be the loss for deﬁning the empir-
ical risks (where RC-PU(g) and RC-NU(g) are redeﬁned).
For any δ > 0, the following inequalities hold separately
with probability at least 1 − δ for all g ∈ G:

C-PUNU(g) + C ′
C-PNPU(g) + C ′
C-PNNU(g) + C ′

I(g) ≤ 4bRγ
I(g) ≤ 4bRγ
I(g) ≤ 4bRγ
w,φ,δ = 4CwCφ +p2 ln(4/δ).

w,φ,δ · χ(1 − γ, γ, 1),
w,φ,δ · χ(1, 1 − γ, γ),
w,φ,δ · χ(1 − γ, 1, γ),

where C ′

Theorem 2 ensures that when ℓTS(m) is used (for evalu-
ating the empirical risks rather than learning the empirical
risk minimizers), I(g) can be bounded from above by four
times the empirical risks plus conﬁdence terms in the op-
timal parametric rate. As ℓTS(m) ≤ ℓS(m), Theorem 2 is
valid (but weaker) if all empirical risks are w.r.t. ℓS(m).

Semi-Supervised Classiﬁcation Based on Classiﬁcation from Positive and Unlabeled Data

4.2. Variance Reduction

Our empirical risk estimators proposed in Section 3 are all
unbiased. The next question is whether their variance can

be smaller than that of bRPN(g), i.e., whether XU can help

reduce the variance in estimating R(g). To answer this
question, pick any g of interest. For simplicity, we assume
that nU → ∞, to illustrate the maximum variance reduc-
tion that could be achieved. Due to limited space, we only
focus on the non-convex methods.

Similarly to RP(g) and RN(g), let σ2
corresponding variance:

P(g) and σ2

N(g) be the

σ2
P(g) := VarP[ℓ(g(x))],

σ2
N(g) := VarN[ℓ(−g(x))],
where VarP and VarN denote the variance over pP(x)
and pN(x). Moreover, denote by ψP = θ2
P(g)/nP
and ψN = θ2
N(g)/nN for short, and let Var be the
1 )··· pP(xP
variance over pP(xP
nN ) ·
p(xU

1 )··· pN(xN

nP ) · pN(xN

Nσ2

Pσ2

1 )··· p(xU

nU).

Theorem 3 Assume nU → ∞. For any ﬁxed g, let
ψP
γN-PUNU = argmin

N-PUNU(g)] =

. (9)

ψP + ψN

γ

Var[bRγ

γN-PUNU

∈

[0, 1].

have

Then, we

by bRγ

ther, Var[bRγ

N-PUNU(g)] < Var[bRPN(g)]

Fur-
for all
γ ∈ (2γN-PUNU − 1/2, 1/2) if ψP < ψN, or for all
γ ∈ (1/2, 2γN-PUNU − 1/2) if ψP > ψN.2
Theorem 3 guarantees that the variance is always reduced
N-PUNU(g) if γ is close to γN-PUNU, which is optimal
for variance reduction. The interval of such good γ val-
ues has the length min{|ψP − ψN|/(ψP + ψN), 1/2}. In
particular, if 3ψP ≤ ψN or ψP ≥ 3ψN, the length is 1/2.
Theorem 4 Assume nU → ∞. For any ﬁxed g, let
ψN − ψP
γN-PNPU= argmin
ψP + ψN
ψP − ψN
ψP + ψN

γN-PNNU= argmin

N-PNNU(g)] =

N-PNPU(g)] =

, (10)

. (11)

γ

γ

Var[bRγ
Var[bRγ

(0, 2γN-PNNU) if ψP > ψN.

Then, we have γN-PNPU ∈ [0, 1] if ψP ≤ ψN or γN-PNNU ∈
N-PNPU(g)] <
for all γ ∈ (0, 2γN-PNPU) if ψP <

[0, 1] if ψP ≥ ψN. Additionally, Var[bRγ
Var[bRPN(g)]
ψN, or Var[bRγ
N-PNNU(g)] < Var[bRPN(g)] for all γ ∈
Theorem 4 implies that the variance of bRPN(g) is re-
N-PNPU(g) if ψP ≤ ψN or bRγ
duced by either bRγ

2Being ﬁxed means g is determined before seeing the data for
evaluating the empirical risk. For example, if g is trained by some
learning method, and the empirical risk is subsequently evaluated
on the validation/test data, g is regarded as ﬁxed in the evaluation.

N-PNNU(g)

can reduce the variance.

As a corollary of Theorems 3 and 4,

if ψP ≥ ψN, where γ should be close to γN-PNPU or
γN-PNNU. The range of such good γ values is of length
min{2|ψP − ψN|/(ψP + ψN), 1}. In particular, if 3ψP ≤
N-PNPU(g) given any γ ∈ (0, 1) can reduce the vari-
N-PNNU(g) given any γ ∈ (0, 1)

ψN, bRγ
ance, and if ψP ≥ 3ψN, bRγ
variance achievable by bRγ
bRγ
Nevertheless, bRγ
N-PNPU(g) and bRγ
wider range of nice γ values than bRγ

the minimum
N-PNPU(g), and
N-PNNU(g) at their optimal γN-PUNU, γN-PNPU, and
γN-PNNU is exactly the same, namely, 4ψPψN/(ψP + ψN).
N-PNNU(g) have a much

N-PUNU(g), bRγ

If we further assume that σP(g) = σN(g), the condition in
Theorems 3 and 4 as to whether ψP ≤ ψN or ψP ≥ ψN will
be independent of g. Also, it will coincide with the condi-
tion in Theorem 7 in Niu et al. (2016) where the minimizers

N-PUNU(g).

of bRPN(g), bRPU(g) and bRNU(g) are compared.

A ﬁnal remark is that learning is uninvolved in Theorems 3
and 4, such that ℓ(m) can be any loss that satisﬁes ℓ(m) +
ℓ(−m) = 1, and g can be any ﬁxed decision function. For
instance, we may adopt ℓ0-1(m) and pick some g resulted
from some other learning methods. As a consequence, the

variance ofbIPN(g) over the validation data can be reduced,

and then the cross-validation should be more stable, given
that nU is sufﬁciently large. Therefore, even without being
minimized, our proposed risk estimators are themselves of
practical importance.

4.3. PUNU vs. PNU Classiﬁcation

We discuss here which approach, PUNU or PNU classiﬁ-
cation, is more promising according to state-of-the-art the-
oretical comparisons (Niu et al., 2016), which are based on
estimation error bounds.

Let bgPN, bgPU, and bgNU be the minimizers of bRPN(g),
bRPU(g), and bRNU(g), respectively.

Let αPU,PN :=
(θP/√nP + 1/√nU)/(θN/√nN) and αNU,PN
:=
(θN/√nN + 1/√nU)/(θP/√nP). The ﬁnite-sample com-
parisons state that if αPU,PN > 1 (αNU,PN > 1), PN clas-
siﬁcation is more promising than PU (NU) classiﬁcation,

i.e., R(bgPN) < R(bgPU) (R(bgPN) < R(bgNU)); otherwise

PU (NU) classiﬁcation is more promising than PN classiﬁ-
cation (cf. Section 3.2 in Niu et al., 2016).

Suppose that nU is not sufﬁciently large against nP and nN.
According to the ﬁnite-sample comparisons, PN classiﬁca-
tion is most promising, and either PU or NU classiﬁcation

is the second best, i.e., R(bgPN) < R(bgPU) < R(bgNU)
or R(bgPN) < R(bgNU) < R(bgPU). On the other hand,

if nU is sufﬁciently large (nU → ∞, which is faster
than nP, nN → ∞), we have the asymptotic compar-
isons: α∗
NU,PN =

PU,PN = limnP,nN,nU→∞ αPU,PN, α∗

Semi-Supervised Classiﬁcation Based on Classiﬁcation from Positive and Unlabeled Data

limnP,nN,nU→∞ αNU,PN, and α∗
NU,PN = 1. From
the last equation, if α∗
NU,PN > 1, imply-
ing that PU (PN) classiﬁcation is more promising than PN

PU,PN·α∗
PU,PN < 1, then α∗

Similarly, when α∗

PU,PN > 1 and α∗

(NU) classiﬁcation, i.e., R(bgPU) < R(bgPN) < R(bgNU).
NU,PN < 1, R(bgNU) <
R(bgPN) < R(bgPU) (cf. Section 3.3 in Niu et al., 2016).

In real-world applications, since we do not know whether
the number of unlabeled samples is sufﬁciently large or not,
a practical approach is to combine the best methods in both
the ﬁnite-sample and asymptotic cases. PNU classiﬁcation
is the combination of the best methods in both cases, but
PUNU classiﬁcation is not.
In addition, PUNU classiﬁ-
cation includes the worst one in its combination in both
cases. From this viewpoint, PNU classiﬁcation would be
more promising than PUNU classiﬁcation, as demonstrated
in the experiments shown in the next section.

5. Experiments

In this section, we ﬁrst numerically analyze the proposed
approach and then compare the proposed semi-supervised
classiﬁcation methods against existing methods. All ex-
periments were carried out using a PC equipped with two
2.60GHz Intel® Xeon® E5-2640 v3 CPUs.

5.1. Experimental Analyses

Here, we numerically analyze the behavior of our proposed
approach. Due to limited space, we show results on two out
of six data sets and move the rest to Appendix C.

Common Setup: As a classiﬁer, we use the Gaussian

kernel model: g(x) =Pn

i=1 wi exp(−kx − xik2/(2σ2)),
where n = nP + nN, {wi}n
i=1 are the parameters,
{xi}n
i=1 = XP∪XN, and σ > 0 is the Gaussian bandwidth.
The bandwidth candidates are {1/8, 1/4, 1/2, 1, 3/2, 2} ×
median(kxi − xjkn
i,j=1). The classiﬁer trained by mini-
ber of labeled samples for training is 20, where the class-
prior was 0.5. In all experiments, we used the squared loss
for training. We note that the class-prior of test data was
the same as that of unlabeled data.

mizing the empirical PN risk is denoted bybgPN. The num-

Variance Reduction in Practice: Here, we numerically
investigate how many unlabeled samples are sufﬁcient in
practice such that the variance of the empirical PNU risk
PNU(g)] <

is smaller than that of the PN risk: Var[bRη
Var[bRPN(g)] given a ﬁxed classiﬁer g.
As the ﬁxed classiﬁer, we used the classiﬁerbgPN, where
and PNU risks, Var[bRPN(bgPN)] and Var[bRη
PNU(bgPN)], we

the hyperparameters were determined by ﬁve-fold cross-
validation. To compute the variance of the empirical PN

repeatedly drew additional nV

P = 10 positive, nV

N = 10

e
c
n
a
i
r
a
V

f
o

o
i
t
a
R

1.4

1.2

1

0.8

0.6

θP = 0.3
θP = 0.5
θP = 0.7

0

50

100 150 200 250 300

nv
U

e
c
n
a
i
r
a
V

f
o

o
i
t
a
R

1.4

1.2

1

0.8

0.6

0.4

0

50

100 150 200 250 300

nv
U

(a) Phoneme (d = 5)

(b) Magic (d = 10)

Figure 1. Average and standard error of
the ratio between
the variance of empirical PNU risk and that of PN risk,
Var[ bRη
PNU(bgPN)]/ Var[ bRPN(bgPN)], as a function of the number
of unlabeled samples over 100 trials. Although the variance re-
duction is proved for an inﬁnite number of samples, it can be ob-
served with a ﬁnite number of samples.

negative, and nV
U unlabeled samples from the rest of the
data set. The additional samples were also used for ap-

Eqs.(10) and (11).

Figure 1 shows the ratio between the variance of
the PN risk,
the empirical PNU risk and that of

proximatingbσP(bgPN) andbσN(bgPN) to compute η, i.e., γ in
Var[bRη
PNU(bgPN)]/ Var[bRPN(bgPN)]. The number of unla-

beled samples for validation nV
U increases from 10 to 300.
We see that with a rather small number of unlabeled sam-
ples, the ratio becomes less than 1. That is, the variance
of the empirical PNU risk becomes smaller than that of the
PN risk. This implies that although the variance reduction
is proved for an inﬁnite number of unlabeled samples, it can
be observed under a ﬁnite number of samples in practice.

Compared to when θP = 0.3 and 0.7, the effect of variance
reduction is small when θP = 0.5. This is because if we
assume σP(g) ≈ σN(g), when nP ≈ nN and θP = 0.5,
we have γN-PNPU ≈ γN-PNNU ≈ 0 (because ψP ≈ ψN.
See Theorem 4). That is, the PNU risk is dominated by

the PN risk, implying that Var[bRη

Note that the class-prior is not the only factor for vari-
ance reduction; for example, if θP = 0.5, nP ≫ nN, and
σP(g) ≈ σN(g), then γN-PNPU 6≈ 0 (because ψP ≪ ψN)
and the variance reduction will be large.

PNU(g)] ≈ Var[bRPN(g)].

PNU Risk in Validation: As discussed in Section 4, the
empirical PNU risk will be a reliable validation score due
to its having smaller variance than the empirical PN risk.
We show here that the empirical PNU risk is a promising
alternative to a validation score.

To focus on the effect of validation scores only, we trained
two classiﬁers by using the same risk, e.g, the empirical
PN risk. We then tune the classiﬁers with the empirical
PN , respectively.
The number of validation samples was the same as in the
previous experiment.

PN and PNU risks denoted bybgPN

PN andbgPNU

Semi-Supervised Classiﬁcation Based on Classiﬁcation from Positive and Unlabeled Data

e
t
a
R
n
o
i
t
a
c
ﬁ

i
s
s
a
l
c
s
i

M

f
o

o
i
t
a
R

1.04

1.02

1

0.98

0.96

0.94

θP = 0.3
θP = 0.5
θP = 0.7

0

50

100 150 200 250 300

nv
U

e
t
a
R
n
o
i
t
a
c
ﬁ

i
s
s
a
l
c
s
i

M

f
o

o
i
t
a
R

1.04

1.02

1

0.98

0.96

0

50

100 150 200 250 300

nv
U

(a) Phoneme (d = 5)

(b) Magic (d = 10)

Figure 2. Average and standard error of the ratio between the mis-
classiﬁcation rates of bgPNU
PN as a function of unlabeled
samples over 1000 trials. In many cases, the ratio becomes less
than 1, implying that the PNU risk is a promising alternative to
the standard PN risk in validation if unlabeled data are available.

PN and bgPN

PNU

PUNU

ER

LapSVM

SMIR

WellSVM

S4VM

103

102

101

100

10−1

]
.
c
e
s
[

i

e
m
T
n
o
i
t
a
t
u
p
m
o
C

Banana
Phoneme

Magic

Image

Susy

ijcnn1

Waveform
German

g50c

Spambase
covtype

phishing
Splice

a9a

Coil2

w8a

Figure 3. Average computation time over 50 trials for benchmark
data sets when nL = 50.

ofbgPNU
PN and that ofbgPN
than 1, i.e., bgPNU
In particular, when θP = 0.3 and 0.7,bgPNU

Figure 2 shows the ratio between the misclassiﬁcation rate
PN. The number of unlabeled sam-
ples for validation increases from 10 to 300. With a rather
small number of unlabeled samples, the ratio becomes less
PN.
PN improved sub-
stantially; the large improvement tends to give the large
variance reduction (cf. Figure 1). This result shows that
the use of the empirical PNU risk for validation improved
the classiﬁcation performance given a relatively large size
of unlabeled data.

PN achieves better performance thanbgPN

5.2. Comparison with Existing Methods

Next, we numerically compare the proposed methods
against existing semi-supervised classiﬁcation methods.

Common Setup: We compare our methods against ﬁve
conventional semi-supervised classiﬁcation methods: en-
tropy regularization (ER) (Grandvalet & Bengio, 2004),
the Laplacian support vector machine (LapSVM) (Belkin
et al., 2006; Melacci & Belkin, 2011), squared-loss mu-
tual information regularization (SMIR) (Niu et al., 2013),
the weakly labeled support vector machine (WellSVM) (Li
et al., 2013), and the safe semi-supervised support vector
machine (S4VM) (Li & Zhou, 2015).

Among the proposed methods, PNU classiﬁcation and

Table 1. Average and standard error of the misclassiﬁcation rates
of each method over 50 trials for benchmark data sets. Boldface
numbers denote the best and comparable methods in terms of av-
erage misclassiﬁcations rate according to a t-test at a signiﬁcance
level of 5%. The bottom row gives the number of best/comparable
cases of each method.

Data set nL

PNU

PUNU

ER

LapSVM SMIR WellSVM S4VM

Banana
d = 2

10 30.1 (1.0) 32.1 (1.1) 35.8 (1.0) 36.9 (1.0) 37.7 (1.1) 41.8 (0.6) 45.3 (1.0)
50 19.0 (0.6) 26.4 (1.2) 20.6 (0.7) 21.3 (0.7) 21.1 (1.0) 42.6 (0.5) 38.7 (0.9)

Phoneme

d = 5

10 32.5 (0.8) 33.5 (1.0) 33.4 (1.2) 36.5 (1.5) 36.4 (1.2) 28.4 (0.6) 33.7 (1.4)
50 28.1 (0.5) 32.8 (0.9) 27.8 (0.6) 27.0 (0.8) 28.6 (1.0) 26.8 (0.4) 25.1 (0.2)

Magic
d = 10

Image
d = 18

Susy
d = 18

10 31.7 (0.8) 34.1 (0.9) 34.2 (1.1) 37.9 (1.3) 36.0 (1.2) 30.1 (0.8) 33.3 (0.9)
50 29.9 (0.8) 33.4 (0.9) 30.9 (0.5) 31.0 (0.9) 30.8 (0.9) 28.8 (0.8) 29.2 (0.4)

10 29.8 (0.9) 31.7 (0.8) 33.7 (1.1) 36.6 (1.2) 36.7 (1.2) 34.7 (1.1) 35.9 (1.0)
50 20.7 (0.8) 26.6 (1.1) 20.8 (0.8) 20.3 (1.0) 20.9 (0.9) 27.2 (1.0) 23.2 (0.7)

10 44.6 (0.6) 45.0 (0.6) 47.7 (0.4) 48.2 (0.4) 45.1 (0.7) 48.0 (0.3) 46.8 (0.3)
50 38.9 (0.6) 41.5 (0.6) 37.9 (0.7) 43.1 (0.6) 43.9 (0.8) 43.8 (0.7) 42.1 (0.4)

German
d = 20

10 40.8 (0.9) 42.4 (0.7) 43.6 (0.9) 45.9 (0.7) 46.2 (0.8) 42.4 (0.8) 42.0 (0.7)
50 36.2 (0.8) 39.0 (0.8) 38.9 (0.6) 40.6 (0.6) 38.4 (1.1) 38.5 (1.0) 34.9 (0.5)

Waveform 10 17.4 (0.6) 18.0 (0.9) 18.5 (0.6) 24.9 (1.4) 18.0 (1.0) 16.7 (0.6) 20.8 (0.8)
50 16.3 (0.6) 23.7 (1.2) 14.2 (0.4) 18.1 (0.8) 15.4 (0.6) 15.5 (0.5) 15.3 (0.3)

d = 21

ijcnn1
d = 22

g50c
d = 50

10 43.6 (0.6) 40.3 (1.0) 49.7 (0.1) 49.2 (0.3) 44.0 (1.0) 45.9 (0.7) 49.3 (0.8)
50 34.5 (0.8) 37.1 (0.9) 35.5 (0.8) 33.4 (1.1) 49.4 (0.3) 46.2 (0.8) 48.6 (0.4)

10 11.4 (0.6) 12.5 (0.6) 23.3 (2.3) 39.8 (1.6) 21.9 (1.3) 6.6 (0.4) 27.0 (1.4)
50 12.5 (1.1) 10.1 (0.6)
8.7 (0.4) 22.5 (1.5) 10.6 (0.6) 7.4 (0.4) 12.1 (0.5)

covtype
d = 54

10 46.2 (0.4) 46.0 (0.4) 46.0 (0.5) 47.1 (0.5) 47.9 (0.5) 46.9 (0.6) 46.4 (0.4)
50 41.3 (0.5) 42.3 (0.5) 41.0 (0.4) 41.5 (0.5) 46.2 (0.8) 43.6 (0.6) 40.8 (0.4)

Spambase 10 27.2 (0.9) 28.1 (1.1) 31.8 (1.4) 39.7 (1.4) 30.9 (1.3) 23.8 (0.8) 36.1 (1.5)
50 23.4 (1.0) 26.6 (1.0) 22.1 (0.7) 28.5 (1.3) 20.9 (0.5) 19.1 (0.4) 24.5 (0.9)

d = 57

Splice
d = 60

10 38.3 (0.8) 39.3 (0.8) 43.9 (0.8) 47.9 (0.5) 41.6 (0.7) 42.0 (1.0) 42.4 (0.6)
50 30.6 (0.8) 34.7 (0.9) 30.9 (0.8) 38.8 (1.0) 30.6 (0.9) 40.9 (0.8) 35.9 (0.7)

phishing
d = 68

10 24.2 (1.2) 25.8 (1.0) 27.3 (1.6) 37.2 (1.6) 27.6 (1.6) 27.5 (1.4) 31.7 (1.3)
50 15.8 (0.6) 18.3 (0.8) 15.4 (0.5) 21.1 (1.3) 14.7 (0.8) 17.2 (0.7) 16.7 (0.8)

a9a

d = 83

10 31.4 (0.9) 31.3 (1.0) 34.3 (1.2) 41.0 (1.1) 37.3 (1.3) 33.1 (1.2) 34.3 (1.2)
50 27.9 (0.6) 29.9 (0.8) 28.6 (0.7) 33.3 (1.0) 26.9 (0.7) 28.9 (0.8) 26.2 (0.4)

Coil2

d = 241

w8a

d = 300

10 38.7 (0.8) 40.1 (0.8) 42.8 (0.7) 43.9 (0.8) 43.2 (0.8) 39.1 (0.9) 44.0 (0.8)
50 23.2 (0.6) 30.5 (0.9) 23.6 (0.9) 22.8 (0.9) 25.1 (0.9) 22.6 (0.8) 25.4 (0.8)

10 35.9 (0.9) 33.6 (1.0) 41.6 (1.0) 46.6 (0.8) 39.4 (0.9) 42.1 (0.8) 43.0 (0.8)
50 28.1 (0.7) 27.6 (0.6) 27.0 (0.9) 38.7 (0.8) 28.0 (0.9) 33.7 (0.8) 35.2 (1.0)

#Best/Comp.

23

13

11

4

9

13

7

PUNU classiﬁcation with the squared loss were tested.3

Data Sets: We used sixteen benchmark data sets taken
from the UCI Machine Learning Repository (Lichman,
2013), the Semi-Supervised Learning book (Chapelle et al.,
2006), the LIBSVM (Chang & Lin, 2011), the ELENA
Project,4 and a paper by Chapelle & Zien (2005).5 Each
feature was scaled to [0, 1]. Similarly to the setting in Sec-
tion 5.1, we used the Gaussian kernel model for all meth-
ods. The training data is {xi}n
i=1 = XP ∪XN ∪XU, where
n = nP + nN + nU. We selected all hyper-parameters with
validation samples of size 20 (nV
N = 10). For train-
ing, we drew nL labeled and nU = 300 unlabeled samples.
The class-prior of labeled data was set at 0.7 and that of un-
labeled samples was set at θP = 0.5 that were assumed to
be known. In practice, the class-prior, θP, can be estimated

P = nV

3In preliminary experiments, we tested other loss functions
such as the ramp and logistic losses and concluded that the dif-
ference in loss functions did not provide noticeable difference.

4https://www.elen.ucl.ac.be/neural-

nets/Research/Projects/ELENA/elena.htm

5http://olivier.chapelle.cc/lds/

Semi-Supervised Classiﬁcation Based on Classiﬁcation from Positive and Unlabeled Data

Table 2. Average and standard error of misclassiﬁcation rates over
30 trials for the Places 205 data set. Boldface numbers denote the
best and comparable methods in terms of the average misclassiﬁ-
cation rate according to a t-test at a signiﬁcance level of 5%.

Data set

nU

θP

bθP

PNU

ER

LapSVM

SMIR

WellSVM

1000 0.50 0.49 (0.01) 27.4 (1.3) 26.6 (0.5) 26.1 (0.7) 40.1 (3.9) 27.5 (0.5)
5000 0.50 0.50 (0.01) 24.8 (0.6) 26.1 (0.5) 26.1 (0.4) 30.1 (1.6)
N/A
N/A
N/A

10000 0.50 0.52 (0.01) 25.6 (0.7) 25.4 (0.5) 25.5 (0.6)

1000 0.73 0.67 (0.01) 13.0 (0.5) 15.3 (0.6) 16.7 (0.8) 17.2 (0.8) 18.2 (0.7)
5000 0.73 0.67 (0.01) 13.4 (0.4) 13.3 (0.5) 16.6 (0.6) 24.4 (0.6)
N/A
N/A
N/A

10000 0.73 0.68 (0.01) 13.3 (0.5) 13.7 (0.6) 16.8 (0.8)

]
.
c
e
s
[

i

e
m
T
n
o
i
t
a
t
u
p
m
o
C

104

103

102

PNU

ER

LapSVM

Arts

Deserts

Fields

Stadiums Platforms Temples

Arts

Deserts

Fields

1000 0.65 0.57 (0.01) 22.4 (1.0) 26.2 (1.0) 26.6 (1.3) 28.2 (1.1) 26.6 (0.8)
5000 0.65 0.57 (0.01) 20.6 (0.5) 22.6 (0.6) 24.7 (0.8) 29.6 (1.2)
N/A
N/A
N/A

10000 0.65 0.57 (0.01) 21.6 (0.6) 22.5 (0.6) 25.0 (0.9)

Figure 4. Average computation time over 30 trials for the Places
205 data set when nU = 10000.

Stadiums

1000 0.50 0.50 (0.01) 11.4 (0.4) 11.5 (0.5) 12.5 (0.5) 17.4 (3.6) 11.7 (0.4)
5000 0.50 0.50 (0.01) 11.0 (0.5) 10.9 (0.3) 11.1 (0.3) 13.4 (0.7)
N/A
N/A
N/A

10000 0.50 0.51 (0.00) 10.7 (0.3) 10.9 (0.3) 11.2 (0.2)

Platforms

1000 0.27 0.33 (0.01) 21.8 (0.5) 23.9 (0.6) 24.1 (0.5) 30.1 (2.3) 26.2 (0.8)
5000 0.27 0.34 (0.01) 23.3 (0.8) 24.4 (0.7) 24.9 (0.7) 26.6 (0.3)
N/A
N/A
N/A

10000 0.27 0.34 (0.01) 21.4 (0.5) 24.3 (0.6) 24.8 (0.5)

Temples

1000 0.55 0.51 (0.01) 43.9 (0.7) 43.9 (0.6) 43.4 (0.6) 50.7 (1.6) 44.3 (0.5)
5000 0.55 0.54 (0.01) 43.4 (0.9) 43.0 (0.6) 43.1 (1.0) 43.6 (0.7)
N/A
N/A
N/A

10000 0.55 0.50 (0.01) 45.2 (0.8) 44.4 (0.8) 44.2 (0.7)

by methods proposed, e.g., by Blanchard et al. (2010), Ra-
maswamy et al. (2016), or Kawakubo et al. (2016).

Table 1 lists the average and standard error of the mis-
classiﬁcation rates over 50 trials and the number of
best/comparable performances of each method in the bot-
tom row. The superior performance of PNU classiﬁcation
over PUNU classiﬁcation agrees well with the discussion
in Section 4.3. With the g50c data set, which well sat-
isﬁes the low-density separation principle, the WellSVM
achieved the best performance. However, in the Banana
data set, where the two classes are highly overlapped, the
performance of WellSVM was worse than the other meth-
ods. In contrast, PNU classiﬁcation achieved consistently
better/comparable performance and its performance did
not degenerate considerably across data sets. These re-
sults show that the idea of using PU classiﬁcation in semi-
supervised classiﬁcation is promising.

Figure 3 plots the computation time, which shows that the
fastest computation was achieved using the proposed meth-
ods with the square loss.

Image Classiﬁcation: Finally, we used the Places 205
data set (Zhou et al., 2014), which contains 2.5 million im-
ages in 205 scene classes. We used a 4096-dimensional fea-
ture vector extracted from each image by AlexNet under the
framework of Caffe,6 which is available on the project web-
site7. We chose two similar scenes to construct binary clas-
siﬁcation tasks (see the description of data sets in Appendix
B.3). We drew 100 labeled and nU unlabeled samples from
each task; the class-prior of labeled and unlabeled data
were respectively set at 0.5 and θP = mP/(mP + mN),
where mP and mN respectively denote the number of total
samples in positive and negative scenes. We used a linear

6http://caffe.berkeleyvision.org/
7http://places.csail.mit.edu/

classiﬁer g(x) = w⊤x + w0, where w is the weight vector
and w0 is the offset (in the SMIR, the linear kernel model
is used; see Niu et al. (2013) for details).

We selected hyper-parameters in PNU classiﬁcation by ap-
plying ﬁve-fold cross-validation with respect to R ¯η
PNU(g)
with the zero-one loss, where ¯η was set at Eq.(10) or
Eq.(11) with σP(g) = σN(g). The class-prior p(y =
+1) = θP was estimated using the method based on en-
ergy distance minimization (Kawakubo et al., 2016).

Table 2 lists the average and standard error of the misclas-
siﬁcation rates over 30 trials, where methods taking more
than 2 hours were omitted and indicated as N/A. The results
show that PNU classiﬁcation was most effective. The av-
erage computation times are shown in Figure 4, revealing
again that PNU classiﬁcation was the fastest method.

6. Conclusions

In this paper, we proposed a novel semi-supervised clas-
siﬁcation approach based on classiﬁcation from positive
and unlabeled data. Unlike most of the conventional meth-
ods, our approach does not require strong assumptions on
the data distribution such as the cluster assumption. We
theoretically analyzed the variance of risk estimators and
showed that unlabeled data help reduce the variance with-
out the conventional distributional assumptions. We also
established generalization error bounds and showed that
the conﬁdence term decreases with respect to the num-
ber of positive, negative, and unlabeled samples without
the conventional distributional assumptions in the optimal
parametric order. We experimentally analyzed the behavior
of the proposed methods and demonstrated that one of the
proposed methods, termed PNU classiﬁcation, was most
effective in terms of both classiﬁcation accuracy and com-
putational efﬁciency. It was recently pointed out that PU
classiﬁcation can behave undesirably for very ﬂexible mod-
els and a modiﬁed PU risk has been proposed (Kiryo et al.,
2017). Our future work is to develop a semi-supervised
classiﬁcation method based on the modiﬁed PU classiﬁca-
tion.

Semi-Supervised Classiﬁcation Based on Classiﬁcation from Positive and Unlabeled Data

Acknowledgements

TS was supported by JSPS KAKENHI 15J09111. GN was
supported by the JST CREST program and Microsoft Re-
search Asia. MCdP and MS were supported by the JST
CREST program.

References

Belkin, M., Niyogi, P., and Sindhwani, V. Manifold reg-
ularization: A geometric framework for learning from
labeled and unlabeled examples. Journal of Machine
Learning Research, 7:2399–2434, 2006.

Kiryo, R., Niu, G., du Plessis, M. C., and Sugiyama, M.
Positive-unlabeled learning with non-negative risk esti-
mator. arXiv preprint arXiv:1703.00593, 2017.

Krijthe, J. H. and Loog, M. Robust semi-supervised least
squares classiﬁcation by implicit constraints. Pattern
Recognition, 63:115–126, 2017.

Ledoux, M. and Talagrand, M. Probability in Banach

Spaces: Isoperimetry and Processes. Springer, 1991.

Li, Y.-F. and Zhou, Z.-H. Towards making unlabeled data
never hurt. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 37(1):175–188, 2015.

Blanchard, G., Lee, G., and Scott, C. Semi-supervised nov-
elty detection. Journal of Machine Learning Research,
11:2973–3009, 2010.

Li, Y.-F., Tsang, I. W., Kwok, J. T., and Zhou, Z.-H. Con-
vex and scalable weakly labeled SVMs. Journal of Ma-
chine Learning Research, 14(1):2151–2188, 2013.

Chang, C.-C. and Lin, C.-J. LIBSVM: A library for sup-
port vector machines. ACM Transactions on Intelli-
gent Systems and Technology, 2:27:1–27:27, 2011. Soft-
ware available at http://www.csie.ntu.edu.
tw/~cjlin/libsvm.

Chapelle, O. and Zien, A. Semi-supervised classiﬁcation
by low density separation. In AISTATS, pp. 57–64, 2005.

Chapelle, O., Schölkopf, B., and Zien, A. (eds.). Semi-

Supervised Learning. MIT Press, 2006.

Collobert, R., Sinz, F., Weston, J., and Bottou, L. Trading

convexity for scalability. In ICML, pp. 201–208, 2006.

Cozman, F. G., Cohen, I., and Cirelo, M. C.

supervised learning of mixture models.
99–106, 2003.

Semi-
In ICML, pp.

Lichman, M. UCI machine learning repository, 2013. URL

http://archive.ics.uci.edu/ml.

Melacci, S. and Belkin, M. Laplacian support vector ma-
chines trained in the primal. Journal of Machine Learn-
ing Research, 12:1149–1184, 2011.

Mendelson, S. Lower bounds for the empirical minimiza-
tion algorithm. IEEE Transactions on Information The-
ory, 54(8):3797–3803, 2008.

Mohri, M., Rostamizadeh, A., and Talwalkar, A. Founda-

tions of Machine Learning. MIT Press, 2012.

Niu, G., Jitkrittum, W., Dai, B., Hachiya, H., and
Sugiyama, M. Squared-loss mutual information regular-
ization: A novel information-theoretic approach to semi-
supervised learning.
In ICML, volume 28, pp. 10–18,
2013.

du Plessis, M. C., Niu, G., and Sugiyama, M. Analysis of
learning from positive and unlabeled data. In NIPS, pp.
703–711, 2014.

Niu, G., du Plessis, M. C., Sakai, T., Ma, Y., and Sugiyama,
M. Theoretical comparisons of positive-unlabeled learn-
ing against positive-negative learning. In NIPS, 2016.

du Plessis, M. C., Niu, G., and Sugiyama, M. Convex for-
mulation for learning from positive and unlabeled data.
In ICML, volume 37, pp. 1386–1394, 2015.

Ramaswamy, H. G., Scott, C., and Tewari, A. Mixture pro-
portion estimation via kernel embedding of distributions.
In ICML, 2016.

Elkan, C. and Noto, K. Learning classiﬁers from only posi-
tive and unlabeled data. In SIGKDD, pp. 213–220, 2008.

Grandvalet, Y. and Bengio, Y. Semi-supervised learning by

entropy minimization. In NIPS, pp. 529–536, 2004.

Jain, S., White, M., and Radivojac, P. Estimating the class
prior and posterior from noisy positives and unlabeled
data. In NIPS, 2016.

Kawakubo, H., du Plessis, M. C., and Sugiyama, M. Com-
putationally efﬁcient class-prior estimation under class
balance change using energy distance. IEICE Transac-
tions on Information and Systems, E99-D(1):176–186,
2016.

Sokolovska, N., Cappé, O., and Yvon, F. The asymptotics
of semi-supervised learning in discriminative probabilis-
tic models. In ICML, pp. 984–991, 2008.

Vapnik, V. N. Statistical Learning Theory. John Wiley &

Sons, 1998.

Vapnik, V.N. The Nature of Statistical Learning Theory.

Springer-Verlag, New York, NY, USA, 1995.

Yuille, A. L. and Rangarajan, A. The concave-convex pro-

cedure (CCCP). In NIPS, pp. 1033–1040, 2002.

Zhou, B., Lapedriza, A., Xiao, J., Torralba, A., and Oliva,
A. Learning deep features for scene recognition using
places database. In NIPS, pp. 487–495, 2014.

