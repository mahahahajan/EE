Tensor Balancing on Statistical Manifold

Mahito Sugiyama 1 2 Hiroyuki Nakahara 3 Koji Tsuda 4 5 6

Abstract

We solve tensor balancing, rescaling an Nth or-
der nonnegative tensor by multiplying N ten-
sors of order N (cid:0) 1 so that every ﬁber sums to
one. This generalizes a fundamental process of
matrix balancing used to compare matrices in a
wide range of applications from biology to eco-
nomics. We present an efﬁcient balancing al-
gorithm with quadratic convergence using New-
ton’s method and show in numerical experiments
that the proposed algorithm is several orders of
magnitude faster than existing ones. To theo-
retically prove the correctness of the algorithm,
we model tensors as probability distributions in
a statistical manifold and realize tensor balanc-
ing as projection onto a submanifold. The key to
our algorithm is that the gradient of the manifold,
used as a Jacobian matrix in Newton’s method,
can be analytically obtained using the M¨obius in-
version formula, the essential of combinatorial
mathematics. Our model is not limited to ten-
sor balancing, but has a wide applicability as it
includes various statistical and machine learning
models such as weighted DAGs and Boltzmann
machines.

1. Introduction
Matrix balancing is the problem of rescaling a given square
nonnegative matrix A 2 Rn(cid:2)n(cid:21)0
to a doubly stochastic ma-
trix RAS, where every row and column sums to one, by
multiplying two diagonal matrices R and S. This is a
fundamental process for analyzing and comparing matri-
ces in a wide range of applications, including input-output
analysis in economics, called the RAS approach (Parikh,
1979; Miller & Blair, 2009; Lahr & de Mesnard, 2004),
seat assignments in elections (Balinski, 2008; Akartunalı &

1National Institute of Informatics 2JST PRESTO 3RIKEN
Brain Science Institute 4Graduate School of Frontier Sciences,
The University of Tokyo 5RIKEN AIP 6NIMS. Correspondence
to: Mahito Sugiyama <mahito@nii.ac.jp>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Figure1. Overview of our approach.

Knight, 2016), Hi-C data analysis (Rao et al., 2014; Wu &
Michor, 2016), the Sudoku puzzle (Moon et al., 2009), and
the optimal transportation problem (Cuturi, 2013; Frogner
et al., 2015; Solomon et al., 2015). An excellent review of
this theory and its applications is given by Idel (2016).
The standard matrix balancing algorithm is the Sinkhorn-
Knopp algorithm (Sinkhorn, 1964; Sinkhorn & Knopp,
1967; Marshall & Olkin, 1968; Knight, 2008), a special
case of Bregman’s balancing method (Lamond & Stewart,
1981) that iterates rescaling of each row and column until
convergence. The algorithm is widely used in the above
applications due to its simple implementation and theo-
retically guaranteed convergence. However, the algorithm
converges linearly (Soules, 1991), which is prohibitively
slow for recently emerging large and sparse matrices. Al-
though Livne & Golub (2004) and Knight & Ruiz (2013)
tried to achieve faster convergence by approximating each
step of Newton’s method, the exact Newton’s method with
quadratic convergence has not been intensively studied yet.
Another open problem is tensor balancing, which is a gen-
eralization of balancing from matrices to higher-order mul-
tidimentional arrays, or tensors. The task is to rescale an
Nth order nonnegative tensor to a multistochastic tensor,
in which every ﬁber sums to one, by multiplying (N (cid:0) 1)th
order N tensors. There are some results about mathemat-
ical properties of multistochastic tensors (Cui et al., 2014;
Chang et al., 2016; Ahmed et al., 2003). However, there
is no result for tensor balancing algorithms with guaran-
teed convergence that transforms a given tensor to a multi-
stochastic tensor until now.

Every (cid:31)bersums to 1Given tensor AMultistochastic tensor A’Submanifold (β)Probabilitydistribution PStatistical manifold (dually (cid:30)at Riemannian manifold)ProjectionTensor balancingProjecteddistribution PβProjecteddistribution PβTensor Balancing on Statistical Manifold

Here we show that Newton’s method with quadratic con-
vergence can be applied to tensor balancing while avoid-
ing solving a linear system on the full tensor. Our strat-
egy is to realize matrix and tensor balancing as projec-
tion onto a dually ﬂat Riemmanian submanifold (Figure 1),
which is a statistical manifold and known to be the es-
sential structure for probability distributions in information
geometry (Amari, 2016). Using a partially ordered out-
come space, we generalize the log-linear model (Agresti,
2012) used to model the higher-order combinations of bi-
nary variables (Amari, 2001; Ganmor et al., 2011; Naka-
hara & Amari, 2002; Nakahara et al., 2003), which allows
us to model tensors as probability distributions in the sta-
tistical manifold. The remarkable property of our model is
that the gradient of the manifold can be analytically com-
puted using the M¨obius inversion formula (Rota, 1964), the
heart of combinatorial mathematics (Ito, 1993), which en-
ables us to directly obtain the Jacobian matrix in Newton’s
method. Moreover, we show that (n (cid:0) 1)N entries for the
size nN of a tensor are invariant with respect to one of the
two coordinate systems of the statistical manifold. Thus
the number of equations in Newton’s method is O(nN(cid:0)1).
The remainder of this paper is organized as follows: We
begin with a low-level description of our matrix balancing
algorithm in Section 2 and demonstrate its efﬁciency in nu-
merical experiments in Section 3. To guarantee the correct-
ness of the algorithm and extend it to tensor balancing, we
provide theoretical analysis in Section 4. In Section 4.1, we
introduce a generalized log-linear model associated with a
partial order structured outcome space, followed by intro-
ducing the dually ﬂat Riemannian structure in Section 4.2.
In Section 4.3, we show how to use Newton’s method to
compute projection of a probability distribution onto a sub-
manifold. Finally, we formulate the matrix and tensor bal-
ancing problem in Section 5 and summarize our contribu-
tions in Section 6.

2. The Matrix Balancing Algorithm
Given a nonnegative square matrix A = (aij) 2 Rn(cid:2)n(cid:21)0 , the
task of matrix balancing is to ﬁnd r; s 2 Rn that satisfy

(RAS)1 = 1;

(RAS)T 1 = 1;

(1)
where R = diag(r) and S = diag(s). The balanced matrix
′
= RAS is called doubly stochastic, in which each entry
A
′
ij = aijrisj and all the rows and columns sum to one.
a
The most popular algorithm is the Sinkhorn-Knopp algo-
rithm, which repeats updating r and s as r = 1=(As) and
s = 1=(AT r). We denote by [n] = f1; 2; : : : ; ng hereafter.
In our algorithm, instead of directly updating r and s, we
update two parameters (cid:18) and (cid:17) deﬁned as

∑

∑

∑

∑

log pij =

i′(cid:20)i

j′(cid:20)j

(cid:18)i′j′;

(cid:17)ij =

i′(cid:21)i

j′(cid:21)j

pi′j′

(2)

Figure2. Matrix balancing with two parameters (cid:18) and (cid:17).

∑

∑

ij aij so that

for each i; j 2 [n], where we normalized entries as pij =
ij pij = 1. We assume for simplic-
aij=
ity that each entry is strictly larger than zero. The assump-
tion will be removed in Section 5.
The key to our approach is that we update (cid:18)(t)
ij with i = 1
or j = 1 by Newton’s method at each iteration t = 1; 2; : : :
while ﬁxing (cid:18)ij with i; j ̸= 1 so that (cid:17)(t)
ij satisﬁes the fol-
lowing condition (Figure 2):
i1 = (n (cid:0) i + 1)=n;
(cid:17)(t)

1j = (n (cid:0) j + 1)=n:
(cid:17)(t)

Note that the rows and columns sum not to 1 but to 1=n due
to the normalization. The update formula is described as

26666666664

(cid:18)(t+1)
11

...

...

(cid:18)(t+1)
1n
(cid:18)(t+1)
21

(cid:18)(t+1)
n1

37777777775(cid:0) J

37777777775 =

26666666664

(cid:18)(t)
11
...
(cid:18)(t)
1n
(cid:18)(t)
21
...
(cid:18)(t)
n1

26666666664

(cid:0)1

(cid:17)(t)
1n
(cid:17)(t)
21

(cid:17)(t)
n1

(cid:0) (n (cid:0) 1 + 1)=n

(cid:17)(t)
11

...

...

(cid:0) (n (cid:0) n + 1)=n
(cid:0) (n (cid:0) 2 + 1)=n

(3)

(cid:0) (n (cid:0) n + 1)=n

37777777775 ;

where J is the Jacobian matrix given as

J(ij)(i′j′) =

@(cid:17)(t)
ij
@(cid:18)(t)
i′j′

= (cid:17)maxfi;i′g maxfj;j′g(cid:0)n2(cid:17)ij(cid:17)i′j′; (4)

ij

∑

, we can compute p(t+1)

which is derived from our theoretical result in Theorem 3.
Since J is a (2n(cid:0)1)(cid:2)(2n(cid:0)1) matrix, the time complexity
of each update is O(n3), which is needed to compute the
inverse of J.
After updating to (cid:18)(t+1)
and (cid:17)(t+1)
by Equation (2). Since this update does not ensure the
condition
as
and recompute p(t+1)
(cid:18)(t+1)
11
and (cid:17)(t+1)
By iterating the above update process in Equation (3) until
convergence, A = (aij) with aij = npij becomes doubly
stochastic.

∑
for each i; j 2 [n].

= 1, we again update (cid:18)(t+1)

ij p(t+1)
(cid:0) log

ij
= (cid:18)(t+1)

ij p(t+1)

ij

11

11

ij

ij

ij

ij

3. Numerical Experiments
We evaluate the efﬁciency of our algorithm compared to the
two prominent balancing methods, the standard Sinkhorn-
Knopp algorithm (Sinkhorn, 1964) and the state-of-the-art

a(cid:31)(cid:31)a(cid:31)(cid:30)a(cid:31)(cid:29)a(cid:31)(cid:28)a(cid:30)(cid:31)a(cid:30)(cid:30)a(cid:30)(cid:29)a(cid:30)(cid:28)a(cid:29)(cid:31)a(cid:29)(cid:30)a(cid:29)(cid:29)a(cid:29)(cid:28)a(cid:28)(cid:31)a(cid:28)(cid:30)a(cid:28)(cid:29)a(cid:28)(cid:28)η(cid:31)(cid:31)η(cid:31)(cid:30)η(cid:31)(cid:29)η(cid:31)(cid:28)η(cid:30)(cid:31)θ(cid:30)(cid:30)θ(cid:30)(cid:29)θ(cid:30)(cid:28)η(cid:29)(cid:31)θ(cid:29)(cid:30)θ(cid:29)(cid:29)θ(cid:29)(cid:28)η(cid:28)(cid:31)θ(cid:28)(cid:30)θ(cid:28)(cid:29)θ(cid:28)(cid:28)MatrixConstraints for balancingInvariantTensor Balancing on Statistical Manifold

Figure3.Results on Hessenberg matrices. The BNEWT algo-
rithm (green) failed to converge for n (cid:21) 200.

Figure5.Results on Trefethen matrices. The BNEWT algorithm
(green) failed to converge for n (cid:21) 200.

is clearly the fastest: It is three to ﬁve orders of magnitude
faster than the standard Sinkhorn-Knopp algorithm (plotted
in red). Although the BNEWT algorithm (plotted in green)
is competitive if n is small, it suddenly fails to converge
whenever n (cid:21) 200, which is consistent with results in the
original paper (Knight & Ruiz, 2013) where there is no re-
sult for the setting n (cid:21) 200 on the same matrix. Moreover,
our method converges around 10 to 20 steps, which is about
three and seven orders of magnitude smaller than BNEWT
and Sinkhorn-Knopp, respectively, at n = 100.
To see the behavior of the rate of convergence in detail, we
plot the convergence graph in Figure 4 for n = 20, where
we observe the slow convergence rate of the Sinkhorn-
Knopp algorithm and unstable convergence of the BNEWT
algorithm, which contrasts with our quick convergence.
Trefethen Matrix. Next, we collected a set of Trefethen
matrices from a collection website2, which are nonnega-
tive diagonal matrices with primes. Results are plotted in
Figure 5, where we observe the same trend as before: Our
algorithm is the fastest and about four orders of magnitude
faster than the Sinkhorn-Knopp algorithm. Note that larger
matrices with n > 300 do not have total support, which
is the necessary condition for matrix balancing (Knight &
Ruiz, 2013), while the BNEWT algorithm fails to converge
if n = 200 or n = 300.

4. Theoretical Analysis
In the following, we provide theoretical support to our al-
gorithm by formulating the problem as a projection within
a statistical manifold, in which a matrix corresponds to an
element, that is, a probability distribution, in the manifold.
We show that a balanced matrix forms a submanifold and
matrix balancing is projection of a given distribution onto
the submanifold, where the Jacobian matrix in Equation (4)
is derived from the gradient of the manifold.

2http://www.cise.ufl.edu/research/sparse/

matrices/

Figure4. Convergence graph on H20.

′

= (a

′

1(cid:0) 1; A

algorithm BNEWT (Knight & Ruiz, 2013), which uses
Newton’s method-like iterations with conjugate gradients.
All experiments were conducted on Amazon Linux AMI
release 2016.09 with a single core of 2.3 GHz Intel Xeon
CPU E5-2686 v4 and 256 GB of memory. All methods
were implemented in C++ with the Eigen library and
compiled with gcc 4.8.31. We have carefully implemented
BNEWT by directly translating the MATLAB code pro-
vided in (Knight & Ruiz, 2013) into C++ with the Eigen
library for fair comparison, and used the default parame-
′
ters. We measured the residual of a matrix A
ij) by
′T 1(cid:0) 1)∥2, where each en-
the squared norm ∥(A
′
try a
ij is obtained as npij in our algorithm, and ran each
of three algorithms until the residual is below the tolerance
threshold 10
Hessenberg Matrix. The ﬁrst set of experiments used a
Hessenberg matrix, which has been a standard benchmark
for matrix balancing (Parlett & Landis, 1982; Knight &
Ruiz, 2013). Each entry of an n (cid:2) n Hessenberg matrix
Hn = (hij) is given as hij = 0 if j < i (cid:0) 1 and hij = 1
otherwise. We varied the size n from 10 to 5; 000, and
measured running time (in seconds) and the number of it-
erations of each method.
Results are plotted in Figure 3. Our balancing algorithm
with the Newton’s method (plotted in blue in the ﬁgures)

(cid:0)6.

1An implementation of algorithms for matrices and third
https://github.com/

order
mahito-sugiyama/newton-balancing

is available at:

tensors

nNumber of iterations1050500500010nRunning time (sec.)1050500500010(cid:31)10(cid:30)10(cid:29)10(cid:28)10(cid:29)10(cid:28)1010–(cid:28)10–(cid:29)Newton (proposed)SinkhornBNEWTNumber of iterationsResidual0500100015002000250030001010–(cid:31)10–(cid:30)10–(cid:29)10–(cid:28)SinkhornNewtonBNEWTnNumber of iterations20100200300nRunning time (sec.)201002003001010(cid:27)10(cid:30)10(cid:26)10(cid:25)10(cid:26)10(cid:24)10–(cid:24)10–(cid:26)Newton (proposed)SinkhornBNEWTTensor Balancing on Statistical Manifold

∑

A probability vector is treated as a mapping p : S ! (0; 1)
x2S p(x) = 1, where every entry p(x) is as-
such that
sumed to be strictly larger than zero.
Using the zeta and the M¨obius functions, let us introduce
two mappings (cid:18) : S ! R and (cid:17) : S ! R as
∑
∑

From the M¨obius inversion formula, we have

(cid:22)(s; x) log p(s);

(cid:16)(x; s)p(s) =

(cid:17)(x) =

(cid:18)(x) =

p(s):

s(cid:21)x

s2S

(6)

(7)

log p(x) =

(cid:16)(s; x)(cid:18)(s) =

(cid:18)(s);

s(cid:20)x

∑
∑
∑
∑

s2S

s2S

(8)

(9)

p(x) =

s2S

(cid:22)(x; s)(cid:17)(s):

They are generalization of the log-linear model (Agresti,
∑
2012) that gives the probability p(x) of an n-dimensional
binary vector x = (x1; : : : ; xn) 2 f0; 1gn as

∑

∑

(cid:18)ixi +

(cid:18)ijxixj +

(cid:18)ijkxixjxk

log p(x) =

i

i<j

+ (cid:1)(cid:1)(cid:1) + (cid:18)1:::nx1x2 : : : xn (cid:0)  ;

i<j<k

where (cid:18) = ((cid:18)1; : : : ; (cid:18)12:::n) is a parameter vector,   is a
normalizer, and (cid:17) = ((cid:17)1; : : : ; (cid:17)12:::n) represents the ex-
pectation of variable combinations such that

(cid:17)i = E[xi] = Pr(xi = 1);
(cid:17)ij = E[xixj] = Pr(xi = xj = 1); i < j; : : :
(cid:17)1:::n = E[x1 : : : xn] = Pr(x1 = (cid:1)(cid:1)(cid:1) = xn = 1):

They coincide with Equations (8) and (7) when we let
S = 2V with V = f1; 2; : : : ; ng, each x 2 S as the set
of indices of “1” of x, and the order (cid:20) as the inclusion re-
lationship, that is, x (cid:20) y if and only if x (cid:18) y. Nakahara
et al. (2006) have pointed out that (cid:18) can be computed from
p using the inclusion-exclusion principle in the log-linear
model. We exploit this combinatorial property of the log-
linear model using the M¨obius inversion formula on posets
and extend the log-linear model from the power set 2V to
any kind of posets (S;(cid:20)). Sugiyama et al. (2016) studied a
relevant log-linear model, but the relationship with M¨obius
inversion formula has not been analyzed yet.

4.1. Formulation

We introduce our log-linear probabilistic model, where the
outcome space is a partially ordered set, or a poset (Gierz
et al., 2003). We prepare basic notations and the key math-
ematical tool for posets, the M¨obius inversion formula, fol-
lowed by formulating the log-linear model.

4.1.1. M ¨OBIUS INVERSION
A poset (S;(cid:20)), the set of elements S and a partial order
(cid:20) on S, is a fundamental structured space in computer
science. A partial order “(cid:20)” is a relation between el-
ements in S that satisﬁes the following three properties:
For all x; y; z 2 S, (1) x (cid:20) x (reﬂexivity), (2) x (cid:20) y,
y (cid:20) x ) x = y (antisymmetry), and (3) x (cid:20) y,
y (cid:20) z ) x (cid:20) z (transitivity). In what follows, S is al-
ways ﬁnite and includes the least element (bottom) ? 2 S;
that is, ? (cid:20) x for all x 2 S. We denote S n f?g by S+.
Rota (1964) introduced the M¨obius inversion formula on
posets by generalizing the inclusion-exclusion principle.
Let (cid:16) : S (cid:2) S ! f0; 1g be the zeta function deﬁned as

{

(cid:16)(s; x) =

8<: 1
(cid:0)∑

0

The M¨obius function (cid:22) : S(cid:2)S ! Z satisﬁes (cid:16)(cid:22) = I, which
is inductively deﬁned for all x; y with x (cid:20) y as

(cid:22)(x; y) =

x(cid:20)s<y (cid:22)(x; s)

From the deﬁnition, it follows that

if x = y;
if x < y;
otherwise:

if s (cid:20) x;
otherwise:

1
0

∑
∑

s2S

∑
∑

x(cid:20)s(cid:20)y

(cid:16)(s; y)(cid:22)(x; s) =

(cid:22)(x; s) = (cid:14)xy;

(5)

(cid:16)(x; s)(cid:22)(s; y) =

(cid:22)(s; y) = (cid:14)xy

x(cid:20)s(cid:20)y

s2S

with the Kronecker delta (cid:14) such that (cid:14)xy = 1 if x = y and
(cid:14)xy = 0 otherwise. Then for any functions f, g, and h with
the domain S such that

g(x) =

(cid:16)(s; x)f (s) =

f (s);

∑
∑

s2S

h(x) =

(cid:16)(x; s)f (s) =

s2S

f (s);

f is uniquely recovered with the M¨obius function:

∑

f (x) =

s2S

(cid:22)(s; x)g(s);

f (x) =

s2S

s(cid:20)x

∑
∑
∑

s(cid:21)x

(cid:22)(x; s)h(s):

4.2. Dually Flat Riemannian Manifold

This is called the M¨obius inversion formula and is at the
heart of enumerative combinatorics (Ito, 1993).

4.1.2. LOG-LINEAR MODEL ON POSETS
We consider a probability vector p on (S;(cid:20)) that gives a
discrete probability distribution with the outcome space S.

We theoretically analyze our log-linear model introduced in
Equations (6), (7) and show that they form dual coordinate
systems on a dually ﬂat manifold, which has been mainly
studied in the area of information geometry (Amari, 2001;
Nakahara & Amari, 2002; Amari, 2014; 2016). Moreover,
we show that the Riemannian metric and connection of our
model can be analytically computed in closed forms.

Tensor Balancing on Statistical Manifold

′

(x)(cid:17)(x): (11)

D [P; Q] =

x2S

q(x) log

q(x)
p(x)

;

In the following, we denote by (cid:24) the function (cid:18) or (cid:17) and by
∇ the gradient operator with respect to S+ = S nf?g, i.e.,
(∇f ((cid:24)))(x) = @f =@(cid:24)(x) for x 2 S+, and denote by S the
set of probability distributions speciﬁed by probability vec-
tors, which forms a statistical manifold. We use uppercase
letters P; Q; R; : : : for points (distributions) in S and their
lowercase letters p; q; r; : : : for the corresponding probabil-
ity vectors treated as mappings. We write (cid:18)P and (cid:17)P if they
are connected with p by Equations (6) and (7), respectively,
and abbreviate subscripts if there is no ambiguity.

4.2.1. DUALLY FLAT STRUCTURE
We show that S has the dually ﬂat Riemannian structure
induced by two functions (cid:18) and (cid:17) in Equation (6) and (7).
We deﬁne  ((cid:18)) as

 ((cid:18)) = (cid:0)(cid:18)(?) = (cid:0) log p(?);

(10)
which corresponds to the normalizer of p. It is a convex
function since we have

 ((cid:18)) = log

exp

∑
?<s(cid:20)x (cid:18)(s) (cid:0)  ((cid:18)). We apply the Leg-

?<s(cid:20)x

x2S

(cid:18)(s)

from log p(x) =
endre transformation to  ((cid:18)) given as

(

1A

∑

0@ ∑
∑

(cid:17) =

(cid:18)
x2S+

)

′

)

; (cid:18)

φ((cid:17)) = max

(cid:18)′

Then φ((cid:17)) coincides with the negative entropy.
Theorem 1 (Legendre dual).

p(x) log p(x):

∑

1A

p(s)

s(cid:21)x
(?) ) :

Proof. From Equation (5), we have

′

(cid:18)

(cid:17) =

′
(cid:22)(s; x) log p

(s)

=

x2S+
Thus it holds that

′

(cid:17) (cid:0)  ((cid:18)

′

) =

(cid:18)

(x) (cid:0) log p
′
∑

′
p(x) log p

x2S

Hence it is maximized with p(x) = p

(cid:18)

′

′

(cid:17) (cid:0)  ((cid:18)
∑
0@ ∑

x2S

φ((cid:17)) =

?<s(cid:20)x

′
p(x) ( log p

∑
∑

x2S+

)
) =

∑

∑
∑

(∑
(∑

Proof. They can be directly derived from our deﬁnitions
(Equations (6) and (11)) as

p(s) = (cid:17)(x);

s(cid:21)x

@ ((cid:18))
@(cid:18)(x)

@φ((cid:17))
@(cid:17)(x)

=

=

(

y2S exp
@

@(cid:17)(x)

y(cid:21)x exp

?<s(cid:20)y (cid:18)(s)

?<s(cid:20)y (cid:18)(s)

)

(cid:18)(cid:17) (cid:0)  ((cid:18))
]

= (cid:18)(x):

∑

[

Moreover, we can conﬁrm the orthogonality of (cid:18) and (cid:17) as

E

@ log p(s)

@ log p(s)

@(cid:18)(x)

@(cid:17)(y)

(cid:16)(x; s)(cid:22)(s; y) = (cid:14)xy:

=

s2S

The last equation holds from Equation (5), hence the
M¨obius inversion directly leads to the orthogonality.
The Bregman divergence is known to be the canonical di-
vergence (Amari, 2016, Section 6.6) to measure the differ-
ence between two distributions P and Q on a dually ﬂat
manifold, which is deﬁned as

D [P; Q] =  ((cid:18)P ) + φ((cid:17)Q) (cid:0) (cid:18)P (cid:17)Q:

In our case, since we have φ((cid:17)Q) =
and (cid:18)P (cid:17)Q(cid:0) ((cid:18)P ) =
and Equation (12), it is given as

x2S q(x) log q(x)
x2S q(x) log p(x) from Theorem 1

∑

∑

∑

which coincides with the Kullback–Leibler divergence (KL
divergence) from Q to P : D [P; Q] = DKL [Q; P ].

4.2.2. RIEMANNIAN STRUCTURE
Next we analyze the Riemannian structure on S and show
that the M¨obius inversion formula enables us to compute
the Riemannian metric of S.
Theorem 3 (Riemannian metric). The manifold (S; g((cid:24)))
is a Riemannian manifold with the Riemannian metric g((cid:24))
such that for all x; y 2 S+

[
(cid:16)(x; s)(cid:16)(y; s)p(s) (cid:0) (cid:17)(x)(cid:17)(y)

]

if (cid:24) = (cid:18);

if (cid:24) = (cid:17):

8>>><>>>:

∑
∑

s2S

s2S

gxy((cid:24)) =

(x):

(12)

(cid:22)(s; x)(cid:22)(s; y)p(s)

(cid:0)1

′

(x).

Proof. Since the Riemannian metric is deﬁned as
g((cid:17)) = ∇∇φ((cid:17));

g((cid:18)) = ∇∇ ((cid:18));

Since they are connected with each other by the Legendre
transformation, they form a dual coordinate system ∇ ((cid:18))
and ∇φ((cid:17)) of S (Amari, 2016, Section 1.5), which coin-
cides with (cid:18) and (cid:17) as follows.
Theorem 2 (dual coordinate system).

∇ ((cid:18)) = (cid:17); ∇φ((cid:17)) = (cid:18):

(13)

when (cid:24) = (cid:18) we have

gxy((cid:18)) =

=

@(cid:18)(x)@(cid:18)(y)

@2

∑

@

@(cid:18)(x)

s2S

 ((cid:18)) =

(cid:17)(y)

@(cid:18)(x)

@

0@ ∑

?<u(cid:20)s

(cid:16)(y; s) exp

(cid:18)(u) (cid:0)  ((cid:18))

1A

∑

s2S

=

Tensor Balancing on Statistical Manifold

(cid:16)(x; s)(cid:16)(y; s)p(s) (cid:0) jSj(cid:17)(x)(cid:17)(y):

@(cid:17)(x)

@

0@∑

u(cid:21)s
(cid:0)1:

]
]

φ((cid:17)) =

(cid:18)(y)

When (cid:24) = (cid:17), it follows that

@(cid:17)(x)@(cid:17)(y)

@2

∑

@

∑

@(cid:17)(x)

gxy((cid:17)) =

=

=

(cid:22)(s; x)(cid:22)(s; y)p(s)

s2S

(cid:22)(s; y) log

s(cid:20)y

(cid:22)(s; u)(cid:17)(u)

1A

[
[

E

E

Since g((cid:24)) coincides with the Fisher information matrix,

@

@(cid:18)(x)

@

@(cid:17)(x)

log p(s)

log p(s)

@

@(cid:18)(y)

@

@(cid:17)(y)

log p(s)

= gxy((cid:18));

log p(s)

= gxy((cid:17)):

)

Then the Riemannian (Levi–Chivita) connection (cid:0)((cid:24)) with
respect to (cid:24), which is deﬁned as

+

1
2

(cid:0)xyz((cid:24)) =

@gyz((cid:24))
@(cid:24)(x)

@gxz((cid:24))
@(cid:24)(y)

(cid:0) @gxy((cid:24))
@(cid:24)(z)
for all x; y; z 2 S+, can be analytically obtained.
Theorem 4 (Riemannian connection). The Riemannian
connection (cid:0)((cid:24)) on the manifold (S; g((cid:24))) is given in the
)
following for all x; y; z 2 S+,
(cid:16)(y; s) (cid:0) (cid:17)(y)
p(s)

(
)(
(
)
(cid:16)(x; s) (cid:0) (cid:17)(x)
(cid:16)(z; s) (cid:0) (cid:17)(z)

s2S

1
2

(cid:0)xyz((cid:24)) =

(

∑
∑

s2S

8>>>>>><>>>>>>:

(cid:0) 1
2

(cid:22)(s; x)(cid:22)(s; y)(cid:22)(s; z)p(s)

if (cid:24) = (cid:18);
(cid:0)2 if (cid:24) = (cid:17):

Proof. Connections (cid:0)xyz((cid:18)) and (cid:0)xyz((cid:17)) can be obtained
by directly computing @gyz((cid:18))=@(cid:18)(x) and @gyz((cid:17))=@(cid:17)(x),
respectively.

4.3. The Projection Algorithm

Projection of a distribution onto a submanifold is essen-
tial; several machine learning algorithms are known to be
formulated as projection of a distribution empirically esti-
mated from data onto a submanifold that is speciﬁed by the
target model (Amari, 2016). Here we deﬁne projection of
distributions on posets and show that Newton’s method can
be applied to perform projection as the Jacobian matrix can
be analytically computed.

4.3.1. DEFINITION
Let S((cid:12)) be a submanifold of S such that
S((cid:12)) = fP 2 S j (cid:18)P (x) = (cid:12)(x); 8x 2 dom((cid:12))g (14)

speciﬁed by a function (cid:12) with dom((cid:12)) (cid:18) S+. Projection
of P 2 S onto S((cid:12)), called m-projection, which is deﬁned
as the distribution P(cid:12) 2 S((cid:12)) such that

(cid:18)P(cid:12) (x) = (cid:12)(x)
(cid:17)P(cid:12) (x) = (cid:17)P (x)

if x 2 dom((cid:12));
if x 2 S+ n dom((cid:12));

is the minimizer of the KL divergence from P to S((cid:12)):

P(cid:12) = argmin
Q2S((cid:12))

DKL[P; Q]:

The dually ﬂat structure with the coordinate systems (cid:18) and
(cid:17) guarantees that the projected distribution P(cid:12) always ex-
ists and is unique (Amari, 2009, Theorem 3). Moreover,
the Pythagorean theorem holds in the dually ﬂat manifold,
that is, for any Q 2 S((cid:12)) we have

DKL[P; Q] = DKL[P; P(cid:12)] + DKL[P(cid:12); Q]:

We can switch (cid:17) and (cid:18) in the submanifold S((cid:12)) by chang-
ing DKL[P; Q] to DKL[Q; P ], where the projected distri-
bution P(cid:12) of P is given as

{

{

(cid:18)P(cid:12) (x) = (cid:18)P (x)
(cid:17)P(cid:12) (x) = (cid:12)(x)

if x 2 S+ n dom((cid:12));
if x 2 dom((cid:12));

This projection is called e-projection.
Example 1 (Boltzmann machine). Given a Boltzmann ma-
chine represented as an undirected graph G = (V; E) with
a vertex set V and an edge set E (cid:18) ffi; jg j i; j 2 V g.
The set of probability distributions that can be modeled by
a Boltzmann machine G coincides with the submanifold
S B = fP 2 S j (cid:18)P (x) = 0 if jxj > 2 or x ̸2 Eg;

with S = 2V . Let ^P be an empirical distribution esti-
mated from a given dataset. The learned model is the m-
projection of the empirical distribution ^P onto S B, where
the resulting distribution P(cid:12) is given as

{

(cid:18)P(cid:12) (x) = 0
(cid:17)P(cid:12) (x) = (cid:17) ^P (x)

if jxj > 2 or x ̸2 E;
if jxj = 1 or x 2 E:

4.3.2. COMPUTATION

Here we show how to compute projection of a given prob-
ability distribution. We show that Newton’s method can be
used to efﬁciently compute the projected distribution P(cid:12) by
iteratively updating P (0)
(cid:12) ; : : : until
converging to P(cid:12).

(cid:12) = P as P (0)

(cid:12) ; P (1)

(cid:12) ; P (2)

Let us start with the m-projection with initializing P (0)
(cid:12) =
(x) for all x 2 dom(cid:12)
P . In each iteration t, we update (cid:18)(t)
P(cid:12)
(x) = (cid:17)P (x) for all x 2 S+ n dom((cid:12)),
while ﬁxing (cid:17)(t)
P(cid:12)
)
which is possible from the orthogonality of (cid:18) and (cid:17). Using
Newton’s method, (cid:17)(t+1)

(x) should satisfy

)

(

(

∑

P(cid:12)

(cid:17)(t+1)
P(cid:12)

(y) (cid:0) (cid:17)(t)

P(cid:12)

(y)

= 0;

(x) (cid:0) (cid:12)(x)

(cid:18)(t)
P(cid:12)

+
Jxy
y2dom((cid:12))

Tensor Balancing on Statistical Manifold

Jxy =

for every x 2 dom((cid:12)), where Jxy is an entry of the
jdom((cid:12))j (cid:2) jdom((cid:12))j Jacobian matrix J and given as
(cid:0)1

∑

(x)

=

(cid:22)(s; x)(cid:22)(s; y)p(t)

(cid:12) (s)

s2S

(y)
from Theorem 3. Therefore, we have the update formula
for all x 2 dom((cid:12)) as
(x) = (cid:17)(t)
P(cid:12)

)
(y) (cid:0) (cid:12)(y)

∑

(x) (cid:0)

(cid:17)(t+1)
P(cid:12)

(cid:18)(t)
P(cid:12)

(

:

@(cid:18)(t)
P(cid:12)
@(cid:17)(t)
P(cid:12)

(cid:0)1
J
xy
y2dom((cid:12))

(x) for x 2 dom((cid:12)) while ﬁx-
(x) = (cid:18)P (x) for all x 2 S+ n dom((cid:12)). To ensure
(?) = 1, we add ? to dom((cid:12)) and (cid:12)(?) = 1. We

In e-projection, update (cid:17)(t)
P(cid:12)
ing (cid:18)(t)
P(cid:12)
(cid:17)(t)
P(cid:12)
update (cid:18)(t)
P(cid:12)
(x) = (cid:18)(t)
P(cid:12)

(x) (cid:0)

(cid:18)(t+1)
P(cid:12)

(x) at each step t as

′(cid:0)1
xy

(y) (cid:0) (cid:12)(y)

(cid:17)(t)
P(cid:12)

;

(

)

′

J

xy =

@(cid:17)(t)
P(cid:12)
@(cid:18)(t)
P(cid:12)

(x)

(y)

(cid:16)(x; s)(cid:16)(y; s)p(t)

(cid:12) (s)
(x)(cid:17)(t)
P(cid:12)

P(cid:12)

(cid:0) jSj(cid:17)(t)
(y):
)
(?) as it is not
) (cid:16)(s; x):

(s)

In this case, we also need to update (cid:18)(t)
P(cid:12)
guaranteed to be ﬁxed. Let us deﬁne

′(t+1)
p
(cid:12)

(x) = p(t)

(cid:12) (x)

exp

s2dom((cid:12))

exp

(s)

∑
∑

J
y2dom((cid:12))

=

s2S

∏
(
(

Since we have

p(t+1)
(cid:12)

(x) =

exp

exp

(cid:18)(t+1)
P(cid:12)
(cid:18)(t)
P(cid:12)

it follows that
(cid:18)(t+1)
P(cid:12)
= (cid:0) log

(
(
(?) (cid:0) (cid:18)(t)

P(cid:12)

exp

(cid:18)(t)
P(cid:12)

)

(?)
(?)

The time complexity of each iteration is O(jdom((cid:12))j3),
which is required to compute the inverse of the Jacobian
matrix.
Global convergence of the projection algorithm is always
guaranteed by the convexity of a submanifold S((cid:12)) deﬁned
in Equation (14). Since S((cid:12)) is always convex with respect
to the (cid:18)- and (cid:17)-coordinates, it is straightforward to see that
our e-projection is an instance of the Bregman algorithm
onto a convex region, which is well known to always con-
verge to the global solution (Censor & Lent, 1981).

5. Balancing Matrices and Tensors
Now we are ready to solve the problem of matrix and tensor
balancing as projection on a dually ﬂat manifold.

(cid:18)(t+1)
P(cid:12)
(cid:18)(t)
P(cid:12)

(
(
)
) p

(?)
(?)
∑

+

x2S+

′(t+1)
(cid:12)

(x);

)

(x)

;

′(t+1)
(cid:12)

p

5.1. Matrix Balancing
Recall that the task of matrix balancing is to ﬁnd r; s 2 Rn
that satisfy (RAS)1 = 1 and (RAS)T 1 = 1 with R =
diag(r) and S = diag(s) for a given nonnegative square
matrix A = (aij) 2 Rn(cid:2)n(cid:21)0 .
Let us deﬁne S as

S = f(i; j) j i; j 2 [n] and aij ̸= 0g;

x = (i; j) (cid:20) y = (k; l) , i (cid:20) j and k (cid:20) l;

(cid:19)k;m = minf x = (i1; i2) 2 S j im = k g;

(15)
∑
where we remove zero entries from the outcome space S as
our formulation cannot treat zero probability, and give each
probability as p((i; j)) = aij=
ij aij. The partial order
(cid:20) of S is naturally introduced as
(16)
resulting in ? = (1; 1). In addition, we deﬁne (cid:19)k;m for
each k 2 [n] and m 2 f1; 2g such that
where the minimum is with respect to the order (cid:20). If (cid:19)k;m
does not exist, we just remove the entire kth row if m = 1
or kth column if m = 2 from A. Then we switch rows and
columns of A so that the condition
(17)
is satisﬁed for each m 2 f1; 2g, which is possible for any
matrices. Since we have
n
(cid:17)((cid:19)k;m) (cid:0) (cid:17)((cid:19)k+1;m) =
j=1 p((k; j))
n
i=1 p((i; k))
if the condition (17) is satisﬁed, the probability distribution
is balanced if for all k 2 [n] and m 2 f1; 2g

(cid:19)1;m (cid:20) (cid:19)2;m (cid:20) (cid:1)(cid:1)(cid:1) (cid:20) (cid:19)n;m

{ ∑
∑

if m = 1;
if m = 2

(cid:17)((cid:19)k;m) =

n(cid:0)k+1

:

n

Therefore, we obtain the following result.
Matrix balancing as e-projection:
Given a matrix A 2 Rn(cid:2)n with its normalized probabil-
ity distribution P 2 S such that p((i; j)) = aij=
ij aij.
Deﬁne the poset (S;(cid:20)) by Equations (15) and (16) and let
S((cid:12)) be the submanifold of S such that
S((cid:12)) = fP 2 S j (cid:17)P (x) = (cid:12)(x) for all x 2 dom((cid:12))g;
where the function (cid:12) is given as

∑

dom((cid:12)) = f(cid:19)k;m 2 S j k 2 [n]; m 2 f1; 2gg;

(cid:12)((cid:19)k;m) =

n(cid:0)k+1

:

n

(cid:18)P(cid:12) (x) = (cid:18)P (x)
(cid:17)P(cid:12) (x) = (cid:12)(x)

Matrix balancing is the e-projection of P onto the subman-
ifold S((cid:12)), that is, the balanced matrix (RAS)=n is the
distribution P(cid:12) such that

{

if x 2 S+ n dom((cid:12));
if x 2 dom((cid:12));
{

)

(

i∑

which is unique and always exists in S, thanks to its dually
ﬂat structure. Moreover, two balancing vectors r and s are

∑
(cid:18)P(cid:12) ((cid:19)k;m) (cid:0) (cid:18)P ((cid:19)k;m)

exp
for every i 2 [n] and r = rn=

k=1

=

ij aij.

ri
ai

if m = 1;
if m = 2;

■

Tensor Balancing on Statistical Manifold

5.2. Tensor Balancing

Next, we generalize our approach from matrices to tensors.
For an Nth order tensor A = (ai1i2:::iN ) 2 Rn1(cid:2)n2(cid:2)(cid:1)(cid:1)(cid:1)(cid:2)nN
and a vector b 2 Rnm, the m-mode product of A and b is
deﬁned as

(A (cid:2)m b)i1:::im(cid:0)1im+1:::iN

=

ai1i2:::iN bim :

nm∑

im=1

We deﬁne tensor balancing as follows: Given a tensor A 2
Rn1(cid:2)n2(cid:2)(cid:1)(cid:1)(cid:1)(cid:2)nN with n1 = (cid:1)(cid:1)(cid:1) = nN = n, ﬁnd (N (cid:0) 1)
order tensors R1; R2; : : : ; RN such that

′ (cid:2)m 1 = 1 (2 Rn1(cid:2)(cid:1)(cid:1)(cid:1)(cid:2)nm(cid:0)1(cid:2)nm+1(cid:2)(cid:1)(cid:1)(cid:1)(cid:2)nN )

A

∑

for all m 2 [N ], i.e.,
′
n
im=1 a
′
i1i2:::iN
i1i2:::iN of the balanced tensor A
entry a
′
i1i2:::iN

= ai1i2:::iN

∏

Rm

a

(18)
= 1, where each
′ is given as

i1:::im(cid:0)1im+1:::iN

:

m2[N ]

′ that satisﬁes Equation (18) is called multi-
A tensor A
stochastic (Cui et al., 2014). Note that this is exactly the
same as the matrix balancing problem if N = 2.
It is straightforward to extend matrix balancing to tensor
balancing as e-projection onto a submanifold. Given a ten-
sor A 2 Rn1(cid:2)n2(cid:2)(cid:1)(cid:1)(cid:1)(cid:2)nN with its normalized probability
distribution P such that

/∑

p(x) = ai1i2:::iN

aj1j2:::jN

(19)

j1j2:::jN

n

{

for all x = (i1; i2; : : : ; iN ). The objective is to obtain
im=1 p(cid:12)((i1; : : : ; iN )) = 1=(nN(cid:0)1) for all
P(cid:12) such that
m 2 [N ] and i1; : : : ; iN 2 [n]. In the same way as matrix
balancing, we deﬁne S as

}
̸= 0
with removing zero entries and the partial order (cid:20) as
x = (i1 : : : iN ) (cid:20) y = (j1 : : : jN ) , 8m 2 [N ]; im (cid:20) jm:
In addition, we introduce (cid:19)k;m as

(cid:12)(cid:12) ai1i2:::iN

(i1; i2; : : : ; iN ) 2 [n]N

S =

(cid:19)k;m = minf x = (i1; i2; : : : ; iN ) 2 S j im = k g:

and require the condition in Equation (17).
Tensor balancing as e-projection:
Given a tensor A 2 Rn1(cid:2)n2(cid:2)(cid:1)(cid:1)(cid:1)(cid:2)nN with its normalized
probability distribution P 2 S given in Equation (19). The
submanifold S((cid:12)) of multistochastic tensors is given as
S((cid:12)) = fP 2 S j (cid:17)P (x) = (cid:12)(x) for all x 2 dom((cid:12))g;
where the domain of the function (cid:12) is given as
dom((cid:12)) = f (cid:19)k;m j k 2 [n]; m 2 [N ] g

and each value is described using the zeta function as

∑

(cid:12)((cid:19)k;m) =

l2[n]

(cid:16)((cid:19)k;m; (cid:19)l;m)

1

nN(cid:0)1 :

∑

Tensor balancing is the e-projection of P onto the subman-
ifold S((cid:12)), that is, the multistochastic tensor is the distri-
bution P(cid:12) such that

{

(cid:18)P(cid:12) (x) = (cid:18)P (x)
(cid:17)P(cid:12) (x) = (cid:12)(x)

if x 2 S+ n dom((cid:12));
if x 2 dom((cid:12));

which is unique and always exists in S, thanks to its dually
ﬂat structure. Moreover, each balancing tensor Rm is

1A

Rm

i1:::im(cid:0)1im+1:::iN

0@ ∑

im′∑

m′̸=m

k=1

= exp

(cid:18)P(cid:12) ((cid:19)k;m′) (cid:0) (cid:18)P ((cid:19)k;m′)

∑

for every m 2 [N ] and R1 = R1nN(cid:0)1=
to recover a multistochastic tensor.
Our result means that the e-projection algorithm based on
Newton’s method proposed in Section 4.3 converges to the
unique balanced tensor whenever S((cid:12)) ̸= ∅ holds.

aj1:::jN
■

j1:::jN

6. Conclusion
In this paper, we have solved the open problem of tensor
balancing and presented an efﬁcient balancing algorithm
using Newton’s method. Our algorithm quadratically con-
verges, while the popular Sinkhorn-Knopp algorithm lin-
early converges. We have examined the efﬁciency of our
algorithm in numerical experiments on matrix balancing
and showed that the proposed algorithm is several orders
of magnitude faster than the existing approaches.
We have analyzed theories behind the algorithm, and
proved that balancing is e-projection in a special type of
a statistical manifold, in particular, a dually ﬂat Rieman-
nian manifold studied in information geometry. Our key
ﬁnding is that the gradient of the manifold, equivalent to
Riemannian metric or the Fisher information matrix, can be
analytically obtained using the M¨obius inversion formula.
Our information geometric formulation can model several
machine learning applications such as statistical analysis
on a DAG structure. Thus, we can perform efﬁcient learn-
ing as projection using information of the gradient of man-
ifolds by reformulating such models, which we will study
in future work.

Acknowledgements
The authors sincerely thank Marco Cuturi for his valu-
able comments.
This work was supported by JSPS
KAKENHI Grant Numbers JP16K16115, JP16H02870
(MS), JP26120732 and JP16H06570 (HN). The research
of K.T. was supported by JST CREST JPMJCR1502,
RIKEN PostK, KAKENHI Nanostructure and KAKENHI
JP15H05711.

Tensor Balancing on Statistical Manifold

References
Agresti, A. Categorical data analysis. Wiley, 3 edition,

2012.

Ahmed, M., De Loera, J., and Hemmecke, R. Polyhedral
Cones of Magic Cubes and Squares, volume 25 of Algo-
rithms and Combinatorics, pp. 25–41. Springer, 2003.

Akartunalı, K. and Knight, P. A. Network models and
biproportional rounding for fair seat allocations in the
UK elections. Annals of Operations Research, pp. 1–19,
2016.

the National Academy of Sciences, 108(23):9679–9684,
2011.

Gierz, G., Hofmann, K. H., Keimel, K., Lawson, J. D., Mis-
love, M., and Scott, D. S. Continuous Lattices and Do-
mains. Cambridge University Press, 2003.

Idel, M. A review of matrix scaling and sinkhorn’s normal
form for matrices and positive maps. arXiv:1609.06349,
2016.

Ito, K. (ed.). Encyclopedic Dictionary of Mathematics. The

MIT Press, 2 edition, 1993.

Amari, S.

Information geometry on hierarchy of proba-
bility distributions. IEEE Transactions on Information
Theory, 47(5):1701–1711, 2001.

Knight, P. A. The Sinkhorn–Knopp algorithm: Conver-
gence and applications. SIAM Journal on Matrix Analy-
sis and Applications, 30(1):261–275, 2008.

Amari, S. Information geometry and its applications: Con-
In Nielsen, F.
vex function and dually ﬂat manifold.
(ed.), Emerging Trends in Visual Computing: LIX Fall
Colloquium, ETVC 2008, Revised Invited Papers, pp.
75–102. Springer, 2009.

Amari, S.

Information geometry of positive measures
and positive-deﬁnite matrices: Decomposable dually ﬂat
structure. Entropy, 16(4):2131–2145, 2014.

Amari, S.

Information Geometry and Its Applications.

Springer, 2016.

Balinski, M. Fair majority voting (or how to eliminate ger-
rymandering). American Mathematical Monthly, 115(2):
97–113, 2008.

Censor, Y. and Lent, A. An iterative row-action method for
interval convex programming. Journal of Optimization
Theory and Applications, 34(3):321–353, 1981.

Chang, H., Paksoy, V. E., and Zhang, F. Polytopes of
stochastic tensors. Annals of Functional Analysis, 7(3):
386–393, 2016.

Cui, L.-B., Li, W., and Ng, M. K. Birkhoff–von Neumann
theorem for multistochastic tensors. SIAM Journal on
Matrix Analysis and Applications, 35(3):956–973, 2014.

Cuturi, M. Sinkhorn distances: Lightspeed computation of
In Advances in Neural Information

optimal transport.
Processing Systems 26, pp. 2292–2300, 2013.

Frogner, C., Zhang, C., Mobahi, H., Araya, M., and Pog-
gio, T. A. Learning with a Wasserstein loss. In Advances
in Neural Information Processing Systems 28, pp. 2053–
2061, 2015.

Ganmor, E., Segev, R., and Schneidman, E. Sparse low-
order interaction network underlies a highly correlated
and learnable neural population code. Proceedings of

Knight, P. A. and Ruiz, D. A fast algorithm for matrix
balancing. IMA Journal of Numerical Analysis, 33(3):
1029–1047, 2013.

Lahr, M. and de Mesnard, L. Biproportional techniques
in input-output analysis: Table updating and structural
analysis. Economic Systems Research, 16(2):115–134,
2004.

Lamond, B. and Stewart, N. F. Bregman’s balancing
method. Transportation Research Part B: Methodologi-
cal, 15(4):239–248, 1981.

Livne, O. E. and Golub, G. H. Scaling by binormalization.

Numerical Algorithms, 35(1):97–120, 2004.

Marshall, A. W. and Olkin, I. Scaling of matrices to achieve
speciﬁed row and column sums. Numerische Mathe-
matik, 12(1):83–90, 1968.

Miller, R. E. and Blair, P. D. Input-Output Analysis: Foun-
dations and Extensions. Cambridge University Press, 2
edition, 2009.

Moon, T. K., Gunther, J. H., and Kupin, J. J. Sinkhorn
solves sudoku. IEEE Transactions on Information The-
ory, 55(4):1741–1746, 2009.

Nakahara, H. and Amari, S. Information-geometric mea-
sure for neural spikes. Neural Computation, 14(10):
2269–2316, 2002.

Nakahara, H., Nishimura, S., Inoue, M., Hori, G., and
Amari, S. Gene interaction in DNA microarray data is
decomposed by information geometric measure. Bioin-
formatics, 19(9):1124–1131, 2003.

Nakahara, H., Amari, S., and Richmond, B. J. A com-
parison of descriptive models of a single spike train by
information-geometric measure. Neural computation, 18
(3):545–568, 2006.

Tensor Balancing on Statistical Manifold

Parikh, A. Forecasts of input-output matrices using the
R.A.S. method. The Review of Economics and Statistics,
61(3):477–481, 1979.

Parlett, B. N. and Landis, T. L. Methods for scaling to dou-
bly stochastic form. Linear Algebra and its Applications,
48:53–79, 1982.

Rao, S. S. P., Huntley, M. H., Durand, N. C., Stamenova,
E. K., Bochkov, I. D., Robinson, J. T., Sanborn, A. L.,
Machol, I., Omer, A. D., Lander, E. S., and Aiden, E. L.
A 3D map of the human genome at kilobase resolution
reveals principles of chromatin looping. Cell, 159(7):
1665–1680, 2014.

Rota, G.-C. On the foundations of combinatorial theory I:
Theory of M¨obius functions. Z. Wahrseheinlichkeitsthe-
orie, 2:340–368, 1964.

Sinkhorn, R. A relationship between arbitrary positive ma-
trices and doubly stochastic matrices. The Annals of
Mathematical Statistics, 35(2):876–879, 06 1964.

Sinkhorn, R. and Knopp, P. Concerning nonnegative ma-
trices and doubly stochastic matrices. Paciﬁc Journal of
Mathematics, 21(2):343–348, 1967.

Solomon, J., de Goes, F., Peyr´e, G., Cuturi, M., Butscher,
A., Nguyen, A., Du, T., and Guibas, L. Convolutional
Wasserstein distances: Efﬁcient optimal transportation
on geometric domains. ACM Transactions on Graphics,
34(4):66:1–66:11, 2015.

Soules, G. W. The rate of convergence of sinkhorn bal-
ancing. Linear Algebra and its Applications, 150:3–40,
1991.

Sugiyama, M., Nakahara, H., and Tsuda, K. Information
In 2016 IEEE In-
decomposition on structured space.
ternational Symposium on Information Theory, pp. 575–
579, July 2016.

Wu, H.-J. and Michor, F. A computational strategy to adjust
for copy number in tumor Hi-C data. Bioinformatics, 32
(24):3695–3701, 2016.

